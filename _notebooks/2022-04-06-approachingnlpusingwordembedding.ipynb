{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc7fd35-8e6b-4432-a162-77624119042a",
   "metadata": {},
   "source": [
    "# \"NLP Approach using Word Embedding\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2e013-e944-46b8-aa30-0b3dff88d2c7",
   "metadata": {},
   "source": [
    "This is a minimal yet very powerful approach of NLP problems that you can use on Kaggle competition. I got my attention to the book written by one of Kaggle Grandmaster in his book called [Approaching (Almost) Any Machine Learning Problem](https://www.amazon.com/Approaching-Almost-Machine-Learning-Problem-ebook/dp/B089P13QHT). I am so interested in how the author approaches the problems for each case in Kaggle Competition started by Supervised and Unsuperviced problems. So, NLP is one of the problems in the competition in Kaggle. We know that many approaches that we can do to preprocess text data from Bag of Words, TFIDF to Word Embedding. I am interested to dive deeper into Word Embedding because this approach yield a  better result compared to BOW or Tfidf approach based on this dataset. You can find the comparison by reading this book. I found this approach quite useful for my NLP competition. Before reading this NLP approach, I hope the readers have the fundamentals concepts of pytorch, data visualization libraries and NLP concept to know the code better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ad11b-8b31-4167-8f8f-976c5b14af49",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ea893f-395b-458a-ae63-a7f3277f69a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "movies = pd.read_csv(\"imdb.csv\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af414522-38cb-4ccc-8a98-747dfd679c1f",
   "metadata": {},
   "source": [
    "## Check Proportion of target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5c7cf7-e26e-4306-abcf-3e3d521b3335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies.sentiment.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad4844a-0ec2-4ee3-a115-6e373b3f5799",
   "metadata": {},
   "source": [
    "## Create Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec938d0d-fc1f-4fd5-a827-0093e44678bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn import model_selection \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df = pd.read_csv(\"imdb.csv\")\n",
    "    df.sentiment = df.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)\n",
    "    \n",
    "    df[\"kfold\"] =-1\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    y = df.sentiment.values\n",
    "    \n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    for f,(t_,v_) in enumerate(kf.split(X=df,y=y)):\n",
    "        df.loc[v_,\"kfold\"] =f\n",
    "        \n",
    "    df.to_csv(\"imdb_folds.csv\",index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d412c1e-3807-4b54-965c-f68df5afbf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I enjoyed Erkan &amp; Stefan  a cool and fast sto...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The only reason I rated this film as 2 is beca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of those movies where you take bets on who...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This series was just like what you would expec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While many people found this film simply too s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  kfold\n",
       "0  I enjoyed Erkan & Stefan  a cool and fast sto...          1      0\n",
       "1  The only reason I rated this film as 2 is beca...          0      0\n",
       "2  One of those movies where you take bets on who...          0      0\n",
       "3  This series was just like what you would expec...          1      0\n",
       "4  While many people found this film simply too s...          1      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_folds = pd.read_csv(\"imdb_folds.csv\")\n",
    "movies_folds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489da936-8fa6-4c49-b61e-92b2edca42c0",
   "metadata": {},
   "source": [
    "There is one additional features called kfold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d80b34-30f0-44b5-8d1b-db8366a392d4",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df02e1b9-4c6f-45d3-9a80-54ba80804a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sentence_to_vec(s,embedding_dict,stop_words,tokenizer):\n",
    "    words =str(s).lower()\n",
    "    words =tokenizer(words)\n",
    "    words = [ w for w in words if w not in stop_words]\n",
    "    \n",
    "    words = [w for w in words if w.alpha()]\n",
    "    \n",
    "    M =[]\n",
    "    for w in words:\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "            \n",
    "    if len(M)==0:\n",
    "        return np.zeros(300)\n",
    "    M = np.array(M)\n",
    "    v = M.sum()\n",
    "\n",
    "    \n",
    "    return v/np.sqrt((v**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db9391-e1a1-464f-b058-bc63f2f04a06",
   "metadata": {},
   "source": [
    "## Create Dataset in pytorch based on model in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fdea46-f836-42f6-a9ac-07a004270f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "class IMDBDataset:\n",
    "    def __init__(self,reviews,targets):\n",
    "        self.reviews =reviews\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        review =self.reviews[item,:]\n",
    "        target =self.target[item]\n",
    "        \n",
    "        return {\n",
    "            \"review\": torch.tensor(review,dtype=torch.long),\n",
    "            \"target\": torch.tensor(target,dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d16f9-1128-4b01-8a43-496b10b7ff67",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05794d1c-99a8-4696-8c26-90f682ca3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,embedding_matrix):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        num_words =embedding_matrix.shape[0]\n",
    "        \n",
    "        embed_dim= embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = num_words,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "                embedding_matrix,\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.embedding.weight.requires_grad=False\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            128,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(512,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "            \n",
    "        x,_ = self.lstm(x)\n",
    "        avg_pool =torch.mean(x,1)\n",
    "        max_pool, _ = torch.max(x,1)\n",
    "            \n",
    "        out = torch.cat((avg_pool,maxpool),1)\n",
    "        out = self.out(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec171247-8cbe-445b-a4d2-546a108ffa5b",
   "metadata": {},
   "source": [
    "## Create Training Function for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5640cef-57f3-47af-825c-c55c047bef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,model,optimizer,device):\n",
    "    model.train()\n",
    "    \n",
    "    for data in data_loader:\n",
    "        reviews = data[\"review\"]\n",
    "        targets = data[\"target\"]\n",
    "    \n",
    "        reviews = reviews.to(device,dtype=torch.long)\n",
    "        targets = targets.to(device,dtype=torch.float)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(reviews)\n",
    "        \n",
    "        loss =nn.BCEWithLogitsLoss()(\n",
    "            predictions,\n",
    "            targets.view(-1,1)\n",
    "        )\n",
    "        \n",
    "        loss.bakward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4b998-eb34-4acb-8689-6a44e2baca47",
   "metadata": {},
   "source": [
    "## Create Evaluation for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d895464-977e-4435-b3fd-7116e2df4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader,model,device):\n",
    "    final_predictions =[]\n",
    "    final_targets = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            reviews =data[\"review\"]\n",
    "            targets =data[\"target\"]\n",
    "            reviews = reviews.to(device,dtype=torch.long)\n",
    "            targets = targets.to(device,dtype=torch.long)\n",
    "            \n",
    "            predictions = model(reviews)\n",
    "            \n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data[\"target\"].cpu().numpy.tolist()\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "            \n",
    "    return final_predictions,final_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a1842-2771-446c-8601-ad9dad3f2fb3",
   "metadata": {},
   "source": [
    "## Word Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918e08b-2c43-424f-b831-52cdd0d6312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n",
      "Loading Embeddings\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "#from tensorflow.keras import \n",
    "import tensorflow as tf\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(\n",
    "        fname,\n",
    "        \"r\",\n",
    "        encoding=\"utf-8\",\n",
    "        newline=\"\\n\",\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    n,d = map(int,fin.readline().split())\n",
    "    data ={}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float,tokens[1:]))\n",
    "    return data\n",
    "\n",
    "def create_embedding_matrix(world_index,embedding_dict):\n",
    "    embedding_matrix = np.zeros((len(word_index)+1,300))\n",
    "    for word , i in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_dict[i] = embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "def run(df,fold):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold ==fold].reset_index(drop=True)\n",
    "    \n",
    "    print(\"Fitting tokenizer\")\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(df.review.values.tolist())\n",
    "    \n",
    "    xtrain = tokenizer.texts_to_sequences(train_df.review.values)\n",
    "    \n",
    "    xtest = tokenizer.texts_to_sequences(valid_df.review.values)\n",
    "    \n",
    "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        xtrain,maxlen=128\n",
    "    )\n",
    "    \n",
    "    xtest = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        xtest,maxlen=128\n",
    "    )\n",
    "    \n",
    "    train_dataset = IMDBDataset(\n",
    "        reviews =xtrain,\n",
    "        targets = train_df.sentiment.values\n",
    "    )\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size =16,\n",
    "        num_workers=2\n",
    "    \n",
    "    )\n",
    "    \n",
    "    valid_dataset =IMDBDataset(\n",
    "        reviews =xtest,\n",
    "        targets = valid_df.sentiment.values\n",
    "    )\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size =8,\n",
    "        num_workers=1\n",
    "    )\n",
    "    \n",
    "    print(\"Loading Embeddings\")\n",
    "    # you can suit based on where you put your vec fasttext\n",
    "    embedding_dict = load_vectors(\"crawl-300d-2M.vec/crawl-300d-2M.vec\")\n",
    "    embedding_matrix = create_embedding_matrix(\n",
    "        tokenizer.word_index,embedding_dict\n",
    "    )\n",
    "    \n",
    "    device =torch.device(\"cuda\")\n",
    "    \n",
    "    model =LSTM(embedding_matrix)\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    \n",
    "    best_accuracy =0\n",
    "    early_stopping_counter =0\n",
    "    for epoch in range(10):\n",
    "        train(train_data_loader,model,optimizer,device)\n",
    "        outputs,targets = evaluate(valid_data_loader,model,device)\n",
    "        outputs = np.array(outputs) >=0.5\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(targets,outputs)\n",
    "        print(f\"{fold}, Epoch {epoch}, Accuracy Score ={accuracy}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "        if early_stopping_counter > 2:\n",
    "            break\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"imdb_folds.csv\")\n",
    "    run(df,0)\n",
    "    run(df,1)\n",
    "    run(df,2)\n",
    "    run(df,3)\n",
    "    run(df,4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aada6f-9327-4d9c-94fb-34d6f0ba6c46",
   "metadata": {},
   "source": [
    "The choice of Machine learning algorithms will determine the quality of our predictions score. However, The simple model will also determine how fast the training process compared to state of art of ML algorithms. If we have laptop/computer with better GPU can help the training process. But in real life, Simple models with better preprocessing will have predictions score that is not too dfferent with newest ML algorithms. So, It is better to discuss with the stakeholder for improving the models based on business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af02b7-17e9-458a-8779-563b0b7cdca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
