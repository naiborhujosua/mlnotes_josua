{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# \"UW-Madison GI Tract Image Segmentation\"\n> \"Helping Radiation Oncologist by tracking healthy organs in Medical scans to improve cancer treatment\"\n\n- toc:true\n- branch: master\n- badges: true\n- comments: true\n- author: kaggle\n- categories: [kaggle, imagesegmentation, keras,dicecoefficient, IOUcoefficient]\n\n","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"{{site.baseurl}}/images/uwmadison.jpg\" alt =\"source: UW Madison Kaggle Competition \"> </center>\n\nThis is a comprehensive End-to End Deep Learning Models Implementation using Keras to do image segmentation based on one of Kaggle Competition provided by `Wisconsin Carbone Cancer Center`. I am interested in this competition to improve my skills related to the implemetation of AI in Health Sector. I will explain my approach about the dataset provided by the organizer. I also explain the some metrics which are available for evaluating the success of Image Segmentation process in MRI scan dataset. I used `KERAS` and some pretrained `segmentation_models` using Efficientbnet7 for running the training process and do the evaluation based on Dice coefficient, IoU coefficient and the loss for this problem statement.","metadata":{}},{"cell_type":"markdown","source":"## Introduction\nIn 2019, There are about 5 million people were diagnosed with cancer of the gastro-intestinal tract worldwide. Half of these patients are eligible for radiation therapy by delivering 10-15 minutes a day for 1-6 weeks. The radiation system works by detecting parts of the tumor from the patient's body by giving high doses of radiation using X-ray beams to part of the body that consists of tumors while avoiding the stomach and intestines in order to avoid the spread of the tumor. Recent technology called MR-Linacs is only able to visualize the daily position of tumors and intestines which can vary day by day. However, The radiation oncologists must manually outline the position of the stomach and intestines so that they can manage the direction of the x-ray beams to the tumor and avoid the stomach and intestines which is labor-intensive and takes time that can prolong treatments from 15 minutes a day to an hour a day. The problem can be difficult for patients to tolerate to wait for the result about the recent condition of their health. Deep Learning, as a subset of Machine Learning through Computer Vision, can detect the segmentation process to distinguish parts of the tumor, stomach, and intestines. This is going to be a game-changer for patients and radiation oncologists to accelerate the treatment of patients in order to detect the condition of the patients to have more effective treatment. I will try solving this problem by using keras as deep learning framework library and do an inference based on the model built using U-Net Architecture for image segmentation by using dataset provided by the UW-Madison Carbone Cancer Center.","metadata":{}},{"cell_type":"code","source":"#  You can take a look at the process of Cancer treatment by using MRI\nfrom IPython.display import YouTubeVideo\n# Full Link: https://www.youtube.com/watch?v=knUTrvJLeEg\n\nYouTubeVideo('knUTrvJLeEg', width=700, height=400)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:02.598294Z","iopub.execute_input":"2022-06-13T14:15:02.600494Z","iopub.status.idle":"2022-06-13T14:15:02.838385Z","shell.execute_reply.started":"2022-06-13T14:15:02.600379Z","shell.execute_reply":"2022-06-13T14:15:02.837479Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## The Dataset","metadata":{}},{"cell_type":"markdown","source":"### Stomach, Large Bowel, Small Bowel\n\nThe `class` inside the `train.csv` file has 3 distinct values: large bowel, small bowel, stomach. These are all part of the digestive system. The bowels (small and large intestine) are responsible for breaking down food and absorbing the nutrients.\n\n<center><img src=\"https://i.imgur.com/v2fobvp.png\" width=700></center>","metadata":{}},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport cv2\nimport gc\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom typing import Optional\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\nfrom tensorflow import keras\nimport tensorflow as tf\nimport keras\nfrom keras.models import load_model, save_model\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.losses import binary_crossentropy\nfrom keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nfrom keras import backend as K\n#from tensorflow.python.keras import backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Input\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\nimport matplotlib as mpl","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:02.842256Z","iopub.execute_input":"2022-06-13T14:15:02.842580Z","iopub.status.idle":"2022-06-13T14:15:09.120043Z","shell.execute_reply.started":"2022-06-13T14:15:02.842554Z","shell.execute_reply":"2022-06-13T14:15:09.118966Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCH = 10\nn_splits = 5\nfold_selected = 2  # 1,...,5\n\nTRAIN_ROOT_DIR = \"../input/uw-madison-gi-tract-image-segmentation/\"\nTEST_ROOT_DIR = \"../input/uw-madison-gi-tract-image-segmentation/test\"","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:09.121343Z","iopub.execute_input":"2022-06-13T14:15:09.122033Z","iopub.status.idle":"2022-06-13T14:15:09.129668Z","shell.execute_reply.started":"2022-06-13T14:15:09.121993Z","shell.execute_reply":"2022-06-13T14:15:09.128535Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df_original = pd.read_csv(TRAIN_ROOT_DIR + 'train.csv')\nprint(train_df_original.shape)\ntrain_df_original.sample(30)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:09.132027Z","iopub.execute_input":"2022-06-13T14:15:09.132751Z","iopub.status.idle":"2022-06-13T14:15:09.708962Z","shell.execute_reply.started":"2022-06-13T14:15:09.132708Z","shell.execute_reply":"2022-06-13T14:15:09.708107Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(f\"There are about { round(train_df_original['segmentation'].isnull().sum()/train_df_original.shape[0],2) }% of pictures without segmentation which is quite a lot\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:09.710274Z","iopub.execute_input":"2022-06-13T14:15:09.711058Z","iopub.status.idle":"2022-06-13T14:15:09.727250Z","shell.execute_reply.started":"2022-06-13T14:15:09.711023Z","shell.execute_reply":"2022-06-13T14:15:09.726416Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We can see that the segmentation images taken from MRI scanners shows an imbalanced data among pictures that shows indices segmentation and not having indices segmentation(NaN). This is not good for training the data without paying attention to imbalanced Dataset. We use StratifieldKFold validation to compensate the balance of this data.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(28, 12))\ntrain_df_original['class'].value_counts(normalize=True).plot.pie()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:09.728514Z","iopub.execute_input":"2022-06-13T14:15:09.729365Z","iopub.status.idle":"2022-06-13T14:15:09.967539Z","shell.execute_reply.started":"2022-06-13T14:15:09.729327Z","shell.execute_reply":"2022-06-13T14:15:09.966555Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(TRAIN_ROOT_DIR + 'sample_submission.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:09.969165Z","iopub.execute_input":"2022-06-13T14:15:09.970246Z","iopub.status.idle":"2022-06-13T14:15:09.993186Z","shell.execute_reply.started":"2022-06-13T14:15:09.970203Z","shell.execute_reply":"2022-06-13T14:15:09.992257Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if len(test_df) == 0:\n    DEBUG=True\n    # test_df=train_df_original.iloc[:300, :]\n    test_df=pd.read_csv(TRAIN_ROOT_DIR + 'train.csv').iloc[:300, :]\n    test_df['segmentation'] = ''\n    test_df = test_df.rename(columns={'segmentation' : 'prediction'})\nelse:\n    DEBUG=False\n    \nsubmission = test_df.copy()\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:09.998123Z","iopub.execute_input":"2022-06-13T14:15:09.998600Z","iopub.status.idle":"2022-06-13T14:15:10.314625Z","shell.execute_reply.started":"2022-06-13T14:15:09.998566Z","shell.execute_reply":"2022-06-13T14:15:10.313686Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_df_original.head()\ntrain_df_original.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:10.316188Z","iopub.execute_input":"2022-06-13T14:15:10.316568Z","iopub.status.idle":"2022-06-13T14:15:10.322937Z","shell.execute_reply.started":"2022-06-13T14:15:10.316533Z","shell.execute_reply":"2022-06-13T14:15:10.322147Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def df_preparation(df, subset=\"train\", DEBUG=False):\n    df[\"case\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[0].replace(\"case\", \"\")))\n    df[\"day\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[1].replace(\"day\", \"\")))\n    df[\"slice\"] = df[\"id\"].apply(lambda x: x.split(\"_\")[3])\n\n    if (subset == \"train\") or (DEBUG):\n        DIR = TRAIN_ROOT_DIR + \"train\"\n    else:\n        DIR = TEST_ROOT_DIR\n\n    \"\"\"Also another cool feature of `glob.glob`, if you want to avoid `/*/*/*` type of pattern( as not all datasets follows certain pattern hence we might not be able find how many `/` to use) you can use,\n    glob.glob(`/kaggle/input/uw-madison-gi-tract-image-segmentation/train/**/*png', recursive=True) \"\"\"\n    all_images = glob(os.path.join(DIR, \"**\", \"*.png\"), recursive=True)\n    print(\"all_images length \", len(all_images))  # 38496\n\n    x = all_images[0].rsplit(\"/\", 4)[0]\n    # print('x ', x)\n    # ../../input/uw-madison-gi-tract-image-segmentation/train\n\n    # Now I need a column named 'path' holding the full path of all the images in this dataframe\n    # But I can not simply create them with the below kind of line\n    # df['path'] = all_images\n    # Because each image is repeated. And so if I do the above line directly I will get below error\n    # ValueError: Length of values (38496) does not match length of index (115488)\n    # So the solution is to create a temporary dataframe > then merge this temp dataframe with the original df > then delete the temp df\n\n    # To make a column which will have th full pathname of all the iamges\n    # Hence I have to build the full path name. Below is an example.\n    # '../../input/uw-madison-gi-tract-image-segmentation/train/case44/case44_day0/scans/slice_0085_266_266_1.50_1.50.png',\n    path_partial_list = []\n    for i in range(0, df.shape[0]):\n        path_partial_list.append(\n            os.path.join(\n                x,\n                \"case\" + str(df[\"case\"].values[i]),\n                \"case\"\n                + str(df[\"case\"].values[i])\n                + \"_\"\n                + \"day\"\n                + str(df[\"day\"].values[i]),\n                \"scans\",\n                \"slice_\" + str(df[\"slice\"].values[i]),\n            )\n        )\n    df[\"path_partial\"] = path_partial_list\n\n    path_partial_list = []\n    for i in range(0, len(all_images)):\n        path_partial_list.append(str(all_images[i].rsplit(\"_\", 4)[0]))\n\n    tmp_df = pd.DataFrame()\n    tmp_df[\"path_partial\"] = path_partial_list\n    tmp_df[\"path\"] = all_images\n\n    df = df.merge(tmp_df, on=\"path_partial\").drop(columns=[\"path_partial\"])\n\n    df[\"width\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[1]))\n    df[\"height\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\", 4)[2]))\n\n    del x, path_partial_list, tmp_df\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:10.327123Z","iopub.execute_input":"2022-06-13T14:15:10.327842Z","iopub.status.idle":"2022-06-13T14:15:10.344530Z","shell.execute_reply.started":"2022-06-13T14:15:10.327799Z","shell.execute_reply":"2022-06-13T14:15:10.343603Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df = df_preparation(train_df_original, subset=\"train\")\n\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:10.346133Z","iopub.execute_input":"2022-06-13T14:15:10.346515Z","iopub.status.idle":"2022-06-13T14:15:18.955730Z","shell.execute_reply.started":"2022-06-13T14:15:10.346477Z","shell.execute_reply":"2022-06-13T14:15:18.954954Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Checking a single full path name\nprint(train_df['path'][0])\nprint(train_df['path'][1])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:18.957136Z","iopub.execute_input":"2022-06-13T14:15:18.957729Z","iopub.status.idle":"2022-06-13T14:15:18.966532Z","shell.execute_reply.started":"2022-06-13T14:15:18.957690Z","shell.execute_reply":"2022-06-13T14:15:18.965286Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"test_df=df_preparation(test_df, subset=\"test\", DEBUG=True)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:18.968344Z","iopub.execute_input":"2022-06-13T14:15:18.968834Z","iopub.status.idle":"2022-06-13T14:15:19.958924Z","shell.execute_reply.started":"2022-06-13T14:15:18.968794Z","shell.execute_reply":"2022-06-13T14:15:19.958048Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def df_rearrange_for_3_segmentation_classes(df, subset=\"train\"):\n    df_restructured = pd.DataFrame({\"id\": df[\"id\"][::3]})\n\n    if subset == \"train\":\n        df_restructured[\"large_bowel\"] = df[\"segmentation\"][::3].values\n        df_restructured[\"small_bowel\"] = df[\"segmentation\"][1::3].values\n        df_restructured[\"stomach\"] = df[\"segmentation\"][2::3].values\n\n    df_restructured[\"path\"] = df[\"path\"][::3].values\n    df_restructured[\"case\"] = df[\"case\"][::3].values\n    df_restructured[\"day\"] = df[\"day\"][::3].values\n    df_restructured[\"slice\"] = df[\"slice\"][::3].values\n    df_restructured[\"width\"] = df[\"width\"][::3].values\n    df_restructured[\"height\"] = df[\"height\"][::3].values\n\n    df_restructured = df_restructured.reset_index(drop=True)\n    df_restructured = df_restructured.fillna(\"\")\n    if subset == \"train\":\n        df_restructured[\"count\"] = np.sum(\n            df_restructured.iloc[:, 1:4] != \"\", axis=1\n        ).values\n\n    return df_restructured","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:19.960227Z","iopub.execute_input":"2022-06-13T14:15:19.961382Z","iopub.status.idle":"2022-06-13T14:15:19.971649Z","shell.execute_reply.started":"2022-06-13T14:15:19.961338Z","shell.execute_reply":"2022-06-13T14:15:19.970813Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df_rearranged=df_rearrange_for_3_segmentation_classes(train_df, subset=\"train\")\ntrain_df_rearranged.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:19.972853Z","iopub.execute_input":"2022-06-13T14:15:19.975346Z","iopub.status.idle":"2022-06-13T14:15:20.064934Z","shell.execute_reply.started":"2022-06-13T14:15:19.975303Z","shell.execute_reply":"2022-06-13T14:15:20.064025Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Remove misslabeled training data based on distcussion forum on the discusion forum at \ntrain_df_rearranged = train_df_rearranged[(train_df['case']!=7)|(train_df['day']!=0)].reset_index(drop=True)\n\ntrain_df_rearranged = train_df_rearranged[(train_df['case']!=81)|(train_df['day']!=30)].reset_index(drop=True)\n\ntrain_df_rearranged = train_df_rearranged[(train_df['case']!=138)|(train_df['day']!=00)].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:20.066129Z","iopub.execute_input":"2022-06-13T14:15:20.066884Z","iopub.status.idle":"2022-06-13T14:15:20.110536Z","shell.execute_reply.started":"2022-06-13T14:15:20.066827Z","shell.execute_reply":"2022-06-13T14:15:20.109620Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:20.111873Z","iopub.execute_input":"2022-06-13T14:15:20.112234Z","iopub.status.idle":"2022-06-13T14:15:20.279767Z","shell.execute_reply.started":"2022-06-13T14:15:20.112200Z","shell.execute_reply":"2022-06-13T14:15:20.278844Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def plot_bar(df):\n    plt.figure(figsize=(12, 6))\n    bar = plt.bar([1, 2, 3], 100 * np.mean(df.iloc[:, 1:4] != \"\", axis=0))\n    plt.title(\"Percent Training Images with Mask\", fontsize=16)\n    plt.ylabel(\"Percent of Train images with mask\")\n    plt.xlabel(\"Class Types\")\n    # labels = [\"large bowel\", \"small bowel\", \"stomach\"]\n    labels = [\"large_bowel\", \"small_bowel\", \"stomach\"]\n\n    for rect, lbl in zip(bar, labels):\n        height = rect.get_height()\n        plt.text(\n            rect.get_x() + rect.get_width() / 3,\n            height,\n            lbl,\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=12,\n        )\n\n    plt.ylim((0, 50))\n    plt.show()\n    \nplot_bar(train_df_rearranged)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:20.281265Z","iopub.execute_input":"2022-06-13T14:15:20.281977Z","iopub.status.idle":"2022-06-13T14:15:20.514709Z","shell.execute_reply.started":"2022-06-13T14:15:20.281936Z","shell.execute_reply":"2022-06-13T14:15:20.513938Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    s = mask_rle.split()\n    starts, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + length\n    img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = color\n    return img.reshape(shape)\n\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, df, batch_size=BATCH_SIZE, subset=\"train\", shuffle=False):\n        super().__init__()\n        self.df = df\n        self.shuffle = shuffle\n        self.subset = subset\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(df))\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.floor(len(self.df) / self.batch_size))\n\n    def on_epoch_end(self):\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n\n    \"\"\" __getitem__ returns a batch of images and masks \"\"\"\n\n    def __getitem__(self, index):\n        X = np.empty((self.batch_size, 224, 224, 3))  # Makes a 4-D Tensor\n        y = np.empty((self.batch_size, 224, 224, 3))  # Makes a 4-D Tensor\n\n        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n\n        for i, img_path in enumerate(self.df[\"path\"].iloc[indexes]):\n            # print(\"df['path'].iloc[indexes].shape \", self.df['path'].iloc[indexes].shape) # (16,)\n            # in above 'i' is just the counter. i.e. starts from 0 and goes upto the max length of all the rows\n            w = self.df[\"width\"].iloc[\n                indexes[i]\n            ]  # selects the row number of indexes[i]\n            h = self.df[\"height\"].iloc[indexes[i]]\n            img = self._load_grayscaled_img(img_path)  # shape: (128,128,1)\n            # print('img shape after _load_grayscaled_img ', img.shape) #(128, 128, 1)\n            # Now update X[i,] to be this image.\n            X[\n                i,\n            ] = img  # broadcast to shape: (128,128,3)\n            # As we know, that arr[1,] is equivalent to arr[1, :]\n            # As NumPy will automatically insert trailing slices for you\n\n            # print('X after ', X.shape) # (16, 128, 128, 3)\n            # The slice notation in the above line means -\n            # Set me the (i+1)th Row of X to be this image\n\n            if self.subset == \"train\":\n                for k, j in enumerate([\"large_bowel\", \"small_bowel\", \"stomach\"]):\n                    # Now 'j' will take each value from the above list\n                    # e.g. self.df['large_bowel']\n                    # and in my train_df_rearranged each of the [\"large_bowel\",\"small_bowel\",\"stomach\"]\n                    # column names contain RLE formatted segmentation data.\n                    rles = self.df[j].iloc[indexes[i]]\n                    # so the above line will actually be something like => self.df['stomach'].iloc[indexes[20]]\n                    # giving me the RLE data for that row and column\n                    # mask = rle_decode(rles, shape=(h, w, 1))\n                    # if all my utils method is in separate file then uncomment below\n                    mask = rle_decode(rles, shape=(h, w, 1))\n                    mask = cv2.resize(mask, (224, 224))\n                    y[i, :, :, k] = mask\n        if self.subset == \"train\":\n            return X, y\n        else:\n            return X\n\n    def _load_grayscaled_img(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n        img_size = (224, 224)\n        img = cv2.resize(img, img_size)\n        img = img.astype(np.float32) / 255.0\n        img = np.expand_dims(img, axis=-1)\n        return img\n\n    \"\"\"cv2.IMREAD_ANYDEPTH => If set, return 16-bit/32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit. \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:20.515904Z","iopub.execute_input":"2022-06-13T14:15:20.517010Z","iopub.status.idle":"2022-06-13T14:15:20.889522Z","shell.execute_reply.started":"2022-06-13T14:15:20.516962Z","shell.execute_reply":"2022-06-13T14:15:20.888609Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#taken from https://www.kaggle.com/code/dschettler8845/uwm-gi-tract-image-segmentation-eda#create_dataset\ndef plot_mask_with_color_patches(df, colors, labels):\n    list_indices_of_mask_random = list(\n        df[df[\"large_bowel\"] != \"\"].sample(BATCH_SIZE).index\n    )\n    list_indices_of_mask_random += list(\n        df[df[\"small_bowel\"] != \"\"].sample(BATCH_SIZE * 2).index\n    )\n    list_indices_of_mask_random += list(\n        df[df[\"stomach\"] != \"\"].sample(BATCH_SIZE * 3).index\n    )\n    # print('list_indices_of_mask_random ', list_indices_of_mask_random)\n    # It will be a list of indexes like [15176, 13709, 30423, ..., 12730]\n\n    batches_from_datagen = DataGenerator(\n        df[df.index.isin(list_indices_of_mask_random)], shuffle=True\n    )\n\n    num_rows = 6\n\n    fig = plt.figure(figsize=(10, 25))\n    gs = gridspec.GridSpec(nrows=num_rows, ncols=2)\n    patches = [\n        mpatches.Patch(color=colors[i], label=f\"{labels[i]}\")\n        for i in range(len(labels))\n    ]\n\n    cmap1 = mpl.colors.ListedColormap(colors[0])\n    cmap2 = mpl.colors.ListedColormap(colors[1])\n    cmap3 = mpl.colors.ListedColormap(colors[2])\n    \"\"\" The `matplotlib.colors.ListedColormap` class is used to create colarmap objects from a list of colors.\n    The class belongs to the `matplotlib.colors` module. This module is used for converting color or numbers arguments to RGBA or RGB and for mapping numbers to colors or color specification conversion in a 1-D array of colors also known as colormap.\n    This can be useful for directly indexing into colormap and it can also be used to create special colormaps for normal mapping. \"\"\"\n\n    for i in range(num_rows):\n        images, mask = batches_from_datagen[i]\n        # print('images.shape ', images.shape) # (16, 128, 128, 3)\n        # print('mask.shape ', mask.shape) # (16, 128, 128, 3)\n        \"\"\"\n        For each ID, we are going to create an image of shape [img height, img width, 3], where 3 (number of channels) are the 3 layers for each class:\n        * the first layer: large bowel\n        * the second layer: small bowel\n        * the third layer: stomach\n        \"\"\"\n        sample_img = images[0, :, :, 0]  # After this the shapes will be (128, 128)\n        mask1 = mask[0, :, :, 0]  # After this the shapes will be (128, 128)\n        mask2 = mask[0, :, :, 1]  # After this the shapes will be (128, 128)\n        mask3 = mask[0, :, :, 2]  # After this the shapes will be (128, 128)\n\n        ax0 = fig.add_subplot(gs[i, 0])  # i here is the row-counter which is 6\n        im = ax0.imshow(sample_img, cmap=\"bone\")\n\n        ax1 = fig.add_subplot(gs[i, 1])\n        if i == 0:\n            ax0.set_title(\"Image\", fontsize=15, weight=\"bold\", y=1.02)\n            ax1.set_title(\"Mask\", fontsize=15, weight=\"bold\", y=1.02)\n            plt.legend(\n                handles=patches,\n                bbox_to_anchor=(1.1, 0.65),\n                loc=2,\n                borderaxespad=0.4,\n                fontsize=14,\n                title=\"Mask Labels\",\n                title_fontsize=14,\n                edgecolor=\"black\",\n                facecolor=\"#c5c6c7\",\n            )\n\n        # print('mask1 ', mask1.shape) # (128, 128)\n        # print('mask2 ', mask2.shape) # (128, 128)\n        # print('mask3 ', mask3.shape) # (128, 128)\n        # print('np.ma.masked_where(mask1== False,  mask1) ', np.ma.masked_where(mask1== True,  mask1))\n        l0 = ax1.imshow(sample_img, cmap=\"bone\")\n        l1 = ax1.imshow(np.ma.masked_where(mask1 == False, mask1), cmap=cmap1, alpha=1)\n        l2 = ax1.imshow(np.ma.masked_where(mask2 == False, mask2), cmap=cmap2, alpha=1)\n        l3 = ax1.imshow(np.ma.masked_where(mask3 == False, mask3), cmap=cmap3, alpha=1)\n        # l1 = ax1.imshow(np.ma.masked_where(mask1== 0,  mask1),cmap=cmap1, alpha=1)\n        # l2 = ax1.imshow(np.ma.masked_where(mask2== 0,  mask2),cmap=cmap2, alpha=1)\n        # l3 = ax1.imshow(np.ma.masked_where(mask3== 0,  mask3),cmap=cmap3, alpha=1)\n        _ = [ax.set_axis_off() for ax in [ax0, ax1]]\n\n        colors = [im.cmap(im.norm(1)) for im in [l1, l2, l3]]","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:20.890742Z","iopub.execute_input":"2022-06-13T14:15:20.891132Z","iopub.status.idle":"2022-06-13T14:15:20.910066Z","shell.execute_reply.started":"2022-06-13T14:15:20.891071Z","shell.execute_reply":"2022-06-13T14:15:20.909262Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"colors = ['blue','green','red']\n\nlabels = [\"large_bowel\", \"small_bowel\", \"stomach\"]\n\nplot_mask_with_color_patches(train_df_rearranged, colors, labels)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:20.911321Z","iopub.execute_input":"2022-06-13T14:15:20.912372Z","iopub.status.idle":"2022-06-13T14:15:23.041530Z","shell.execute_reply.started":"2022-06-13T14:15:20.912317Z","shell.execute_reply":"2022-06-13T14:15:23.039659Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"We can now see the the segmentation among three labels( large bowel,small bowel, and stomach) to envision how we will predict these 3 classes with unseen test data in the private leaderboard or in real health industry where given the scan of image from patient taken from MRI scanner, the predictive system can predict these 3 classes so that radiation oncologist can fasten their treatment process.","metadata":{}},{"cell_type":"code","source":"train_df_rearranged.head(100)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.042588Z","iopub.execute_input":"2022-06-13T14:15:23.042965Z","iopub.status.idle":"2022-06-13T14:15:23.069215Z","shell.execute_reply.started":"2022-06-13T14:15:23.042931Z","shell.execute_reply":"2022-06-13T14:15:23.068145Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nfor fold, (_, val_idx) in enumerate(\n    skf.split(\n        X=train_df_rearranged,\n        y=train_df_rearranged[\"count\"],\n        groups=train_df_rearranged[\"case\"],\n    ),\n    1,\n):\n    train_df_rearranged.loc[val_idx, \"fold\"] = fold\n\ntrain_df_rearranged[\"fold\"] = train_df_rearranged[\"fold\"].astype(np.uint8)\n\ntrain_ids = train_df_rearranged[train_df_rearranged[\"fold\"] != fold_selected].index\nvalid_ids = train_df_rearranged[train_df_rearranged[\"fold\"] == fold_selected].index\n\nX_train = train_df_rearranged[train_df_rearranged.index.isin(train_ids)]\nX_valid = train_df_rearranged[train_df_rearranged.index.isin(valid_ids)]\n\ntrain_df_rearranged.groupby(\"fold\").size()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.071156Z","iopub.execute_input":"2022-06-13T14:15:23.071706Z","iopub.status.idle":"2022-06-13T14:15:23.267972Z","shell.execute_reply.started":"2022-06-13T14:15:23.071661Z","shell.execute_reply":"2022-06-13T14:15:23.267048Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_df_rearranged.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.269449Z","iopub.execute_input":"2022-06-13T14:15:23.269828Z","iopub.status.idle":"2022-06-13T14:15:23.284447Z","shell.execute_reply.started":"2022-06-13T14:15:23.269790Z","shell.execute_reply":"2022-06-13T14:15:23.283429Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_df_rearranged.groupby(['fold','count'])['id'].count()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.286264Z","iopub.execute_input":"2022-06-13T14:15:23.286739Z","iopub.status.idle":"2022-06-13T14:15:23.306888Z","shell.execute_reply.started":"2022-06-13T14:15:23.286702Z","shell.execute_reply":"2022-06-13T14:15:23.306045Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"experiment = False\nif experiment:\n    X_train = X_train[X_train.case.isin(X_train.case.unique()[:5])]\n    X_valid = X_valid[X_valid.case.isin(X_valid.case.unique()[:2])]\n    \nprint(X_train.shape)\nprint(X_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.308190Z","iopub.execute_input":"2022-06-13T14:15:23.308824Z","iopub.status.idle":"2022-06-13T14:15:23.318169Z","shell.execute_reply.started":"2022-06-13T14:15:23.308783Z","shell.execute_reply":"2022-06-13T14:15:23.316768Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_generator = DataGenerator(X_train, shuffle = True)\nval_generator = DataGenerator(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.320919Z","iopub.execute_input":"2022-06-13T14:15:23.321823Z","iopub.status.idle":"2022-06-13T14:15:23.328985Z","shell.execute_reply.started":"2022-06-13T14:15:23.321782Z","shell.execute_reply":"2022-06-13T14:15:23.328049Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"! pip install segmentation-models","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:23.334342Z","iopub.execute_input":"2022-06-13T14:15:23.335462Z","iopub.status.idle":"2022-06-13T14:15:35.569530Z","shell.execute_reply.started":"2022-06-13T14:15:23.335417Z","shell.execute_reply":"2022-06-13T14:15:35.568459Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"! pip install git+https://github.com/qubvel/segmentation_models","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:35.571065Z","iopub.execute_input":"2022-06-13T14:15:35.571463Z","iopub.status.idle":"2022-06-13T14:15:47.518768Z","shell.execute_reply.started":"2022-06-13T14:15:35.571420Z","shell.execute_reply":"2022-06-13T14:15:47.517313Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Training Process","metadata":{}},{"cell_type":"markdown","source":"In the training process, We divide the data into training and validation dataset so that we can measure how effective our predictive model to predict the 3 classes in the image segmentation. We use a few metrics which are widely used for evaluating the success of image segmentation problems. They are Dice Coefficient, IOU Coefficient and the loss for this multiclassification problem. You can check this article written by [Ekin Tiu](https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2) how he explained these concepts and why we use these metrics for image segmentation problems and the implemetation of these metrics using KERAS in a very easy way to understand.","metadata":{}},{"cell_type":"code","source":"#metrics\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef iou_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n    return iou\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(tf.cast(y_true, tf.float32), y_pred) + 0.5 * dice_loss(tf.cast(y_true, tf.float32), y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:47.524493Z","iopub.execute_input":"2022-06-13T14:15:47.526900Z","iopub.status.idle":"2022-06-13T14:15:47.550647Z","shell.execute_reply.started":"2022-06-13T14:15:47.526038Z","shell.execute_reply":"2022-06-13T14:15:47.549521Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"import segmentation_models as sm\n\nsm.set_framework('tf.keras')\n\nsm.framework()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:47.558017Z","iopub.execute_input":"2022-06-13T14:15:47.561600Z","iopub.status.idle":"2022-06-13T14:15:47.877873Z","shell.execute_reply.started":"2022-06-13T14:15:47.561552Z","shell.execute_reply":"2022-06-13T14:15:47.876681Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from segmentation_models import Unet\nfrom segmentation_models.utils import set_trainable\n\nmodel = Unet('efficientnetb7', input_shape=(224, 224, 3), classes=3, activation='sigmoid', encoder_weights = 'imagenet' )\nmodel.compile(optimizer = 'adam', loss=bce_dice_loss, metrics=[dice_coef, iou_coef])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:47.879470Z","iopub.execute_input":"2022-06-13T14:15:47.880342Z","iopub.status.idle":"2022-06-13T14:15:59.559548Z","shell.execute_reply.started":"2022-06-13T14:15:47.880296Z","shell.execute_reply":"2022-06-13T14:15:59.558618Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'UNET_model',\n    monitor = 'val_loss',\n    verbose=1,\n    save_best_only=True,\n    mode = 'auto'\n)\n\nearly_stopping = EarlyStopping(\n    patience = 5,\n    min_delta = 0.0001,\n    restore_best_weights= True\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:59.561011Z","iopub.execute_input":"2022-06-13T14:15:59.561428Z","iopub.status.idle":"2022-06-13T14:15:59.567904Z","shell.execute_reply.started":"2022-06-13T14:15:59.561389Z","shell.execute_reply":"2022-06-13T14:15:59.566923Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_generator,\n    validation_data=val_generator,\n    callbacks=[checkpoint, early_stopping],\n    use_multiprocessing=False,\n    workers=4,\n    epochs=EPOCH\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T14:15:59.569504Z","iopub.execute_input":"2022-06-13T14:15:59.569914Z","iopub.status.idle":"2022-06-13T18:25:36.768173Z","shell.execute_reply.started":"2022-06-13T14:15:59.569873Z","shell.execute_reply":"2022-06-13T18:25:36.767275Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.to_csv(\"history_df.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:25:36.773408Z","iopub.execute_input":"2022-06-13T18:25:36.773711Z","iopub.status.idle":"2022-06-13T18:25:36.817136Z","shell.execute_reply.started":"2022-06-13T18:25:36.773684Z","shell.execute_reply":"2022-06-13T18:25:36.816068Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nplt.subplot(1, 3, 1)\nplt.plot(range(history.epoch[-1] + 1), history.history['loss'], label='Train Loss' )\nplt.plot(range(history.epoch[-1] + 1), history.history['val_loss'], label='Validation Loss' )\nplt.title('Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Losses')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(range(history.epoch[-1] + 1), history.history['dice_coef'], label='Train Dice Coeff' )\nplt.plot(range(history.epoch[-1] + 1), history.history['val_dice_coef'], label='Validation Dice Coef' )\nplt.title('Dice Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Dice Coef')\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.plot(range(history.epoch[-1] + 1), history.history['iou_coef'], label='Train IoU Coeff' )\nplt.plot(range(history.epoch[-1] + 1), history.history['val_iou_coef'], label='Validation IoU Coef' )\nplt.title('IoU Loss')\nplt.xlabel('Epochs')\nplt.ylabel('IoU Coef')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:25:36.818460Z","iopub.execute_input":"2022-06-13T18:25:36.821015Z","iopub.status.idle":"2022-06-13T18:25:37.347404Z","shell.execute_reply.started":"2022-06-13T18:25:36.820983Z","shell.execute_reply":"2022-06-13T18:25:37.346629Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"We can see from these graphs, show a good result where we can decrease the loss and imcrease the value of dice coefficient and IoU coefficient eventhough there are some rooms where we can improve this model. By implementating data augmentation. Due to lack of compute machine of using GPU in my local machine. so i give the ide how to improve the model for getting better result and decrease the loss as well.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Model","metadata":{}},{"cell_type":"code","source":"pred_batches = DataGenerator(X_valid.iloc[200:208, :], batch_size =1, subset = 'train', shuffle = True)\n\npreds = model.predict(pred_batches, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:25:37.348821Z","iopub.execute_input":"2022-06-13T18:25:37.349306Z","iopub.status.idle":"2022-06-13T18:25:44.108943Z","shell.execute_reply.started":"2022-06-13T18:25:37.349269Z","shell.execute_reply":"2022-06-13T18:25:44.108135Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"Threshold = 0.5\n# Visualizing\nfig = plt.figure(figsize=(10, 25))\ngs = gridspec.GridSpec(nrows=8, ncols=3)\ncolors = ['yellow','green','red']\nlabels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\npatches = [ mpatches.Patch(color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n\ncmap1 = mpl.colors.ListedColormap(colors[0])\ncmap2 = mpl.colors.ListedColormap(colors[1])\ncmap3= mpl.colors.ListedColormap(colors[2])\n\nfor i in range(8):\n    images, mask = pred_batches[i]\n    sample_img=images[0,:,:,0]\n    mask1=mask[0,:,:,0]\n    mask2=mask[0,:,:,1]\n    mask3=mask[0,:,:,2]\n    \n    pre=preds[i]\n    predict1=pre[:,:,0]\n    predict2=pre[:,:,1]\n    predict3=pre[:,:,2]\n    \n    predict1= (predict1 > Threshold).astype(np.float32)\n    predict2= (predict2 > Threshold).astype(np.float32)\n    predict3= (predict3 > Threshold).astype(np.float32)\n    \n    ax0 = fig.add_subplot(gs[i, 0])\n    im = ax0.imshow(sample_img, cmap='bone')\n    ax0.set_title(\"Image\", fontsize=12, y=1.01)\n    #--------------------------\n    ax1 = fig.add_subplot(gs[i, 1])\n    ax1.set_title(\"Mask\", fontsize=12,  y=1.01)\n    l0 = ax1.imshow(sample_img, cmap='bone')\n    l1 = ax1.imshow(np.ma.masked_where(mask1== False,  mask1),cmap=cmap1, alpha=1)\n    l2 = ax1.imshow(np.ma.masked_where(mask2== False,  mask2),cmap=cmap2, alpha=1)\n    l3 = ax1.imshow(np.ma.masked_where(mask3== False,  mask3),cmap=cmap3, alpha=1)\n    #--------------------------\n    ax2 = fig.add_subplot(gs[i, 2])\n    ax2.set_title(\"Predict\", fontsize=12, y=1.01)\n    l0 = ax2.imshow(sample_img, cmap='bone')\n    l1 = ax2.imshow(np.ma.masked_where(predict1== False,  predict1),cmap=cmap1, alpha=1)\n    l2 = ax2.imshow(np.ma.masked_where(predict2== False,  predict2),cmap=cmap2, alpha=1)\n    l3 = ax2.imshow(np.ma.masked_where(predict3== False,  predict3),cmap=cmap3, alpha=1)\n   \n\n    _ = [ax.set_axis_off() for ax in [ax0,ax1,ax2]]\n    colors = [im.cmap(im.norm(1)) for im in [l1,l2, l3]]\n    plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4,fontsize = 12,title='Mask Labels', title_fontsize=12, edgecolor=\"black\",  facecolor='#c5c6c7')","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:25:44.110653Z","iopub.execute_input":"2022-06-13T18:25:44.111023Z","iopub.status.idle":"2022-06-13T18:25:45.990806Z","shell.execute_reply.started":"2022-06-13T18:25:44.110987Z","shell.execute_reply":"2022-06-13T18:25:45.990025Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"You can see there are a few errors based on segmentation and it is quite good because we are going to do generalization where the model do not overfit.","metadata":{}},{"cell_type":"code","source":"custom_objects = custom_objects={\n    'dice_coef': dice_coef,\n    'iou_coef': iou_coef,\n    'bce_dice_loss': bce_dice_loss\n}\n\nmodel = load_model('./UNET_model', custom_objects=custom_objects)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:25:45.992230Z","iopub.execute_input":"2022-06-13T18:25:45.993065Z","iopub.status.idle":"2022-06-13T18:26:31.349782Z","shell.execute_reply.started":"2022-06-13T18:25:45.993020Z","shell.execute_reply":"2022-06-13T18:26:31.348924Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    \n    return ' '.join(str(x) for x in runs)\npred_batches = DataGenerator(test_df, batch_size = BATCH_SIZE, subset=\"test\", shuffle=False)\nnum_batches = int(len(test_df)/BATCH_SIZE)\n\nfor i in range(num_batches):\n    # Predict\n    preds = model.predict(pred_batches[i],verbose=0)     # shape: (16,128,128,3)\n    \n    # Rle encode\n    for j in range(BATCH_SIZE):\n        for k in range(3):\n            pred_img = cv2.resize(preds[j,:,:,k], (test_df.loc[i*BATCH_SIZE+j,\"width\"], test_df.loc[i*BATCH_SIZE+j,\"height\"]), interpolation=cv2.INTER_NEAREST) # resize probabilities to original shape\n            pred_img = (pred_img>0.5).astype(dtype='uint8')    # classify\n            submission.loc[3*(i*BATCH_SIZE+j)+k,'prediction'] = rle_encode(pred_img)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:26:31.351100Z","iopub.execute_input":"2022-06-13T18:26:31.351625Z","iopub.status.idle":"2022-06-13T18:26:41.690870Z","shell.execute_reply.started":"2022-06-13T18:26:31.351589Z","shell.execute_reply":"2022-06-13T18:26:41.690098Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nsubmission.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T18:26:41.692368Z","iopub.execute_input":"2022-06-13T18:26:41.692718Z","iopub.status.idle":"2022-06-13T18:26:41.712233Z","shell.execute_reply.started":"2022-06-13T18:26:41.692683Z","shell.execute_reply":"2022-06-13T18:26:41.711017Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}