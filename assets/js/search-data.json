{
  
    
        "post0": {
            "title": "Understanding GANs",
            "content": "Week 4 Assignment: GANs with Hands . For the last programming assignment of this course, you will build a Generative Adversarial Network (GAN) that generates pictures of hands. These will be trained on a dataset of hand images doing sign language. . The model you will build will be very similar to the DCGAN model that you saw in the second ungraded lab of this week. Feel free to review it in case you get stuck with any of the required steps. . Important: This colab notebook has read-only access so you won&#39;t be able to save your changes. If you want to save your work periodically, please click File -&gt; Save a Copy in Drive to create a copy in your account, then work from there. . Imports . import tensorflow as tf import tensorflow.keras as keras import matplotlib.pyplot as plt import numpy as np import urllib.request import zipfile from IPython import display . Utilities . def plot_results(images, n_cols=None): &#39;&#39;&#39;visualizes fake images&#39;&#39;&#39; display.clear_output(wait=False) n_cols = n_cols or len(images) n_rows = (len(images) - 1) // n_cols + 1 if images.shape[-1] == 1: images = np.squeeze(images, axis=-1) plt.figure(figsize=(n_cols, n_rows)) for index, image in enumerate(images): plt.subplot(n_rows, n_cols, index + 1) plt.imshow(image, cmap=&quot;binary&quot;) plt.axis(&quot;off&quot;) . Get the training data . You will download the dataset and extract it to a directory in your workspace. As mentioned, these are images of human hands performing sign language. . training_url = &quot;https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Resources/signs-training.zip&quot; training_file_name = &quot;signs-training.zip&quot; urllib.request.urlretrieve(training_url, training_file_name) # extract to local directory training_dir = &quot;/tmp&quot; zip_ref = zipfile.ZipFile(training_file_name, &#39;r&#39;) zip_ref.extractall(training_dir) zip_ref.close() . Preprocess the images . Next, you will prepare the dataset to a format suitable for the model. You will read the files, convert it to a tensor of floats, then normalize the pixel values. . BATCH_SIZE = 32 # mapping function for preprocessing the image files def map_images(file): &#39;&#39;&#39;converts the images to floats and normalizes the pixel values&#39;&#39;&#39; img = tf.io.decode_png(tf.io.read_file(file)) img = tf.dtypes.cast(img, tf.float32) img = img / 255.0 return img # create training batches filename_dataset = tf.data.Dataset.list_files(&quot;/tmp/signs-training/*.png&quot;) image_dataset = filename_dataset.map(map_images).batch(BATCH_SIZE) . Build the generator . You are free to experiment but here is the recommended architecture: . Dense: number of units should equal 7 * 7 * 128, input_shape takes in a list containing the random normal dimensions. random_normal_dimensions is a hyperparameter that defines how many random numbers in a vector you&#39;ll want to feed into the generator as a starting point for generating images. | . | Reshape: reshape the vector to a 7 x 7 x 128 tensor. | BatchNormalization | Conv2DTranspose: takes 64 units, kernel size is 5, strides is 2, padding is SAME, activation is selu. | BatchNormalization | Conv2DTranspose: 1 unit, kernel size is 5, strides is 2, padding is SAME, and activation is tanh. | . random_normal_dimensions = 32 ### START CODE HERE ### generator = keras.models.Sequential([ keras.layers.Dense(7 * 7 * 128, input_shape=[random_normal_dimensions]), keras.layers.Reshape([7, 7, 128]), keras.layers.BatchNormalization(), keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=&quot;selu&quot;), keras.layers.BatchNormalization(), keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=&quot;tanh&quot;), ]) generator.summary() ### END CODE HERE ### . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 6272) 206976 reshape (Reshape) (None, 7, 7, 128) 0 batch_normalization (BatchN (None, 7, 7, 128) 512 ormalization) conv2d_transpose (Conv2DTra (None, 14, 14, 64) 204864 nspose) batch_normalization_1 (Batc (None, 14, 14, 64) 256 hNormalization) conv2d_transpose_1 (Conv2DT (None, 28, 28, 1) 1601 ranspose) ================================================================= Total params: 414,209 Trainable params: 413,825 Non-trainable params: 384 _________________________________________________________________ . Build the discriminator . Here is the recommended architecture for the discriminator: . Conv2D: 64 units, kernel size of 5, strides of 2, padding is SAME, activation is a leaky relu with alpha of 0.2, input shape is 28 x 28 x 1 | Dropout: rate is 0.4 (fraction of input units to drop) | Conv2D: 128 units, kernel size of 5, strides of 2, padding is SAME, activation is LeakyRelu with alpha of 0.2 | Dropout: rate is 0.4. | Flatten | Dense: with 1 unit and a sigmoid activation | . discriminator = keras.models.Sequential([ keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=keras.layers.LeakyReLU(0.2), input_shape=[28, 28, 1]), keras.layers.Dropout(0.4), keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=&quot;SAME&quot;, activation=keras.layers.LeakyReLU(0.2)), keras.layers.Dropout(0.4), keras.layers.Flatten(), keras.layers.Dense(1, activation=&quot;sigmoid&quot;) ]) ### END CODE HERE ### . Compile the discriminator . Compile the discriminator with a binary_crossentropy loss and rmsprop optimizer. | Set the discriminator to not train on its weights (set its &quot;trainable&quot; field). | . discriminator.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;) discriminator.trainable = False ### END CODE HERE ### . Build and compile the GAN model . Build the sequential model for the GAN, passing a list containing the generator and discriminator. | Compile the model with a binary cross entropy loss and rmsprop optimizer. | . gan = keras.models.Sequential([generator, discriminator]) gan.compile(loss=&quot;binary_crossentropy&quot;, optimizer=&quot;rmsprop&quot;) ### END CODE HERE ### . Train the GAN . Phase 1 . real_batch_size: Get the batch size of the input batch (it&#39;s the zero-th dimension of the tensor) | noise: Generate the noise using tf.random.normal. The shape is batch size x random_normal_dimension | fake images: Use the generator that you just created. Pass in the noise and produce fake images. | mixed_images: concatenate the fake images with the real images. Set the axis to 0. | . | discriminator_labels: Set to 0. for fake images and 1. for real images. | Set the discriminator as trainable. | Use the discriminator&#39;s train_on_batch() method to train on the mixed images and the discriminator labels. | . Phase 2 . noise: generate random normal values with dimensions batch_size x random_normal_dimensions Use real_batch_size. | . | Generator_labels: Set to 1. to mark the fake images as real The generator will generate fake images that are labeled as real images and attempt to fool the discriminator. | . | Set the discriminator to NOT be trainable. | Train the GAN on the noise and the generator labels. | . def train_gan(gan, dataset, random_normal_dimensions, n_epochs=50): &quot;&quot;&quot; Defines the two-phase training loop of the GAN Args: gan -- the GAN model which has the generator and discriminator dataset -- the training set of real images random_normal_dimensions -- dimensionality of the input to the generator n_epochs -- number of epochs &quot;&quot;&quot; # get the two sub networks from the GAN model generator, discriminator = gan.layers for epoch in range(n_epochs): print(&quot;Epoch {}/{}&quot;.format(epoch + 1, n_epochs)) for real_images in dataset: ### START CODE HERE ### # infer batch size from the current batch of real images real_batch_size = real_images.shape[0] # Train the discriminator - PHASE 1 # Create the noise noise = tf.random.normal(shape=[real_batch_size, random_normal_dimensions]) # Use the noise to generate fake images fake_images = generator(noise) # Create a list by concatenating the fake images with the real ones mixed_images = tf.concat([fake_images, real_images], axis=0) # Create the labels for the discriminator # 0 for the fake images # 1 for the real images discriminator_labels = tf.constant([[0.]] * real_batch_size + [[1.]] * real_batch_size) # Ensure that the discriminator is trainable discriminator.trainable = True # Use train_on_batch to train the discriminator with the mixed images and the discriminator labels discriminator.train_on_batch(mixed_images, discriminator_labels) # Train the generator - PHASE 2 # create a batch of noise input to feed to the GAN noise = tf.random.normal(shape=[real_batch_size, random_normal_dimensions]) # label all generated images to be &quot;real&quot; generator_labels = tf.constant([[1.]] * real_batch_size) # Freeze the discriminator discriminator.trainable = False # Train the GAN on the noise with the labels all set to be true gan.train_on_batch(noise, generator_labels) ### END CODE HERE ### plot_results(fake_images, 16) plt.show() return fake_images . Run the training . For each epoch, a set of 31 images will be displayed onscreen. The longer you train, the better your output fake images will be. You will pick your best images to submit to the grader. . EPOCHS = 60 # run the training loop and collect images fake_images = train_gan(gan, image_dataset, random_normal_dimensions, EPOCHS) . Choose your best images to submit for grading! . Please visually inspect your 31 generated hand images. They are indexed from 0 to 30, from left to right on the first row on top, and then continuing from left to right on the second row below it. . Choose 16 images that you think look most like actual hands. | Use the append_to_grading_images() function, pass in fake_images and a list of the indices for the 16 images that you choose to submit for grading (e.g. append_to_grading_images(fake_images, [1, 4, 5, 6, 8... until you have 16 elements])). | . def append_to_grading_images(images, indexes): l = [] for index in indexes: if len(l) &gt;= 16: print(&quot;The list is full&quot;) break l.append(tf.squeeze(images[index:(index+1),...], axis=0)) l = tf.convert_to_tensor(l) return l . Please fill in the empty list (2nd parameter) with 16 indices indicating the images you want to submit to the grader. . grading_images = append_to_grading_images(fake_images, [0,1,2,3,4,5,6,7,13,14,15,16,18,20,24,26]) . Zip your selected images for grading . Please run the code below. This will save the images you chose to a zip file named my-signs.zip. . Please download this file from the Files explorer on the left. | Please return to the Coursera classroom and upload the zip file for grading. | . from PIL import Image from zipfile import ZipFile denormalized_images = grading_images * 255 denormalized_images = tf.dtypes.cast(denormalized_images, dtype = tf.uint8) file_paths = [] for this_image in range(0,16): i = tf.reshape(denormalized_images[this_image], [28,28]) im = Image.fromarray(i.numpy()) im = im.convert(&quot;L&quot;) filename = &quot;hand&quot; + str(this_image) + &quot;.png&quot; file_paths.append(filename) im.save(filename) with ZipFile(&#39;hands.zip&#39;, &#39;w&#39;) as zip: for file in file_paths: zip.write(file) . Congratulations on completing the final assignment of this course! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/gans/generator/discriminator/selu/reakyrelu/coursera/2022/06/28/building_GANs.html",
            "relUrl": "/gans/generator/discriminator/selu/reakyrelu/coursera/2022/06/28/building_GANs.html",
            "date": " • Jun 28, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Algorithmic Dimensionality Reduction",
            "content": "Ungraded lab: Algorithmic Dimensionality Reduction . Welcome, during this ungraded lab you are going to perform several algorithms that aim to reduce the dimensionality of data. This topic is very important because it is not uncommon that reduced models outperform the ones trained on the raw data because noise and redundant information are present in most datasets. This will also allow your models to train and make predictions faster, which might be really important depending on the problem you are working on. In particular you will: . Use Principal Component Analysis (PCA) to reduce the dimensionality of a dataset that classifies celestial bodies. | Use Single Value Decomposition (SVD) to create low level representations of images of handwritten digits. | Use Non-negative Matrix Factorization (NMF) to segment text into topics. | Let&#39;s get started! . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt . Principal Components Analysis - PCA . This is an unsupervised algorithm that creates linear combinations of the original features. PCA is a widely used technique for dimension reduction since it is fast and easy to implement. PCA aims to keep as much variance as possible from the original data in a lower dimensional space. It finds the best axis to project the data so that the variance of the projections is maximized. . In the lecture you saw PCA applied to the Iris dataset. This dataset has been used extensively to showcase PCA so here you are going to do something different. You are going to use the HTRU_2 dataset which describes several celestial objects and the idea is to be able to classify if an object is a pulsar star or not. . Begin by downloading the dataset: . !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip # Unzip it !unzip HTRU2.zip . Load the data into a dataframe for easier inspection: . data = pd.read_csv(&quot;HTRU_2.csv&quot;, names=[&#39;mean_ip&#39;, &#39;sd_ip&#39;, &#39;ec_ip&#39;, &#39;sw_ip&#39;, &#39;mean_dm&#39;, &#39;sd_dm&#39;, &#39;ec_dm&#39;, &#39;sw_dm&#39;, &#39;pulsar&#39;]) # Take a look at the data data.head() . This dataset has 8 numerical features (the &quot;pulsar&quot; column is the label). Now you are going to perform PCA reduce this 8th-dimensional input space to a lower dimensional one. . But first, scale the data. If you do an exploratory analysis of the data you will see that this dataset has a lot of outliers. Because of this you are going to use a RobustScaler, which scales features using statistics that are robust to outliers. . from sklearn.preprocessing import RobustScaler # Split features from labels features = data[[col for col in data.columns if col != &quot;pulsar&quot;]] labels = data[&quot;pulsar&quot;] # Scale data robust_data = RobustScaler().fit_transform(features) . Now perform PCA using sklearn. In this first iteration you are going to create a principal component for each one of the features so there is no dimensionality reduction: . from sklearn.decomposition import PCA # Instantiate PCA without specifying number of components pca_all = PCA() # Fit to scaled data pca_all.fit(robust_data) # Save cumulative explained variance cum_var = (np.cumsum(pca_all.explained_variance_ratio_)) n_comp = [i for i in range(1, pca_all.n_components_ + 1)] # Plot cumulative variance ax = sns.pointplot(x=n_comp, y=cum_var) ax.set(xlabel=&#39;number of principal components&#39;, ylabel=&#39;cumulative explained variance&#39;) plt.show() . Wow! With just 3 components almost all of the variance of the original data is explained! This makes you think that there were some highly correlated features in the original data. . Let&#39;s plot the first 3 principal components: . from mpl_toolkits.mplot3d import Axes3D # Instantiate PCA with 3 components pca_3 = PCA(3) # Fit to scaled data pca_3.fit(robust_data) # Transform scaled data data_3pc = pca_3.transform(robust_data) # Render the 3D plot fig = plt.figure(figsize=(15,15)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.scatter(data_3pc[:, 0], data_3pc[:, 1], data_3pc[:, 2], c=labels, cmap=plt.cm.Set1, edgecolor=&#39;k&#39;, s=25, label=data[&#39;pulsar&#39;]) ax.legend([&quot;non-pulsars&quot;], fontsize=&quot;large&quot;) ax.set_title(&quot;First three PCA directions&quot;) ax.set_xlabel(&quot;1st principal component&quot;) ax.w_xaxis.set_ticklabels([]) ax.set_ylabel(&quot;2nd principal component&quot;) ax.w_yaxis.set_ticklabels([]) ax.set_zlabel(&quot;3rd principal component&quot;) ax.w_zaxis.set_ticklabels([]) plt.show() . It is possible to visualize a plane that would be able to separate both classes since non-pulsars tend to group on the edge of this surface while pulsars are mostly located on the inner side of the surface. . In this case it is reasonable to think that the dimension can be reduced even more since with 2 principal components more than 95% of the variance of the original data is explained. Now let&#39;s plot just the first two principal components: . pca_2 = PCA(2) # Fit and transform scaled data pca_2.fit(robust_data) data_2pc = pca_2.transform(robust_data) # Render the 2D plot ax = sns.scatterplot(x=data_2pc[:,0], y=data_2pc[:,1], hue=labels, palette=sns.color_palette(&quot;muted&quot;, n_colors=2)) ax.set(xlabel=&#39;1st principal component&#39;, ylabel=&#39;2nd principal component&#39;, title=&#39;First two PCA directions&#39;) plt.show() . Even in 2D the 2 classes look linearly separable (not perfectly, of course) but this is quite remarkable considering that the initial space was 8th dimensional. . Using PCA you&#39;ve successfully reduced the dimensionality from 8 to 2 while maintaining a lot of the variance of the original data! . Singular Value Decomposition - SVD . SVD is one way to decompose matrices. Remember that matrices can be seen as linear transformations in space. PCA relies on eigendecomposition, which can only be done for square matrices. You might wonder why the first example worked with PCA if the data had far more observations than features. The reason is that when performing PCA you end up using the matrix product $X^{t}X$ which is a square matrix. . However you don’t always have square matrices, and sometimes you have really sparse matrices. . To decompose these types of matrices, which can’t be decomposed with eigendecomposition, you can use techniques such as Singular Value Decomposition. SVD decomposes the original dataset into its constituents, resulting in a reduction of dimensionality. It is used to remove redundant features from the dataset. . To check SVD you are going to use the digits dataset, which is made up of 1797 8x8 images of handwritten digits: . from sklearn.datasets import load_digits # Load the digits dataset digits = load_digits() # Plot first digit image = digits.data[0].reshape((8, 8)) plt.matshow(image, cmap = &#39;gray&#39;) plt.show() . Let&#39;s continue by normalizing the data and checking its dimensions: . X = digits.data # Normalize pixel values X = X/255 # Print shapes of dataset and data points print(f&quot;Digits data has shape {X.shape} n&quot;) print(f&quot;Each data point has shape {X[0].shape} n&quot;) . Plot the first digit to check how normalization affects the images: . image = X[0].reshape((8, 8)) plt.matshow(image, cmap = &#39;gray&#39;) plt.show() . The image should be identical to the one without normalization. This is because the relative brightness of each pixel with the others is maintained. Normalization is done as a preprocessing step when feeding the data into a Neural Network. Here it is done since it is a step that is usually always done when working with image data. . Now perform SVD on the data and plot the cumulative amount of explained variance for every number of components. Note that the TruncatedSVD needs a number of components strictly lower to the number of features. . from sklearn.decomposition import TruncatedSVD # Instantiate Truncated SVD with (original dimension - 1) components org_dim = X.shape[1] tsvd = TruncatedSVD(org_dim - 1) tsvd.fit(X) # Save cumulative explained variance cum_var = (np.cumsum(tsvd.explained_variance_ratio_)) n_comp = [i for i in range(1, org_dim)] # Plot cumulative variance ax = sns.scatterplot(x=n_comp, y=cum_var) ax.set(xlabel=&#39;number of components&#39;, ylabel=&#39;cumulative explained variance&#39;) plt.show() . Looking at the plot it can be seen that with only 5 components near the 50% of the variance of the original data is explained. . Let&#39;s double check this: . print(f&quot;Explained variance with 5 components: {float(cum_var[4:5])*100:.2f}%&quot;) . It is not a lot but let&#39;s check what you get when performing SVD with only 5 components: . tsvd = TruncatedSVD(n_components=5) # Get the transformed data X_tsvd = tsvd.fit_transform(X) # Print shapes of dataset and data points print(f&quot;Original data points have shape {X[0].shape} n&quot;) print(f&quot;Transformed data points have shape {X_tsvd[0].shape} n&quot;) . By doing this you are now representing each digit using 5 dimensions instead of the original 64! Isn&#39;t that amazing? . Now check how this looks like visually: . image_reduced_5 = tsvd.inverse_transform(X_tsvd[0].reshape(1, -1)) image_reduced_5 = image_reduced_5.reshape((8, 8)) plt.matshow(image_reduced_5, cmap = &#39;gray&#39;) plt.show() . It looks blurry but you can still tell this is a zero. . Using more components . Let’s try again, only, this time, we use half the number of features in the original data. . But first define a function that performs this process for any number of components: . def image_given_components(n_components, verbose=True): tsvd = TruncatedSVD(n_components=n_components) X_tsvd = tsvd.fit_transform(X) if verbose: print(f&quot;Explained variance with {n_components} components: {float(tsvd.explained_variance_ratio_.sum())*100:.2f}% n&quot;) image = tsvd.inverse_transform(X_tsvd[0].reshape(1, -1)) image = image.reshape((8, 8)) return image . Use the function to generate the image that use 32 components: . image_reduced_32 = image_given_components(32) plt.matshow(image_reduced_32, cmap = &#39;gray&#39;) plt.show() . Wow! This image looks very similar to the original one (no wonder since more than 95% of the original variance is explained) but the dimensionality of the representations have been cut in half! . To better grasp how the images look like depending on the dimensionality of the representations, the next cell plots them side by side (the last figure has a parameter that you can tweak): . fig = plt.figure() # Original image ax1 = fig.add_subplot(1,4,1) ax1.matshow(image, cmap = &#39;gray&#39;) ax1.title.set_text(&#39;Original&#39;) ax1.axis(&#39;off&#39;) # Using 32 components ax2 = fig.add_subplot(1,4,2) ax2.matshow(image_reduced_32, cmap = &#39;gray&#39;) ax2.title.set_text(&#39;32 components&#39;) ax2.axis(&#39;off&#39;) # Using 5 components ax3 = fig.add_subplot(1,4,3) ax3.matshow(image_reduced_5, cmap = &#39;gray&#39;) ax3.title.set_text(&#39;5 components&#39;) ax3.axis(&#39;off&#39;) # Using 1 components ax4 = fig.add_subplot(1,4,4) ax4.matshow(image_given_components(1), cmap = &#39;gray&#39;) # Change this parameter to see other representations ax4.title.set_text(&#39;1 component&#39;) ax4.axis(&#39;off&#39;) plt.tight_layout() plt.show() . Notice how with 1 component it is not possible to determine that the image is a zero. What is the minimun number of components that are needed for this? Be sure to try out different values and see what you get! . Non-negative Matrix Factorization - NMF . NMF expresses samples as combinations of interpretable parts. For example, it represents documents as combinations of topics, and images in terms of commonly occurring visual patterns. NMF, like PCA, is a dimensionality reduction technique. In contrast to PCA, however, NMF models are interpretable. This means NMF models are easier to understand and much easier for us to explain to others. NMF can&#39;t be applied to every dataset, however. It requires the sample features be non-negative, so greater than or equal to 0. . To test NMF you will use the 20newsgroups dataset which comprises around 12000 newsgroups posts on 20 topics. . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import NMF from sklearn.datasets import fetch_20newsgroups # Download data data = fetch_20newsgroups(remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) # Get the actual text data from the sklearn Bunch data = data.get(&quot;data&quot;) . At this point you have the data in a list format. Let&#39;s check it out: . print(f&quot;Data has {len(data)} elements. n&quot;) print(f&quot;First 2 elements: n&quot;) for n, d in enumerate(data[:2], start=1): print(&quot;======&quot;*10) print(f&quot;Element number {n}: n n{d} n&quot;) . Notice that you only have the actual text without information of the topic it belongs to (labels). . Now you need to represent the text as vectors, for this you will use a TfidfVectorizer with max_features set to 500. This will be the original dimensionality of the data (which you will reduce via NMF). . # The stop_words param refer to words (in english) that don&#39;t add much value to the content of the document and must be ommited vectorizer = TfidfVectorizer(max_features=500, stop_words=&#39;english&#39;) # Vectorize original data vect_data = vectorizer.fit_transform(data) # Print dimensionality print(f&quot;Data has shape {vect_data.shape} after vectorization.&quot;) print(f&quot;Each data point has shape {vect_data[0].shape} after vectorization.&quot;) . Every one of the texts in the original data is represented as a 1x500 vector. . Now use NMF to reduce this dimensionality: . n_comp = 5 # Instantiate NMF with the desired number of components nmf = NMF(n_components=n_comp, random_state=42) # Apply NMF to the vectorized data nmf.fit(vect_data) reduced_vect_data = nmf.transform(vect_data) # Print dimensionality print(f&quot;Data has shape {reduced_vect_data.shape} after NMF.&quot;) print(f&quot;Each data point has shape {reduced_vect_data[0].shape} after NMF.&quot;) # Save feature names for plotting feature_names = vectorizer.get_feature_names() . Now every data point is being represented by a vector of n_comp dimensions rather than the original 500! . In this case every component represents a topic and each data point is represented as a combination of those topics. The value for each topic can be interpreted as how strong the relationship between the text and that particular topic is. . Check this for the 1st element of the text data: . print(f&quot;Original text: n{data[0]} n&quot;) print(f&quot;Representation based on topics: n{reduced_vect_data[0]}&quot;) . Looks like this text can be expressed as a combination of the first, fourth and fifth topic. Specially the later two. . At this point you might wonder what these topics are. Since we didn&#39;t provide labels, these topics arised from the data. To have a sense of what these topics are, plot the top 20 words for each topic: . def plot_words_for_topics(n_comp, nmf, feature_names): fig, axes = plt.subplots(((n_comp-1)//5)+1, 5, figsize=(25, 15)) axes = axes.flatten() for num_topic, topic in enumerate(nmf.components_, start=1): # Plot only the top 20 words # Get the top 20 indexes top_indexes = np.flip(topic.argsort()[-20:]) # Get the corresponding feature name top_features = [feature_names[i] for i in top_indexes] # Get the importance of each word importance = topic[top_indexes] # Plot a barplot ax = axes[num_topic-1] ax.barh(top_features, importance, color=&quot;green&quot;) ax.set_title(f&quot;Topic {num_topic}&quot;, {&quot;fontsize&quot;: 20}) ax.invert_yaxis() ax.tick_params(labelsize=15) plt.tight_layout() plt.show() # Run the function plot_words_for_topics(n_comp, nmf, feature_names) . Let&#39;s try to summarize each topic based on the top most common words for each one: . The first topic is hard to describe but seems to be related to people and actions. . | The second one is clearly abouth tech stuff. . | Third one is about religion. . | Fourth one seems to revolve around sports and/or games. . | And the fifth one about education and/or information. . | . This makes sense considering the example with the first element of the text data. That text is mostly about cars (sports) and information. . Pretty cool, right? . The following function condenses the previously used code so you can play trying out with different number of components: . def try_NMF(n_comp): nmf = NMF(n_components=n_comp, random_state=42) nmf.fit(vect_data) feature_names = vectorizer.get_feature_names() plot_words_for_topics(n_comp, nmf, feature_names) . try_NMF(20) . Congratulations on finishing this ungraded lab! Now you should have a clearer understanding of how to implement dimensionality reduction techniques. . The great thing about dimensionality reduction algorithms is that aside from making training and predicting faster, they perform some kind of automatic feature engineering by transforming the raw data into more meaningful representations. . Keep it up! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/nmf/pca/svd/2022/06/28/_Algorithmic_Dimensionality.html",
            "relUrl": "/nmf/pca/svd/2022/06/28/_Algorithmic_Dimensionality.html",
            "date": " • Jun 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Tensorflow Data Validation (TFDV)",
            "content": "Table of Contents . 1 - Setup and Imports | 2 - Load the Dataset 2.1 - Read and Split the Dataset 2.1.1 - Data Splits | 2.1.2 - Label Column | . | . | 3 - Generate and Visualize Training Data Statistics 3.1 - Removing Irrelevant Features | Exercise 1 - Generate Training Statistics | Exercise 2 - Visualize Training Statistics | . | 4 - Infer a Data Schema Exercise 3: Infer the training set schema | . | 5 - Calculate, Visualize and Fix Evaluation Anomalies Exercise 4: Compare Training and Evaluation Statistics | Exercise 5: Detecting Anomalies | Exercise 6: Fix evaluation anomalies in the schema | . | 6 - Schema Environments Exercise 7: Check anomalies in the serving set | Exercise 8: Modifying the domain | Exercise 9: Detecting anomalies with environments | . | 7 - Check for Data Drift and Skew | 8 - Display Stats for Data Slices | 9 - Freeze the Schema | . . 1 - Setup and Imports . import os import pandas as pd import tensorflow as tf import tempfile, urllib, zipfile import tensorflow_data_validation as tfdv from tensorflow.python.lib.io import file_io from tensorflow_data_validation.utils import slicing_util from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics # Set TF&#39;s logger to only display errors to avoid internal warnings being shown tf.get_logger().setLevel(&#39;ERROR&#39;) . . 2 - Load the Dataset . You will be using the Diabetes 130-US hospitals for years 1999-2008 Data Set donated to the University of California, Irvine (UCI) Machine Learning Repository. The dataset represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. . This dataset has already been included in your Jupyter workspace so you can easily load it. . . 2.1 Read and Split the Dataset . df = pd.read_csv(&#39;data/diabetic_data.csv&#39;, header=0, na_values = &#39;?&#39;) # Preview the dataset df.head() . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | NaN | 6 | 25 | 1 | 1 | ... | No | No | No | No | No | No | No | No | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | NaN | 1 | 1 | 7 | 3 | ... | No | Up | No | No | No | No | No | Ch | Yes | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | NaN | 1 | 1 | 7 | 2 | ... | No | No | No | No | No | No | No | No | Yes | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | NaN | 1 | 1 | 7 | 2 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | NaN | 1 | 1 | 7 | 1 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 5 rows × 50 columns . . Data splits . In a production ML system, the model performance can be negatively affected by anomalies and divergence between data splits for training, evaluation, and serving. To emulate a production system, you will split the dataset into: . 70% training set | 15% evaluation set | 15% serving set | . You will then use TFDV to visualize, analyze, and understand the data. You will create a data schema from the training dataset, then compare the evaluation and serving sets with this schema to detect anomalies and data drift/skew. . . Label Column . This dataset has been prepared to analyze the factors related to readmission outcome. In this notebook, you will treat the readmitted column as the target or label column. . The target (or label) is important to know while splitting the data into training, evaluation and serving sets. In supervised learning, you need to include the target in the training and evaluation datasets. For the serving set however (i.e. the set that simulates the data coming from your users), the label column needs to be dropped since that is the feature that your model will be trying to predict. . The following function returns the training, evaluation and serving partitions of a given dataset: . def prepare_data_splits_from_dataframe(df): &#39;&#39;&#39; Splits a Pandas Dataframe into training, evaluation and serving sets. Parameters: df : pandas dataframe to split Returns: train_df: Training dataframe(70% of the entire dataset) eval_df: Evaluation dataframe (15% of the entire dataset) serving_df: Serving dataframe (15% of the entire dataset, label column dropped) &#39;&#39;&#39; # 70% of records for generating the training set train_len = int(len(df) * 0.7) # Remaining 30% of records for generating the evaluation and serving sets eval_serv_len = len(df) - train_len # Half of the 30%, which makes up 15% of total records, for generating the evaluation set eval_len = eval_serv_len // 2 # Remaining 15% of total records for generating the serving set serv_len = eval_serv_len - eval_len # Sample the train, validation and serving sets. We specify a random state for repeatable outcomes. train_df = df.iloc[:train_len].sample(frac=1, random_state=48).reset_index(drop=True) eval_df = df.iloc[train_len: train_len + eval_len].sample(frac=1, random_state=48).reset_index(drop=True) serving_df = df.iloc[train_len + eval_len: train_len + eval_len + serv_len].sample(frac=1, random_state=48).reset_index(drop=True) # Serving data emulates the data that would be submitted for predictions, so it should not have the label column. serving_df = serving_df.drop([&#39;readmitted&#39;], axis=1) return train_df, eval_df, serving_df . train_df, eval_df, serving_df = prepare_data_splits_from_dataframe(df) print(&#39;Training dataset has {} records nValidation dataset has {} records nServing dataset has {} records&#39;.format(len(train_df),len(eval_df),len(serving_df))) . Training dataset has 71236 records Validation dataset has 15265 records Serving dataset has 15265 records . . 3 - Generate and Visualize Training Data Statistics . In this section, you will be generating descriptive statistics from the dataset. This is usually the first step when dealing with a dataset you are not yet familiar with. It is also known as performing an exploratory data analysis and its purpose is to understand the data types, the data itself and any possible issues that need to be addressed. . It is important to mention that exploratory data analysis should be perfomed on the training dataset only. This is because getting information out of the evaluation or serving datasets can be seen as &quot;cheating&quot; since this data is used to emulate data that you have not collected yet and will try to predict using your ML algorithm. In general, it is a good practice to avoid leaking information from your evaluation and serving data into your model. . . Removing Irrelevant Features . Before you generate the statistics, you may want to drop irrelevant features from your dataset. You can do that with TFDV with the tfdv.StatsOptions class. It is usually not a good idea to drop features without knowing what information they contain. However there are times when this can be fairly obvious. . One of the important parameters of the StatsOptions class is feature_allowlist, which defines the features to include while calculating the data statistics. You can check the documentation to learn more about the class arguments. . In this case, you will omit the statistics for encounter_id and patient_nbr since they are part of the internal tracking of patients in the hospital and they don&#39;t contain valuable information for the task at hand. . features_to_remove = {&#39;encounter_id&#39;, &#39;patient_nbr&#39;} # Collect features to include while computing the statistics approved_cols = [col for col in df.columns if (col not in features_to_remove)] # Instantiate a StatsOptions class and define the feature_allowlist property stats_options = tfdv.StatsOptions(feature_allowlist=approved_cols) # Review the features to generate the statistics for feature in stats_options.feature_allowlist: print(feature) . race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital payer_code medical_specialty num_lab_procedures num_procedures num_medications number_outpatient number_emergency number_inpatient diag_1 diag_2 diag_3 number_diagnoses max_glu_serum A1Cresult metformin repaglinide nateglinide chlorpropamide glimepiride acetohexamide glipizide glyburide tolbutamide pioglitazone rosiglitazone acarbose miglitol troglitazone tolazamide examide citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . . Exercise 1: Generate Training Statistics . TFDV allows you to generate statistics from different data formats such as CSV or a Pandas DataFrame. . Since you already have the data stored in a DataFrame you can use the function tfdv.generate_statistics_from_dataframe() which, given a DataFrame and stats_options, generates an object of type DatasetFeatureStatisticsList. This object includes the computed statistics of the given dataset. . Complete the cell below to generate the statistics of the training set. Remember to pass the training dataframe and the stats_options that you defined above as arguments. . train_stats = tfdv.generate_statistics_from_dataframe(train_df,stats_options) ### END CODE HERE . # get the number of features used to compute statistics print(f&quot;Number of features used: {len(train_stats.datasets[0].features)}&quot;) # check the number of examples used print(f&quot;Number of examples used: {train_stats.datasets[0].num_examples}&quot;) # check the column names of the first and last feature print(f&quot;First feature: {train_stats.datasets[0].features[0].path.step[0]}&quot;) print(f&quot;Last feature: {train_stats.datasets[0].features[-1].path.step[0]}&quot;) . Number of features used: 48 Number of examples used: 71236 First feature: race Last feature: readmitted . Expected Output: . Number of features used: 48 Number of examples used: 71236 First feature: race Last feature: readmitted . . Exercise 2: Visualize Training Statistics . Now that you have the computed statistics in the DatasetFeatureStatisticsList instance, you will need a way to visualize these to get actual insights. TFDV provides this functionality through the method tfdv.visualize_statistics(). . Using this function in an interactive Python environment such as this one will output a very nice and convenient way to interact with the descriptive statistics you generated earlier. . Try it out yourself! Remember to pass in the generated training statistics in the previous exercise as an argument. . tfdv.visualize_statistics(train_stats) ### END CODE HERE . . 4 - Infer a data schema . A schema defines the properties of the data and can thus be used to detect errors. Some of these properties include: . which features are expected to be present | feature type | the number of values for a feature in each example | the presence of each feature across all examples | the expected domains of features | . The schema is expected to be fairly static, whereas statistics can vary per data split. So, you will infer the data schema from only the training dataset. Later, you will generate statistics for evaluation and serving datasets and compare their state with the data schema to detect anomalies, drift and skew. . . Exercise 3: Infer the training set schema . Schema inference is straightforward using tfdv.infer_schema(). This function needs only the statistics (an instance of DatasetFeatureStatisticsList) of your data as input. The output will be a Schema protocol buffer containing the results. . A complimentary function is tfdv.display_schema() for displaying the schema in a table. This accepts a Schema protocol buffer as input. . Fill the code below to infer the schema from the training statistics using TFDV and display the result. . # Infer the data schema by using the training statistics that you generated schema = tfdv.infer_schema(statistics=train_stats) # Display the data schema tfdv.display_schema(schema) ### END CODE HERE . Type Presence Valency Domain . Feature name . &#39;race&#39; STRING | optional | single | &#39;race&#39; | . &#39;gender&#39; STRING | required | | &#39;gender&#39; | . &#39;age&#39; STRING | required | | &#39;age&#39; | . &#39;weight&#39; STRING | optional | single | &#39;weight&#39; | . &#39;admission_type_id&#39; INT | required | | - | . &#39;discharge_disposition_id&#39; INT | required | | - | . &#39;admission_source_id&#39; INT | required | | - | . &#39;time_in_hospital&#39; INT | required | | - | . &#39;payer_code&#39; STRING | optional | single | &#39;payer_code&#39; | . &#39;medical_specialty&#39; STRING | optional | single | &#39;medical_specialty&#39; | . &#39;num_lab_procedures&#39; INT | required | | - | . &#39;num_procedures&#39; INT | required | | - | . &#39;num_medications&#39; INT | required | | - | . &#39;number_outpatient&#39; INT | required | | - | . &#39;number_emergency&#39; INT | required | | - | . &#39;number_inpatient&#39; INT | required | | - | . &#39;diag_1&#39; BYTES | optional | single | - | . &#39;diag_2&#39; BYTES | optional | single | - | . &#39;diag_3&#39; BYTES | optional | single | - | . &#39;number_diagnoses&#39; INT | required | | - | . &#39;max_glu_serum&#39; STRING | required | | &#39;max_glu_serum&#39; | . &#39;A1Cresult&#39; STRING | required | | &#39;A1Cresult&#39; | . &#39;metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;repaglinide&#39; STRING | required | | &#39;repaglinide&#39; | . &#39;nateglinide&#39; STRING | required | | &#39;nateglinide&#39; | . &#39;chlorpropamide&#39; STRING | required | | &#39;chlorpropamide&#39; | . &#39;glimepiride&#39; STRING | required | | &#39;glimepiride&#39; | . &#39;acetohexamide&#39; STRING | required | | &#39;acetohexamide&#39; | . &#39;glipizide&#39; STRING | required | | &#39;glipizide&#39; | . &#39;glyburide&#39; STRING | required | | &#39;glyburide&#39; | . &#39;tolbutamide&#39; STRING | required | | &#39;tolbutamide&#39; | . &#39;pioglitazone&#39; STRING | required | | &#39;pioglitazone&#39; | . &#39;rosiglitazone&#39; STRING | required | | &#39;rosiglitazone&#39; | . &#39;acarbose&#39; STRING | required | | &#39;acarbose&#39; | . &#39;miglitol&#39; STRING | required | | &#39;miglitol&#39; | . &#39;troglitazone&#39; STRING | required | | &#39;troglitazone&#39; | . &#39;tolazamide&#39; STRING | required | | &#39;tolazamide&#39; | . &#39;examide&#39; STRING | required | | &#39;examide&#39; | . &#39;citoglipton&#39; STRING | required | | &#39;citoglipton&#39; | . &#39;insulin&#39; STRING | required | | &#39;insulin&#39; | . &#39;glyburide-metformin&#39; STRING | required | | &#39;glyburide-metformin&#39; | . &#39;glipizide-metformin&#39; STRING | required | | &#39;glipizide-metformin&#39; | . &#39;glimepiride-pioglitazone&#39; STRING | required | | &#39;glimepiride-pioglitazone&#39; | . &#39;metformin-rosiglitazone&#39; STRING | required | | &#39;metformin-rosiglitazone&#39; | . &#39;metformin-pioglitazone&#39; STRING | required | | &#39;metformin-pioglitazone&#39; | . &#39;change&#39; STRING | required | | &#39;change&#39; | . &#39;diabetesMed&#39; STRING | required | | &#39;diabetesMed&#39; | . &#39;readmitted&#39; STRING | required | | &#39;readmitted&#39; | . Values . Domain . &#39;race&#39; &#39;AfricanAmerican&#39;, &#39;Asian&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;, &#39;Other&#39; | . &#39;gender&#39; &#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39; | . &#39;age&#39; &#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39; | . &#39;weight&#39; &#39;&gt;200&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[125-150)&#39;, &#39;[150-175)&#39;, &#39;[175-200)&#39;, &#39;[25-50)&#39;, &#39;[50-75)&#39;, &#39;[75-100)&#39; | . &#39;payer_code&#39; &#39;BC&#39;, &#39;CH&#39;, &#39;CM&#39;, &#39;CP&#39;, &#39;DM&#39;, &#39;HM&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;MP&#39;, &#39;OG&#39;, &#39;OT&#39;, &#39;PO&#39;, &#39;SI&#39;, &#39;SP&#39;, &#39;UN&#39;, &#39;WC&#39; | . &#39;medical_specialty&#39; &#39;AllergyandImmunology&#39;, &#39;Anesthesiology&#39;, &#39;Anesthesiology-Pediatric&#39;, &#39;Cardiology&#39;, &#39;Cardiology-Pediatric&#39;, &#39;Dentistry&#39;, &#39;Dermatology&#39;, &#39;Emergency/Trauma&#39;, &#39;Endocrinology&#39;, &#39;Family/GeneralPractice&#39;, &#39;Gastroenterology&#39;, &#39;Gynecology&#39;, &#39;Hematology&#39;, &#39;Hematology/Oncology&#39;, &#39;Hospitalist&#39;, &#39;InfectiousDiseases&#39;, &#39;InternalMedicine&#39;, &#39;Nephrology&#39;, &#39;Neurology&#39;, &#39;Obsterics&amp;Gynecology-GynecologicOnco&#39;, &#39;Obstetrics&#39;, &#39;ObstetricsandGynecology&#39;, &#39;Oncology&#39;, &#39;Ophthalmology&#39;, &#39;Orthopedics&#39;, &#39;Orthopedics-Reconstructive&#39;, &#39;Osteopath&#39;, &#39;Otolaryngology&#39;, &#39;OutreachServices&#39;, &#39;Pathology&#39;, &#39;Pediatrics&#39;, &#39;Pediatrics-AllergyandImmunology&#39;, &#39;Pediatrics-CriticalCare&#39;, &#39;Pediatrics-EmergencyMedicine&#39;, &#39;Pediatrics-Endocrinology&#39;, &#39;Pediatrics-Hematology-Oncology&#39;, &#39;Pediatrics-InfectiousDiseases&#39;, &#39;Pediatrics-Neurology&#39;, &#39;Pediatrics-Pulmonology&#39;, &#39;Perinatology&#39;, &#39;PhysicalMedicineandRehabilitation&#39;, &#39;PhysicianNotFound&#39;, &#39;Podiatry&#39;, &#39;Proctology&#39;, &#39;Psychiatry&#39;, &#39;Psychiatry-Addictive&#39;, &#39;Psychiatry-Child/Adolescent&#39;, &#39;Psychology&#39;, &#39;Pulmonology&#39;, &#39;Radiologist&#39;, &#39;Radiology&#39;, &#39;Rheumatology&#39;, &#39;Speech&#39;, &#39;SportsMedicine&#39;, &#39;Surgeon&#39;, &#39;Surgery-Cardiovascular&#39;, &#39;Surgery-Cardiovascular/Thoracic&#39;, &#39;Surgery-Colon&amp;Rectal&#39;, &#39;Surgery-General&#39;, &#39;Surgery-Maxillofacial&#39;, &#39;Surgery-Neuro&#39;, &#39;Surgery-Pediatric&#39;, &#39;Surgery-Plastic&#39;, &#39;Surgery-PlasticwithinHeadandNeck&#39;, &#39;Surgery-Thoracic&#39;, &#39;Surgery-Vascular&#39;, &#39;SurgicalSpecialty&#39;, &#39;Urology&#39; | . &#39;max_glu_serum&#39; &#39;&gt;200&#39;, &#39;&gt;300&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;A1Cresult&#39; &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;repaglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;nateglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;chlorpropamide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glimepiride&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acetohexamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glipizide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;tolbutamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;pioglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;rosiglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acarbose&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;miglitol&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;troglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;tolazamide&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;examide&#39; &#39;No&#39; | . &#39;citoglipton&#39; &#39;No&#39; | . &#39;insulin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide-metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glipizide-metformin&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glimepiride-pioglitazone&#39; &#39;No&#39; | . &#39;metformin-rosiglitazone&#39; &#39;No&#39; | . &#39;metformin-pioglitazone&#39; &#39;No&#39; | . &#39;change&#39; &#39;Ch&#39;, &#39;No&#39; | . &#39;diabetesMed&#39; &#39;No&#39;, &#39;Yes&#39; | . &#39;readmitted&#39; &#39;&lt;30&#39;, &#39;&gt;30&#39;, &#39;NO&#39; | . # Check number of features print(f&quot;Number of features in schema: {len(schema.feature)}&quot;) # Check domain name of 2nd feature print(f&quot;Second feature in schema: {list(schema.feature)[1].domain}&quot;) . Number of features in schema: 48 Second feature in schema: gender . Expected Output: . Number of features in schema: 48 Second feature in schema: gender . Be sure to check the information displayed before moving forward. . . 5 - Calculate, Visualize and Fix Evaluation Anomalies . It is important that the schema of the evaluation data is consistent with the training data since the data that your model is going to receive should be consistent to the one you used to train it with. . Moreover, it is also important that the features of the evaluation data belong roughly to the same range as the training data. This ensures that the model will be evaluated on a similar loss surface covered during training. . . Exercise 4: Compare Training and Evaluation Statistics . Now you are going to generate the evaluation statistics and compare it with training statistics. You can use the tfdv.generate_statistics_from_dataframe() function for this. But this time, you&#39;ll need to pass the evaluation data. For the stats_options parameter, the list you used before works here too. . Remember that to visualize the evaluation statistics you can use tfdv.visualize_statistics(). . However, it is impractical to visualize both statistics separately and do your comparison from there. Fortunately, TFDV has got this covered. You can use the visualize_statistics function and pass additional parameters to overlay the statistics from both datasets (referenced as left-hand side and right-hand side statistics). Let&#39;s see what these parameters are: . lhs_statistics: Required parameter. Expects an instance of DatasetFeatureStatisticsList. | . rhs_statistics: Expects an instance of DatasetFeatureStatisticsList to compare with lhs_statistics. | . lhs_name: Name of the lhs_statistics dataset. | . rhs_name: Name of the rhs_statistics dataset. | . For this case, remember to define the lhs_statistics protocol with the eval_stats, and the optional rhs_statistics protocol with the train_stats. . Additionally, check the function for the protocol name declaration, and define the lhs and rhs names as &#39;EVAL_DATASET&#39; and &#39;TRAIN_DATASET&#39; respectively. . # Generate evaluation dataset statistics # HINT: Remember to use the evaluation dataframe and to pass the stats_options (that you defined before) as an argument eval_stats = tfdv.generate_statistics_from_dataframe(eval_df, stats_options=stats_options) # Compare evaluation data with training data # HINT: Remember to use both the evaluation and training statistics with the lhs_statistics and rhs_statistics arguments # HINT: Assign the names of &#39;EVAL_DATASET&#39; and &#39;TRAIN_DATASET&#39; to the lhs and rhs protocols tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats, lhs_name=&#39;EVAL_DATASET&#39; , rhs_name=&#39;TRAIN_DATASET&#39;) ### END CODE HERE . # get the number of features used to compute statistics print(f&quot;Number of features: {len(eval_stats.datasets[0].features)}&quot;) # check the number of examples used print(f&quot;Number of examples: {eval_stats.datasets[0].num_examples}&quot;) # check the column names of the first and last feature print(f&quot;First feature: {eval_stats.datasets[0].features[0].path.step[0]}&quot;) print(f&quot;Last feature: {eval_stats.datasets[0].features[-1].path.step[0]}&quot;) . Number of features: 48 Number of examples: 15265 First feature: race Last feature: readmitted . Expected Output: . Number of features: 48 Number of examples: 15265 First feature: race Last feature: readmitted . . Exercise 5: Detecting Anomalies . At this point, you should ask if your evaluation dataset matches the schema from your training dataset. For instance, if you scroll through the output cell in the previous exercise, you can see that the categorical feature glimepiride-pioglitazone has 1 unique value in the training set while the evaluation dataset has 2. You can verify with the built-in Pandas describe() method as well. . train_df[&quot;glimepiride-pioglitazone&quot;].describe() . count 71236 unique 1 top No freq 71236 Name: glimepiride-pioglitazone, dtype: object . eval_df[&quot;glimepiride-pioglitazone&quot;].describe() . count 15265 unique 2 top No freq 15264 Name: glimepiride-pioglitazone, dtype: object . It is possible but highly inefficient to visually inspect and determine all the anomalies. So, let&#39;s instead use TFDV functions to detect and display these. . You can use the function tfdv.validate_statistics() for detecting anomalies and tfdv.display_anomalies() for displaying them. . The validate_statistics() method has two required arguments: . an instance of DatasetFeatureStatisticsList | an instance of Schema | . Fill in the following graded function which, given the statistics and schema, displays the anomalies found. . def calculate_and_display_anomalies(statistics, schema): &#39;&#39;&#39; Calculate and display anomalies. Parameters: statistics : Data statistics in statistics_pb2.DatasetFeatureStatisticsList format schema : Data schema in schema_pb2.Schema format Returns: display of calculated anomalies &#39;&#39;&#39; ### START CODE HERE # HINTS: Pass the statistics and schema parameters into the validation function anomalies = tfdv.validate_statistics(statistics, schema) # HINTS: Display input anomalies by using the calculated anomalies tfdv.display_anomalies(anomalies) ### END CODE HERE . You should see detected anomalies in the medical_specialty and glimepiride-pioglitazone features by running the cell below. . calculate_and_display_anomalies(eval_stats, schema=schema) . Anomaly short description Anomaly long description . Feature name . &#39;glimepiride-pioglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;medical_specialty&#39; Unexpected string values | Examples contain values missing from the schema: Neurophysiology (&lt;1%). | . . Exercise 6: Fix evaluation anomalies in the schema . The evaluation data has records with values for the features glimepiride-pioglitazone and medical_speciality that were not included in the schema generated from the training data. You can fix this by adding the new values that exist in the evaluation dataset to the domain of these features. . To get the domain of a particular feature you can use tfdv.get_domain(). . You can use the append() method to the value property of the returned domain to add strings to the valid list of values. To be more explicit, given a domain you can do something like: . domain.value.append(&quot;feature_value&quot;) . # Get the domain associated with the input feature, glimepiride-pioglitazone, from the schema glimepiride_pioglitazone_domain = tfdv.get_domain(schema, &#39;glimepiride-pioglitazone&#39;) # HINT: Append the missing value &#39;Steady&#39; to the domain glimepiride_pioglitazone_domain.value.append(&#39;Steady&#39;) # Get the domain associated with the input feature, medical_specialty, from the schema medical_specialty_domain = tfdv.get_domain(schema, &#39;medical_specialty&#39;) # HINT: Append the missing value &#39;Neurophysiology&#39; to the domain medical_specialty_domain.value.append(&#39;Neurophysiology&#39;) # HINT: Re-calculate and re-display anomalies with the new schema calculate_and_display_anomalies(eval_stats, schema=schema) ### END CODE HERE . No anomalies found. . If you did the exercise correctly, you should see &quot;No anomalies found.&quot; after running the cell above. . . 6 - Schema Environments . By default, all datasets in a pipeline should use the same schema. However, there are some exceptions. . For example, the label column is dropped in the serving set so this will be flagged when comparing with the training set schema. . In this case, introducing slight schema variations is necessary. . . Exercise 7: Check anomalies in the serving set . Now you are going to check for anomalies in the serving data. The process is very similar to the one you previously did for the evaluation data with a little change. . Let&#39;s create a new StatsOptions that is aware of the information provided by the schema and use it when generating statistics from the serving DataFrame. . options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True, feature_allowlist=approved_cols) . # Generate serving dataset statistics # HINT: Remember to use the serving dataframe and to pass the newly defined statistics options serving_stats = tfdv.generate_statistics_from_dataframe(serving_df, stats_options=stats_options) # HINT: Calculate and display anomalies using the generated serving statistics calculate_and_display_anomalies(serving_stats, schema=schema) ### END CODE HERE . Anomaly short description Anomaly long description . Feature name . &#39;medical_specialty&#39; Unexpected string values | Examples contain values missing from the schema: DCPTEAM (&lt;1%), Endocrinology-Metabolism (&lt;1%), Resident (&lt;1%). | . &#39;metformin-pioglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;readmitted&#39; Column dropped | Column is completely missing | . &#39;payer_code&#39; Unexpected string values | Examples contain values missing from the schema: FR (&lt;1%). | . &#39;metformin-rosiglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . You should see that metformin-rosiglitazone, metformin-pioglitazone, payer_code and medical_specialty features have an anomaly (i.e. Unexpected string values) which is less than 1%. . Let&#39;s relax the anomaly detection constraints for the last two of these features by defining the min_domain_mass of the feature&#39;s distribution constraints. . # Get the feature and relax to match 90% of the domain payer_code = tfdv.get_feature(schema, &#39;payer_code&#39;) payer_code.distribution_constraints.min_domain_mass = 0.9 # Get the feature and relax to match 90% of the domain medical_specialty = tfdv.get_feature(schema, &#39;medical_specialty&#39;) medical_specialty.distribution_constraints.min_domain_mass = 0.9 # Detect anomalies with the updated constraints calculate_and_display_anomalies(serving_stats, schema=schema) . Anomaly short description Anomaly long description . Feature name . &#39;readmitted&#39; Column dropped | Column is completely missing | . &#39;metformin-rosiglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;metformin-pioglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . If the payer_code and medical_specialty are no longer part of the output cell, then the relaxation worked! . . Exercise 8: Modifying the Domain . Let&#39;s investigate the possible cause of the anomalies for the other features, namely metformin-pioglitazone and metformin-rosiglitazone. From the output of the previous exercise, you&#39;ll see that the anomaly long description says: &quot;Examples contain values missing from the schema: Steady (&lt;1%)&quot;. You can redisplay the schema and look at the domain of these features to verify this statement. . When you inferred the schema at the start of this lab, it&#39;s possible that some values were not detected in the training data so it was not included in the expected domain values of the feature&#39;s schema. In the case of metformin-rosiglitazone and metformin-pioglitazone, the value &quot;Steady&quot; is indeed missing. You will just see &quot;No&quot; in the domain of these two features after running the code cell below. . tfdv.display_schema(schema) . Towards the bottom of the Domain-Values pairs of the cell above, you can see that many features (including &#39;metformin&#39;) have the same values: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;]. These values are common to many features including the ones with missing values during schema inference. . TFDV allows you to modify the domains of some features to match an existing domain. To address the detected anomaly, you can set the domain of these features to the domain of the metformin feature. . Complete the function below to set the domain of a feature list to an existing feature domain. . For this, use the tfdv.set_domain() function, which has the following parameters: . schema: The schema | . feature_path: The name of the feature whose domain needs to be set. | . domain: A domain protocol buffer or the name of a global string domain present in the input schema. | . def modify_domain_of_features(features_list, schema, to_domain_name): &#39;&#39;&#39; Modify a list of features&#39; domains. Parameters: features_list : Features that need to be modified schema: Inferred schema to_domain_name : Target domain to be transferred to the features list Returns: schema: new schema &#39;&#39;&#39; ### START CODE HERE # HINT: Loop over the feature list and use set_domain with the inferred schema, feature name and target domain name for feature in features_list: tfdv.set_domain(schema,feature,to_domain_name) ### END CODE HERE return schema . Using this function, set the domain of the features defined in the domain_change_features list below to be equal to metformin&#39;s domain to address the anomalies found. . Since you are overriding the existing domain of the features, it is normal to get a warning so you don&#39;t do this by accident. . domain_change_features = [&#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;] # Infer new schema by using your modify_domain_of_features function # and the defined domain_change_features feature list schema = modify_domain_of_features(domain_change_features, schema, &#39;metformin&#39;) # Display new schema tfdv.display_schema(schema) . WARNING:root:Replacing existing domain of feature &#34;repaglinide&#34;. WARNING:root:Replacing existing domain of feature &#34;nateglinide&#34;. WARNING:root:Replacing existing domain of feature &#34;chlorpropamide&#34;. WARNING:root:Replacing existing domain of feature &#34;glimepiride&#34;. WARNING:root:Replacing existing domain of feature &#34;acetohexamide&#34;. WARNING:root:Replacing existing domain of feature &#34;glipizide&#34;. WARNING:root:Replacing existing domain of feature &#34;glyburide&#34;. WARNING:root:Replacing existing domain of feature &#34;tolbutamide&#34;. WARNING:root:Replacing existing domain of feature &#34;pioglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;rosiglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;acarbose&#34;. WARNING:root:Replacing existing domain of feature &#34;miglitol&#34;. WARNING:root:Replacing existing domain of feature &#34;troglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;tolazamide&#34;. WARNING:root:Replacing existing domain of feature &#34;examide&#34;. WARNING:root:Replacing existing domain of feature &#34;citoglipton&#34;. WARNING:root:Replacing existing domain of feature &#34;insulin&#34;. WARNING:root:Replacing existing domain of feature &#34;glyburide-metformin&#34;. WARNING:root:Replacing existing domain of feature &#34;glipizide-metformin&#34;. WARNING:root:Replacing existing domain of feature &#34;glimepiride-pioglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;metformin-rosiglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;metformin-pioglitazone&#34;. . Type Presence Valency Domain . Feature name . &#39;race&#39; STRING | optional | single | &#39;race&#39; | . &#39;gender&#39; STRING | required | | &#39;gender&#39; | . &#39;age&#39; STRING | required | | &#39;age&#39; | . &#39;weight&#39; STRING | optional | single | &#39;weight&#39; | . &#39;admission_type_id&#39; INT | required | | - | . &#39;discharge_disposition_id&#39; INT | required | | - | . &#39;admission_source_id&#39; INT | required | | - | . &#39;time_in_hospital&#39; INT | required | | - | . &#39;payer_code&#39; STRING | optional | single | &#39;payer_code&#39; | . &#39;medical_specialty&#39; STRING | optional | single | &#39;medical_specialty&#39; | . &#39;num_lab_procedures&#39; INT | required | | - | . &#39;num_procedures&#39; INT | required | | - | . &#39;num_medications&#39; INT | required | | - | . &#39;number_outpatient&#39; INT | required | | - | . &#39;number_emergency&#39; INT | required | | - | . &#39;number_inpatient&#39; INT | required | | - | . &#39;diag_1&#39; BYTES | optional | single | - | . &#39;diag_2&#39; BYTES | optional | single | - | . &#39;diag_3&#39; BYTES | optional | single | - | . &#39;number_diagnoses&#39; INT | required | | - | . &#39;max_glu_serum&#39; STRING | required | | &#39;max_glu_serum&#39; | . &#39;A1Cresult&#39; STRING | required | | &#39;A1Cresult&#39; | . &#39;metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;repaglinide&#39; STRING | required | | &#39;metformin&#39; | . &#39;nateglinide&#39; STRING | required | | &#39;metformin&#39; | . &#39;chlorpropamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glimepiride&#39; STRING | required | | &#39;metformin&#39; | . &#39;acetohexamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glipizide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glyburide&#39; STRING | required | | &#39;metformin&#39; | . &#39;tolbutamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;rosiglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;acarbose&#39; STRING | required | | &#39;metformin&#39; | . &#39;miglitol&#39; STRING | required | | &#39;metformin&#39; | . &#39;troglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;tolazamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;examide&#39; STRING | required | | &#39;metformin&#39; | . &#39;citoglipton&#39; STRING | required | | &#39;metformin&#39; | . &#39;insulin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glyburide-metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glipizide-metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glimepiride-pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;metformin-rosiglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;metformin-pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;change&#39; STRING | required | | &#39;change&#39; | . &#39;diabetesMed&#39; STRING | required | | &#39;diabetesMed&#39; | . &#39;readmitted&#39; STRING | required | | &#39;readmitted&#39; | . Values . Domain . &#39;race&#39; &#39;AfricanAmerican&#39;, &#39;Asian&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;, &#39;Other&#39; | . &#39;gender&#39; &#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39; | . &#39;age&#39; &#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39; | . &#39;weight&#39; &#39;&gt;200&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[125-150)&#39;, &#39;[150-175)&#39;, &#39;[175-200)&#39;, &#39;[25-50)&#39;, &#39;[50-75)&#39;, &#39;[75-100)&#39; | . &#39;payer_code&#39; &#39;BC&#39;, &#39;CH&#39;, &#39;CM&#39;, &#39;CP&#39;, &#39;DM&#39;, &#39;HM&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;MP&#39;, &#39;OG&#39;, &#39;OT&#39;, &#39;PO&#39;, &#39;SI&#39;, &#39;SP&#39;, &#39;UN&#39;, &#39;WC&#39; | . &#39;medical_specialty&#39; &#39;AllergyandImmunology&#39;, &#39;Anesthesiology&#39;, &#39;Anesthesiology-Pediatric&#39;, &#39;Cardiology&#39;, &#39;Cardiology-Pediatric&#39;, &#39;Dentistry&#39;, &#39;Dermatology&#39;, &#39;Emergency/Trauma&#39;, &#39;Endocrinology&#39;, &#39;Family/GeneralPractice&#39;, &#39;Gastroenterology&#39;, &#39;Gynecology&#39;, &#39;Hematology&#39;, &#39;Hematology/Oncology&#39;, &#39;Hospitalist&#39;, &#39;InfectiousDiseases&#39;, &#39;InternalMedicine&#39;, &#39;Nephrology&#39;, &#39;Neurology&#39;, &#39;Obsterics&amp;Gynecology-GynecologicOnco&#39;, &#39;Obstetrics&#39;, &#39;ObstetricsandGynecology&#39;, &#39;Oncology&#39;, &#39;Ophthalmology&#39;, &#39;Orthopedics&#39;, &#39;Orthopedics-Reconstructive&#39;, &#39;Osteopath&#39;, &#39;Otolaryngology&#39;, &#39;OutreachServices&#39;, &#39;Pathology&#39;, &#39;Pediatrics&#39;, &#39;Pediatrics-AllergyandImmunology&#39;, &#39;Pediatrics-CriticalCare&#39;, &#39;Pediatrics-EmergencyMedicine&#39;, &#39;Pediatrics-Endocrinology&#39;, &#39;Pediatrics-Hematology-Oncology&#39;, &#39;Pediatrics-InfectiousDiseases&#39;, &#39;Pediatrics-Neurology&#39;, &#39;Pediatrics-Pulmonology&#39;, &#39;Perinatology&#39;, &#39;PhysicalMedicineandRehabilitation&#39;, &#39;PhysicianNotFound&#39;, &#39;Podiatry&#39;, &#39;Proctology&#39;, &#39;Psychiatry&#39;, &#39;Psychiatry-Addictive&#39;, &#39;Psychiatry-Child/Adolescent&#39;, &#39;Psychology&#39;, &#39;Pulmonology&#39;, &#39;Radiologist&#39;, &#39;Radiology&#39;, &#39;Rheumatology&#39;, &#39;Speech&#39;, &#39;SportsMedicine&#39;, &#39;Surgeon&#39;, &#39;Surgery-Cardiovascular&#39;, &#39;Surgery-Cardiovascular/Thoracic&#39;, &#39;Surgery-Colon&amp;Rectal&#39;, &#39;Surgery-General&#39;, &#39;Surgery-Maxillofacial&#39;, &#39;Surgery-Neuro&#39;, &#39;Surgery-Pediatric&#39;, &#39;Surgery-Plastic&#39;, &#39;Surgery-PlasticwithinHeadandNeck&#39;, &#39;Surgery-Thoracic&#39;, &#39;Surgery-Vascular&#39;, &#39;SurgicalSpecialty&#39;, &#39;Urology&#39;, &#39;Neurophysiology&#39; | . &#39;max_glu_serum&#39; &#39;&gt;200&#39;, &#39;&gt;300&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;A1Cresult&#39; &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;repaglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;nateglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;chlorpropamide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glimepiride&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acetohexamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glipizide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;tolbutamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;pioglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;rosiglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acarbose&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;miglitol&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;troglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;tolazamide&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;examide&#39; &#39;No&#39; | . &#39;citoglipton&#39; &#39;No&#39; | . &#39;insulin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide-metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glipizide-metformin&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glimepiride-pioglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;metformin-rosiglitazone&#39; &#39;No&#39; | . &#39;metformin-pioglitazone&#39; &#39;No&#39; | . &#39;change&#39; &#39;Ch&#39;, &#39;No&#39; | . &#39;diabetesMed&#39; &#39;No&#39;, &#39;Yes&#39; | . &#39;readmitted&#39; &#39;&lt;30&#39;, &#39;&gt;30&#39;, &#39;NO&#39; | . # check that the domain of some features are now switched to `metformin` print(f&quot;Domain name of &#39;chlorpropamide&#39;: {tfdv.get_feature(schema, &#39;chlorpropamide&#39;).domain}&quot;) print(f&quot;Domain values of &#39;chlorpropamide&#39;: {tfdv.get_domain(schema, &#39;chlorpropamide&#39;).value}&quot;) print(f&quot;Domain name of &#39;repaglinide&#39;: {tfdv.get_feature(schema, &#39;repaglinide&#39;).domain}&quot;) print(f&quot;Domain values of &#39;repaglinide&#39;: {tfdv.get_domain(schema, &#39;repaglinide&#39;).value}&quot;) print(f&quot;Domain name of &#39;nateglinide&#39;: {tfdv.get_feature(schema, &#39;nateglinide&#39;).domain}&quot;) print(f&quot;Domain values of &#39;nateglinide&#39;: {tfdv.get_domain(schema, &#39;nateglinide&#39;).value}&quot;) . Domain name of &#39;chlorpropamide&#39;: metformin Domain values of &#39;chlorpropamide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;repaglinide&#39;: metformin Domain values of &#39;repaglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;nateglinide&#39;: metformin Domain values of &#39;nateglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] . Expected Output: . Domain name of &#39;chlorpropamide&#39;: metformin Domain values of &#39;chlorpropamide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;repaglinide&#39;: metformin Domain values of &#39;repaglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;nateglinide&#39;: metformin Domain values of &#39;nateglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] . Let&#39;s do a final check of anomalies to see if this solved the issue. . calculate_and_display_anomalies(serving_stats, schema=schema) . Anomaly short description Anomaly long description . Feature name . &#39;readmitted&#39; Column dropped | Column is completely missing | . You should now see the metformin-pioglitazone and metformin-rosiglitazone features dropped from the output anomalies. . . Exercise 9: Detecting anomalies with environments . There is still one thing to address. The readmitted feature (which is the label column) showed up as an anomaly (&#39;Column dropped&#39;). Since labels are not expected in the serving data, let&#39;s tell TFDV to ignore this detected anomaly. . This requirement of introducing slight schema variations can be expressed by using environments. In particular, features in the schema can be associated with a set of environments using default_environment, in_environment and not_in_environment. . schema.default_environment.append(&#39;TRAINING&#39;) schema.default_environment.append(&#39;SERVING&#39;) . Complete the code below to exclude the readmitted feature from the SERVING environment. . To achieve this, you can use the tfdv.get_feature() function to get the readmitted feature from the inferred schema and use its not_in_environment attribute to specify that readmitted should be removed from the SERVING environment&#39;s schema. This attribute is a list so you will have to append the name of the environment that you wish to omit this feature for. . To be more explicit, given a feature you can do something like: . feature.not_in_environment.append(&#39;NAME_OF_ENVIRONMENT&#39;) . The function tfdv.get_feature receives the following parameters: . schema: The schema. | feature_path: The path of the feature to obtain from the schema. In this case this is equal to the name of the feature. | . # Specify that &#39;readmitted&#39; feature is not in SERVING environment. # HINT: Append the &#39;SERVING&#39; environmnet to the not_in_environment attribute of the feature tfdv.get_feature(schema, &#39;readmitted&#39;).not_in_environment.append(&#39;SERVING&#39;) # HINT: Calculate anomalies with the validate_statistics function by using the serving statistics, # inferred schema and the SERVING environment parameter. serving_anomalies_with_env = tfdv.validate_statistics(serving_stats, schema, environment=&#39;SERVING&#39;) ### END CODE HERE . You should see &quot;No anomalies found&quot; by running the cell below. . tfdv.display_anomalies(serving_anomalies_with_env) . No anomalies found. . Now you have succesfully addressed all anomaly-related issues! . . 7 - Check for Data Drift and Skew . During data validation, you also need to check for data drift and data skew between the training and serving data. You can do this by specifying the skew_comparator and drift_comparator in the schema. . Drift and skew is expressed in terms of L-infinity distance which evaluates the difference between vectors as the greatest of the differences along any coordinate dimension. . You can set the threshold distance so that you receive warnings when the drift is higher than is acceptable. Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation. . Let&#39;s check for the skew in the diabetesMed feature and drift in the payer_code feature. . diabetes_med = tfdv.get_feature(schema, &#39;diabetesMed&#39;) diabetes_med.skew_comparator.infinity_norm.threshold = 0.03 # domain knowledge helps to determine this threshold # Calculate drift for the payer_code feature payer_code = tfdv.get_feature(schema, &#39;payer_code&#39;) payer_code.drift_comparator.infinity_norm.threshold = 0.03 # domain knowledge helps to determine this threshold # Calculate anomalies skew_drift_anomalies = tfdv.validate_statistics(train_stats, schema, previous_statistics=eval_stats, serving_statistics=serving_stats) # Display anomalies tfdv.display_anomalies(skew_drift_anomalies) . Anomaly short description Anomaly long description . Feature name . &#39;payer_code&#39; High Linfty distance between current and previous | The Linfty distance between current and previous is 0.0342144 (up to six significant digits), above the threshold 0.03. The feature value with maximum difference is: MC | . &#39;diabetesMed&#39; High Linfty distance between training and serving | The Linfty distance between training and serving is 0.0325464 (up to six significant digits), above the threshold 0.03. The feature value with maximum difference is: No | . In both of these cases, the detected anomaly distance is not too far from the threshold value of 0.03. For this exercise, let&#39;s accept this as within bounds (i.e. you can set the distance to something like 0.035 instead). . However, if the anomaly truly indicates a skew and drift, then further investigation is necessary as this could have a direct impact on model performance. . . 8 - Display Stats for Data Slices . Finally, you can slice the dataset and calculate the statistics for each unique value of a feature. By default, TFDV computes statistics for the overall dataset in addition to the configured slices. Each slice is identified by a unique name which is set as the dataset name in the DatasetFeatureStatistics protocol buffer. Generating and displaying statistics over different slices of data can help track model and anomaly metrics. . Let&#39;s first define a few helper functions to make our code in the exercise more neat. . def split_datasets(dataset_list): &#39;&#39;&#39; split datasets. Parameters: dataset_list: List of datasets to split Returns: datasets: sliced data &#39;&#39;&#39; datasets = [] for dataset in dataset_list.datasets: proto_list = DatasetFeatureStatisticsList() proto_list.datasets.extend([dataset]) datasets.append(proto_list) return datasets def display_stats_at_index(index, datasets): &#39;&#39;&#39; display statistics at the specified data index Parameters: index : index to show the anomalies datasets: split data Returns: display of generated sliced data statistics at the specified index &#39;&#39;&#39; if index &lt; len(datasets): print(datasets[index].datasets[0].name) tfdv.visualize_statistics(datasets[index]) . The function below returns a list of DatasetFeatureStatisticsList protocol buffers. As shown in the ungraded lab, the first one will be for All Examples followed by individual slices through the feature you specified. . To configure TFDV to generate statistics for dataset slices, you will use the function tfdv.StatsOptions() with the following 4 arguments: . schema | . slice_functions passed as a list. | . infer_type_from_schema set to True. | . feature_allowlist set to the approved features. | . Remember that slice_functions only work with generate_statistics_from_csv() so you will need to convert the dataframe to CSV. . def sliced_stats_for_slice_fn(slice_fn, approved_cols, dataframe, schema): &#39;&#39;&#39; generate statistics for the sliced data. Parameters: slice_fn : slicing definition approved_cols: list of features to pass to the statistics options dataframe: pandas dataframe to slice schema: the schema Returns: slice_info_datasets: statistics for the sliced dataset &#39;&#39;&#39; # Set the StatsOptions slice_stats_options = tfdv.StatsOptions(schema=schema, slice_functions=[slice_fn], infer_type_from_schema=True, feature_allowlist=approved_cols) # Convert Dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv` CSV_PATH = &#39;slice_sample.csv&#39; dataframe.to_csv(CSV_PATH) # Calculate statistics for the sliced dataset sliced_stats = tfdv.generate_statistics_from_csv(CSV_PATH, stats_options=slice_stats_options) # Split the dataset using the previously defined split_datasets function slice_info_datasets = split_datasets(sliced_stats) return slice_info_datasets . With that, you can now use the helper functions to generate and visualize statistics for the sliced datasets. . slice_fn = slicing_util.get_feature_value_slicer(features={&#39;medical_specialty&#39;: None}) # Generate stats for the sliced dataset slice_datasets = sliced_stats_for_slice_fn(slice_fn, approved_cols, dataframe=train_df, schema=schema) # Print name of slices for reference print(f&#39;Statistics generated for: n&#39;) print(&#39; n&#39;.join([sliced.datasets[0].name for sliced in slice_datasets])) # Display at index 10, which corresponds to the slice named `medical_specialty_Gastroenterology` display_stats_at_index(10, slice_datasets) . WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter. . Statistics generated for: All Examples medical_specialty_Orthopedics medical_specialty_InternalMedicine medical_specialty_Cardiology medical_specialty_Family/GeneralPractice medical_specialty_Surgery-General medical_specialty_Emergency/Trauma medical_specialty_Nephrology medical_specialty_Surgery-Neuro medical_specialty_Oncology medical_specialty_Gastroenterology medical_specialty_Orthopedics-Reconstructive medical_specialty_ObstetricsandGynecology medical_specialty_Surgery-Cardiovascular/Thoracic medical_specialty_Radiologist medical_specialty_Urology medical_specialty_Surgery-Vascular medical_specialty_Hematology/Oncology medical_specialty_Neurology medical_specialty_Psychology medical_specialty_Psychiatry medical_specialty_PhysicalMedicineandRehabilitation medical_specialty_Pulmonology medical_specialty_Otolaryngology medical_specialty_Obsterics&amp;Gynecology-GynecologicOnco medical_specialty_Endocrinology medical_specialty_Anesthesiology medical_specialty_Pediatrics-Endocrinology medical_specialty_Radiology medical_specialty_Pediatrics medical_specialty_Pediatrics-Pulmonology medical_specialty_Osteopath medical_specialty_Surgery-Plastic medical_specialty_Podiatry medical_specialty_Surgery-Thoracic medical_specialty_Rheumatology medical_specialty_Obstetrics medical_specialty_Pediatrics-AllergyandImmunology medical_specialty_Surgery-Cardiovascular medical_specialty_Anesthesiology-Pediatric medical_specialty_Pathology medical_specialty_Pediatrics-CriticalCare medical_specialty_PhysicianNotFound medical_specialty_Gynecology medical_specialty_AllergyandImmunology medical_specialty_Surgery-Maxillofacial medical_specialty_Hospitalist medical_specialty_Hematology medical_specialty_Surgeon medical_specialty_Proctology medical_specialty_InfectiousDiseases medical_specialty_Psychiatry-Child/Adolescent medical_specialty_SurgicalSpecialty medical_specialty_Ophthalmology medical_specialty_Surgery-Pediatric medical_specialty_Pediatrics-Neurology medical_specialty_Surgery-PlasticwithinHeadandNeck medical_specialty_OutreachServices medical_specialty_Pediatrics-Hematology-Oncology medical_specialty_Dentistry medical_specialty_Pediatrics-EmergencyMedicine medical_specialty_Psychiatry-Addictive medical_specialty_Surgery-Colon&amp;Rectal medical_specialty_Pediatrics-InfectiousDiseases medical_specialty_Dermatology medical_specialty_Perinatology medical_specialty_SportsMedicine medical_specialty_Cardiology-Pediatric medical_specialty_Speech medical_specialty_Gastroenterology . If you are curious, try different slice indices to extract the group statistics. For instance, index=5 corresponds to all medical_specialty_Surgery-General records. You can also try slicing through multiple features as shown in the ungraded lab. . Another challenge is to implement your own helper functions. For instance, you can make a display_stats_for_slice_name() function so you don&#39;t have to determine the index of a slice. If done correctly, you can just do display_stats_for_slice_name(&#39;medical_specialty_Gastroenterology&#39;, slice_datasets) and it will generate the same result as display_stats_at_index(10, slice_datasets). . . 9 - Freeze the schema . Now that the schema has been reviewed, you will store the schema in a file in its &quot;frozen&quot; state. This can be used to validate incoming data once your application goes live to your users. . This is pretty straightforward using Tensorflow&#39;s io utils and TFDV&#39;s write_schema_text() function. . OUTPUT_DIR = &quot;output&quot; file_io.recursive_create_dir(OUTPUT_DIR) # Use TensorFlow text output format pbtxt to store the schema schema_file = os.path.join(OUTPUT_DIR, &#39;schema.pbtxt&#39;) # write_schema_text function expect the defined schema and output path as parameters tfdv.write_schema_text(schema, schema_file) . After submitting this assignment, you can click the Jupyter logo in the left upper corner of the screen to check the Jupyter filesystem. The schema.pbtxt file should be inside the output directory. . Congratulations on finishing this week&#39;s assignment! A lot of concepts where introduced and now you should feel more familiar with using TFDV for inferring schemas, anomaly detection and other data-related tasks. . Keep it up! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/mlops/tfdv/2022/06/26/tensorflow_data_validation.html",
            "relUrl": "/mlops/tfdv/2022/06/26/tensorflow_data_validation.html",
            "date": " • Jun 26, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Predicting Bounding Boxes",
            "content": "Predicting Bounding Boxes . Welcome to Course 3, Week 1 Programming Assignment! . In this week&#39;s assignment, you&#39;ll build a model to predict bounding boxes around images. . You will use transfer learning on any of the pre-trained models available in Keras. | You&#39;ll be using the Caltech Birds - 2010 dataset. | . How to submit your work . Notice that there is not a &quot;submit assignment&quot; button in this notebook. . To check your work and get graded on your work, you&#39;ll train the model, save it and then upload the model to Coursera for grading. . Initial steps 0.1 Set up your Colab | 0.2 Set up the data location | 0.3 Choose the GPU Runtime | 0.4 Mount your drive | 0.5 Imports | . | 1. Visualization Utilities 1.1 Bounding Boxes Utilities | 1.2 Data and Predictions Utilities | . | 2. Preprocessing and Loading the Dataset 2.1 Preprocessing Utilities | 2.2 Visualize the prepared Data | 2.3 Loading the Dataset | . | 3. Define the Network Exercise 1 | Exercise 2 | Exercise 3 | Exercise 4 | Exercise 5 | . | 4. Training the Model Prepare to train the model | Exercise 6 | Fit the model to the data | Exercise 7 | . | 5. Validate the Model 5.1 Loss | 5.2 Save your Model | 5.3 Plot the Loss Function | 5.4 Evaluate performance using IoU | . | 6. Visualize Predictions | 7. Upload your model for grading | . . 0. Initial steps . . 0.1 Set up your Colab . As you cannot save the changes you make to this colab, you have to make a copy of this notebook in your own drive and run that. | You can do so by going to File -&gt; Save a copy in Drive. | Close this colab and open the copy which you have made in your own drive. Then continue to the next step to set up the data location. | . . Set up the data location . A copy of the dataset that you&#39;ll be using is stored in a publicly viewable Google Drive folder. You&#39;ll want to add a shortcut to it to your own Google Drive. . Go to this google drive folder named TF3 C3 W1 Data | Next to the folder name &quot;TF3 C3 W1 Data&quot; (at the top of the page beside &quot;Shared with me&quot;), hover your mouse over the triangle to reveal the drop down menu. | Use the drop down menu to select &quot;Add shortcut to Drive&quot; A pop-up menu will open up. | In the pop-up menu, &quot;My Drive&quot; is selected by default. Click the ADD SHORTCUT button. This should add a shortcut to the folder TF3 C3 W1 Data within your own Google Drive. | To verify, go to the left-side menu and click on &quot;My Drive&quot;. Scroll through your files to look for the shortcut named TF3 C3 W1 Data. If the shortcut is named caltech_birds2010, then you might have missed a step above and need to repeat the process. | . Please make sure the shortcut is created, as you&#39;ll be reading the data for this notebook from this folder. . . 0.3 Choose the GPU Runtime . Make sure your runtime is GPU (not CPU or TPU). And if it is an option, make sure you are using Python 3. You can select these settings by going to Runtime -&gt; Change runtime type -&gt; Select the above mentioned settings and then press SAVE | . . 0.4 Mount your drive . Please run the next code cell and follow these steps to mount your Google Drive so that it can be accessed by this Colab. . Run the code cell below. A web link will appear below the cell. | Please click on the web link, which will open a new tab in your browser, which asks you to choose your google account. | Choose your google account to login. | The page will display &quot;Google Drive File Stream wants to access your Google Account&quot;. Please click &quot;Allow&quot;. | The page will now show a code (a line of text). Please copy the code and return to this Colab. | Paste the code the textbox that is labeled &quot;Enter your authorization code:&quot; and hit &lt;Enter&gt; | The text will now say &quot;Mounted at /content/drive/&quot; | Please look at the files explorer of this Colab (left side) and verify that you can navigate to drive/MyDrive/TF3 C3 W1 Data/caltech_birds2010/0.1.1 . If the folder is not there, please redo the steps above and make sure that you&#39;re able to add the shortcut to the hosted dataset. | . from google.colab import drive drive.mount(&#39;/content/drive/&#39;, force_remount=True) . Mounted at /content/drive/ . . 0.5 Imports . import os, re, time, json import PIL.Image, PIL.ImageFont, PIL.ImageDraw import numpy as np import tensorflow as tf from matplotlib import pyplot as plt import tensorflow_datasets as tfds import cv2 . Store the path to the data. . Remember to follow the steps to set up the data location (above) so that you&#39;ll have a shortcut to the data in your Google Drive. | . data_dir = &quot;/content/drive/MyDrive/TF3 C3 W1 Data&quot; . . 1. Visualization Utilities . . 1.1 Bounding Boxes Utilities . We have provided you with some functions which you will use to draw bounding boxes around the birds in the image. . draw_bounding_box_on_image: Draws a single bounding box on an image. | draw_bounding_boxes_on_image: Draws multiple bounding boxes on an image. | draw_bounding_boxes_on_image_array: Draws multiple bounding boxes on an array of images. | . def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax, color=(255, 0, 0), thickness=5): &quot;&quot;&quot; Adds a bounding box to an image. Bounding box coordinates can be specified in either absolute (pixel) or normalized coordinates by setting the use_normalized_coordinates argument. Args: image: a PIL.Image object. ymin: ymin of bounding box. xmin: xmin of bounding box. ymax: ymax of bounding box. xmax: xmax of bounding box. color: color to draw bounding box. Default is red. thickness: line thickness. Default value is 4. &quot;&quot;&quot; image_width = image.shape[1] image_height = image.shape[0] cv2.rectangle(image, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, thickness) def draw_bounding_boxes_on_image(image, boxes, color=[], thickness=5): &quot;&quot;&quot; Draws bounding boxes on image. Args: image: a PIL.Image object. boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax). The coordinates are in normalized format between [0, 1]. color: color to draw bounding box. Default is red. thickness: line thickness. Default value is 4. Raises: ValueError: if boxes is not a [N, 4] array &quot;&quot;&quot; boxes_shape = boxes.shape if not boxes_shape: return if len(boxes_shape) != 2 or boxes_shape[1] != 4: raise ValueError(&#39;Input must be of size [N, 4]&#39;) for i in range(boxes_shape[0]): draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3], boxes[i, 2], color[i], thickness) def draw_bounding_boxes_on_image_array(image, boxes, color=[], thickness=5): &quot;&quot;&quot; Draws bounding boxes on image (numpy array). Args: image: a numpy array object. boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax). The coordinates are in normalized format between [0, 1]. color: color to draw bounding box. Default is red. thickness: line thickness. Default value is 4. display_str_list_list: a list of strings for each bounding box. Raises: ValueError: if boxes is not a [N, 4] array &quot;&quot;&quot; draw_bounding_boxes_on_image(image, boxes, color, thickness) return image . . 1.2 Data and Predictions Utilities . We&#39;ve given you some helper functions and code that are used to visualize the data and the model&#39;s predictions. . display_digits_with_boxes: This displays a row of &quot;digit&quot; images along with the model&#39;s predictions for each image. | plot_metrics: This plots a given metric (like loss) as it changes over multiple epochs of training. | . plt.rc(&#39;image&#39;, cmap=&#39;gray&#39;) plt.rc(&#39;grid&#39;, linewidth=0) plt.rc(&#39;xtick&#39;, top=False, bottom=False, labelsize=&#39;large&#39;) plt.rc(&#39;ytick&#39;, left=False, right=False, labelsize=&#39;large&#39;) plt.rc(&#39;axes&#39;, facecolor=&#39;F8F8F8&#39;, titlesize=&quot;large&quot;, edgecolor=&#39;white&#39;) plt.rc(&#39;text&#39;, color=&#39;a8151a&#39;) plt.rc(&#39;figure&#39;, facecolor=&#39;F0F0F0&#39;)# Matplotlib fonts MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), &quot;mpl-data/fonts/ttf&quot;) # utility to display a row of digits with their predictions def display_digits_with_boxes(images, pred_bboxes, bboxes, iou, title, bboxes_normalized=False): n = len(images) fig = plt.figure(figsize=(20, 4)) plt.title(title) plt.yticks([]) plt.xticks([]) for i in range(n): ax = fig.add_subplot(1, 10, i+1) bboxes_to_plot = [] if (len(pred_bboxes) &gt; i): bbox = pred_bboxes[i] bbox = [bbox[0] * images[i].shape[1], bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0]] bboxes_to_plot.append(bbox) if (len(bboxes) &gt; i): bbox = bboxes[i] if bboxes_normalized == True: bbox = [bbox[0] * images[i].shape[1],bbox[1] * images[i].shape[0], bbox[2] * images[i].shape[1], bbox[3] * images[i].shape[0] ] bboxes_to_plot.append(bbox) img_to_draw = draw_bounding_boxes_on_image_array(image=images[i], boxes=np.asarray(bboxes_to_plot), color=[(255,0,0), (0, 255, 0)]) plt.xticks([]) plt.yticks([]) plt.imshow(img_to_draw) if len(iou) &gt; i : color = &quot;black&quot; if (iou[i][0] &lt; iou_threshold): color = &quot;red&quot; ax.text(0.2, -0.3, &quot;iou: %s&quot; %(iou[i][0]), color=color, transform=ax.transAxes) # utility to display training and validation curves def plot_metrics(metric_name, title, ylim=5): plt.title(title) plt.ylim(0,ylim) plt.plot(history.history[metric_name],color=&#39;blue&#39;,label=metric_name) plt.plot(history.history[&#39;val_&#39; + metric_name],color=&#39;green&#39;,label=&#39;val_&#39; + metric_name) . . 2. Preprocess and Load the Dataset . . 2.1 Preprocessing Utilities . We have given you some helper functions to pre-process the image data. . read_image_tfds . Resizes image to (224, 224) | Normalizes image | Translates and normalizes bounding boxes | . def read_image_tfds(image, bbox): image = tf.cast(image, tf.float32) shape = tf.shape(image) factor_x = tf.cast(shape[1], tf.float32) factor_y = tf.cast(shape[0], tf.float32) image = tf.image.resize(image, (224, 224,)) image = image/127.5 image -= 1 bbox_list = [bbox[0] / factor_x , bbox[1] / factor_y, bbox[2] / factor_x , bbox[3] / factor_y] return image, bbox_list . read_image_with_shape . This is very similar to read_image_tfds except it also keeps a copy of the original image (before pre-processing) and returns this as well. . Makes a copy of the original image. | Resizes image to (224, 224) | Normalizes image | Translates and normalizes bounding boxes | . def read_image_with_shape(image, bbox): original_image = image image, bbox_list = read_image_tfds(image, bbox) return original_image, image, bbox_list . read_image_tfds_with_original_bbox . This function reads image from data | It also denormalizes the bounding boxes (it undoes the bounding box normalization that is performed by the previous two helper functions.) | . def read_image_tfds_with_original_bbox(data): image = data[&quot;image&quot;] bbox = data[&quot;bbox&quot;] shape = tf.shape(image) factor_x = tf.cast(shape[1], tf.float32) factor_y = tf.cast(shape[0], tf.float32) bbox_list = [bbox[1] * factor_x , bbox[0] * factor_y, bbox[3] * factor_x, bbox[2] * factor_y] return image, bbox_list . dataset_to_numpy_util . This function converts a dataset into numpy arrays of images and boxes. . This will be used when visualizing the images and their bounding boxes | . def dataset_to_numpy_util(dataset, batch_size=0, N=0): # eager execution: loop through datasets normally take_dataset = dataset.shuffle(1024) if batch_size &gt; 0: take_dataset = take_dataset.batch(batch_size) if N &gt; 0: take_dataset = take_dataset.take(N) if tf.executing_eagerly(): ds_images, ds_bboxes = [], [] for images, bboxes in take_dataset: ds_images.append(images.numpy()) ds_bboxes.append(bboxes.numpy()) return (np.array(ds_images), np.array(ds_bboxes)) . dataset_to_numpy_with_original_bboxes_util . This function converts a dataset into numpy arrays of original images | resized and normalized images | bounding boxes | . | This will be used for plotting the original images with true and predicted bounding boxes. | . def dataset_to_numpy_with_original_bboxes_util(dataset, batch_size=0, N=0): normalized_dataset = dataset.map(read_image_with_shape) if batch_size &gt; 0: normalized_dataset = normalized_dataset.batch(batch_size) if N &gt; 0: normalized_dataset = normalized_dataset.take(N) if tf.executing_eagerly(): ds_original_images, ds_images, ds_bboxes = [], [], [] for original_images, images, bboxes in normalized_dataset: ds_images.append(images.numpy()) ds_bboxes.append(bboxes.numpy()) ds_original_images.append(original_images.numpy()) return np.array(ds_original_images), np.array(ds_images), np.array(ds_bboxes) . . 2.2 Visualize the images and their bounding box labels . Now you&#39;ll take a random sample of images from the training and validation sets and visualize them by plotting the corresponding bounding boxes. . Visualize the training images and their bounding box labels . def get_visualization_training_dataset(): dataset, info = tfds.load(&quot;caltech_birds2010&quot;, split=&quot;train&quot;, with_info=True, data_dir=data_dir, download=False) print(info) visualization_training_dataset = dataset.map(read_image_tfds_with_original_bbox, num_parallel_calls=16) return visualization_training_dataset visualization_training_dataset = get_visualization_training_dataset() (visualization_training_images, visualization_training_bboxes) = dataset_to_numpy_util(visualization_training_dataset, N=10) display_digits_with_boxes(np.array(visualization_training_images), np.array([]), np.array(visualization_training_bboxes), np.array([]), &quot;training images and their bboxes&quot;) . tfds.core.DatasetInfo( name=&#39;caltech_birds2010&#39;, version=0.1.1, description=&#39;Caltech-UCSD Birds 200 (CUB-200) is an image dataset with photos of 200 bird species (mostly North American). The total number of categories of birds is 200 and there are 6033 images in the 2010 dataset and 11,788 images in the 2011 dataset. Annotations include bounding boxes, segmentation labels.&#39;, homepage=&#39;http://www.vision.caltech.edu/visipedia/CUB-200.html&#39;, features=FeaturesDict({ &#39;bbox&#39;: BBoxFeature(shape=(4,), dtype=tf.float32), &#39;image&#39;: Image(shape=(None, None, 3), dtype=tf.uint8), &#39;image/filename&#39;: Text(shape=(), dtype=tf.string), &#39;label&#39;: ClassLabel(shape=(), dtype=tf.int64, num_classes=200), &#39;label_name&#39;: Text(shape=(), dtype=tf.string), &#39;segmentation_mask&#39;: Image(shape=(None, None, 1), dtype=tf.uint8), }), total_num_examples=6033, splits={ &#39;test&#39;: 3033, &#39;train&#39;: 3000, }, supervised_keys=(&#39;image&#39;, &#39;label&#39;), citation=&#34;&#34;&#34;@techreport{WelinderEtal2010, Author = {P. Welinder and S. Branson and T. Mita and C. Wah and F. Schroff and S. Belongie and P. Perona}, Institution = {California Institute of Technology}, Number = {CNS-TR-2010-001}, Title = {{Caltech-UCSD Birds 200}}, Year = {2010} }&#34;&#34;&#34;, redistribution_info=, ) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. . Visualize the validation images and their bounding boxes . def get_visualization_validation_dataset(): dataset = tfds.load(&quot;caltech_birds2010&quot;, split=&quot;test&quot;, data_dir=data_dir, download=False) visualization_validation_dataset = dataset.map(read_image_tfds_with_original_bbox, num_parallel_calls=16) return visualization_validation_dataset visualization_validation_dataset = get_visualization_validation_dataset() (visualization_validation_images, visualization_validation_bboxes) = dataset_to_numpy_util(visualization_validation_dataset, N=10) display_digits_with_boxes(np.array(visualization_validation_images), np.array([]), np.array(visualization_validation_bboxes), np.array([]), &quot;validation images and their bboxes&quot;) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. . . 2.3 Load and prepare the datasets for the model . These next two functions read and prepare the datasets that you&#39;ll feed to the model. . They use read_image_tfds to resize, and normalize each image and its bounding box label. | They performs shuffling and batching. | You&#39;ll use these functions to create training_dataset and validation_dataset, which you will give to the model that you&#39;re about to build. | . BATCH_SIZE = 64 def get_training_dataset(dataset): dataset = dataset.map(read_image_tfds, num_parallel_calls=16) dataset = dataset.shuffle(512, reshuffle_each_iteration=True) dataset = dataset.repeat() dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(-1) return dataset def get_validation_dataset(dataset): dataset = dataset.map(read_image_tfds, num_parallel_calls=16) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.repeat() return dataset training_dataset = get_training_dataset(visualization_training_dataset) validation_dataset = get_validation_dataset(visualization_validation_dataset) . . 3. Define the Network . Bounding box prediction is treated as a &quot;regression&quot; task, in that you want the model to output numerical values. . You will be performing transfer learning with MobileNet V2. The model architecture is available in TensorFlow Keras. | You&#39;ll also use pretrained &#39;imagenet&#39; weights as a starting point for further training. These weights are also readily available | You will choose to retrain all layers of MobileNet V2 along with the final classification layers. | . Note: For the following exercises, please use the TensorFlow Keras Functional API (as opposed to the Sequential API). . . Exercise 1 . Please build a feature extractor using MobileNetV2. . First, create an instance of the mobilenet version 2 model . Please check out the documentation for MobileNetV2 | Set the following parameters: input_shape: (height, width, channel): input images have height and width of 224 by 224, and have red, green and blue channels. | include_top: you do not want to keep the &quot;top&quot; fully connected layer, since you will customize your model for the current task. | weights: Use the pre-trained &#39;imagenet&#39; weights. | . | . | Next, make the feature extractor for your specific inputs by passing the inputs into your mobilenet model. . For example, if you created a model object called some_model and have inputs stored in x, you&#39;d invoke the model and pass in your inputs like this: some_model(x) to get the feature extractor for your given inputs x. | . | . Note: please use mobilenet_v2 and not mobile_net or mobile_net_v3 . def feature_extractor(inputs): ### YOUR CODE HERE ### # Create a mobilenet version 2 model object mobilenet_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=&#39;imagenet&#39;)(inputs) # pass the inputs into this modle object to get a feature extractor for these inputs feature_extractor = mobilenet_model ### END CODE HERE ### # return the feature_extractor return feature_extractor . . Exercise 2 . Next, you&#39;ll define the dense layers to be used by your model. . You&#39;ll be using the following layers . GlobalAveragePooling2D: pools the features. | Flatten: flattens the pooled layer. | Dense: Add two dense layers: A dense layer with 1024 neurons and a relu activation. | A dense layer following that with 512 neurons and a relu activation. | . | . Note: Remember, please build the model using the Functional API syntax (as opposed to the Sequential API). . def dense_layers(features): ### YOUR CODE HERE ### # global average pooling 2d layer x = tf.keras.layers.GlobalAveragePooling2D()(features) # flatten layer x = tf.keras.layers.Flatten()(x) # 1024 Dense layer, with relu x = tf.keras.layers.Dense(1024, activation=&quot;relu&quot;)(x) # 512 Dense layer, with relu x = tf.keras.layers.Dense(512, activation=&quot;relu&quot;)(x) ### END CODE HERE ### return x . . Exercise 3 . Now you&#39;ll define a layer that outputs the bounding box predictions. . You&#39;ll use a Dense layer. | Remember that you have 4 units in the output layer, corresponding to (xmin, ymin, xmax, ymax). | The prediction layer follows the previous dense layer, which is passed into this function as the variable x. | For grading purposes, please set the name parameter of this Dense layer to be `bounding_box&#39; | . def bounding_box_regression(x): ### YOUR CODE HERE ### # Dense layer named `bounding_box` bounding_box_regression_output = bounding_box_regression_output = tf.keras.layers.Dense(units = &#39;4&#39;, name = &#39;bounding_box&#39;)(x) ### END CODE HERE ### return bounding_box_regression_output . . Exercise 4 . Now, you&#39;ll use those functions that you have just defined above to construct the model. . feature_extractor(inputs) | dense_layers(features) | bounding_box_regression(x) | . Then you&#39;ll define the model object using Model. Set the two parameters: . inputs | outputs | . def final_model(inputs): ### YOUR CODE HERE ### # features feature_cnn = feature_extractor(inputs) # dense layers last_dense_layer = dense_layers(feature_cnn) # bounding box bounding_box_output = bounding_box_regression(last_dense_layer) # define the TensorFlow Keras model using the inputs and outputs to your model model = tf.keras.Model(inputs = inputs, outputs = [bounding_box_output]) ### END CODE HERE ### return model . . Exercise 5 . Define the input layer, define the model, and then compile the model. . inputs: define an Input layer Set the shape parameter. Check your definition of feature_extractor to see the expected dimensions of the input image. | . | model: use the final_model function that you just defined to create the model. | compile the model: Check the Model documentation for how to compile the model. Set the optimizer parameter to Stochastic Gradient Descent using SGD When using SGD, set the momentum to 0.9 and keep the default learning rate. | . | Set the loss function of SGD to mean squared error (see the SGD documentation for an example of how to choose mean squared error loss). | . | . def define_and_compile_model(): ### YOUR CODE HERE ### # define the input layer inputs = tf.keras.Input(shape=(224,224,3)) # create the model model = final_model(inputs) # compile your model model.compile(tf.keras.optimizers.SGD(momentum=0.9), loss = {&#39;bounding_box&#39; : &#39;mse&#39; }, metrics = { &#39;bounding_box&#39; : &#39;mse&#39; }) ### END CODE HERE ### return model . Run the cell below to define your model and print the model summary. . model = define_and_compile_model() # print model layers model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_7 (InputLayer) [(None, 224, 224, 3)] 0 mobilenetv2_1.00_224 (Funct (None, 7, 7, 1280) 2257984 ional) global_average_pooling2d_2 (None, 1280) 0 (GlobalAveragePooling2D) flatten_1 (Flatten) (None, 1280) 0 dense_2 (Dense) (None, 1024) 1311744 dense_3 (Dense) (None, 512) 524800 bounding_box (Dense) (None, 4) 2052 ================================================================= Total params: 4,096,580 Trainable params: 4,062,468 Non-trainable params: 34,112 _________________________________________________________________ . Your expected model summary: . . . Train the Model . . 4.1 Prepare to Train the Model . You&#39;ll fit the model here, but first you&#39;ll set some of the parameters that go into fitting the model. . EPOCHS: You&#39;ll train the model for 50 epochs | BATCH_SIZE: Set the BATCH_SIZE to an appropriate value. You can look at the ungraded labs from this week for some examples. | length_of_training_dataset: this is the number of training examples. You can find this value by getting the length of visualization_training_dataset. Note: You won&#39;t be able to get the length of the object training_dataset. (You&#39;ll get an error message). | . | length_of_validation_dataset: this is the number of validation examples. You can find this value by getting the length of visualization_validation_dataset. Note: You won&#39;t be able to get the length of the object validation_dataset. | . | steps_per_epoch: This is the number of steps it will take to process all of the training data. . If the number of training examples is not evenly divisible by the batch size, there will be one last batch that is not the full batch size. | Try to calculate the number steps it would take to train all the full batches plus one more batch containing the remaining training examples. There are a couples ways you can calculate this. You can use regular division / and import math to use math.ceil() Python math module docs | Alternatively, you can use // for integer division, % to check for a remainder after integer division, and an if statement. | . | . | validation_steps: This is the number of steps it will take to process all of the validation data. You can use similar calculations that you did for the step_per_epoch, but for the validation dataset. . | . . Exercise 6 . EPOCHS = 50 ### START CODE HERE ### # Choose a batch size BATCH_SIZE = 64 # Get the length of the training set length_of_training_dataset = len(visualization_training_dataset) # Get the length of the validation set length_of_validation_dataset = len(visualization_validation_dataset) # Get the steps per epoch (may be a few lines of code) steps_per_epoch = length_of_training_dataset//BATCH_SIZE # get the validation steps (per epoch) (may be a few lines of code) validation_steps = length_of_validation_dataset//BATCH_SIZE if length_of_validation_dataset % BATCH_SIZE &gt; 0: validation_steps += 1 ### END CODE HERE . . 4.2 Fit the model to the data . Check out the parameters that you can set to fit the Model. Please set the following parameters. . x: this can be a tuple of both the features and labels, as is the case here when using a tf.Data dataset. Please use the variable returned from get_training_dataset(). | Note, don&#39;t set the y parameter when the x is already set to both the features and labels. | . | steps_per_epoch: the number of steps to train in order to train on all examples in the training dataset. | validation_data: this is a tuple of both the features and labels of the validation set. Please use the variable returned from get_validation_dataset() | . | validation_steps: teh number of steps to go through the validation set, batch by batch. | epochs: the number of epochs. | . If all goes well your model&#39;s training will start. . . Exercise 7 . # Fit the model, setting the parameters noted in the instructions above. history =model.fit(training_dataset,steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS) ### END CODE HERE ### . Epoch 1/50 46/46 [==============================] - 48s 668ms/step - loss: 0.1331 - mse: 0.1331 - val_loss: 0.5185 - val_mse: 0.5185 Epoch 2/50 46/46 [==============================] - 30s 654ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.3700 - val_mse: 0.3700 Epoch 3/50 46/46 [==============================] - 30s 652ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.2722 - val_mse: 0.2722 Epoch 4/50 46/46 [==============================] - 32s 692ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.2212 - val_mse: 0.2212 Epoch 5/50 46/46 [==============================] - 30s 659ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.1740 - val_mse: 0.1740 Epoch 6/50 46/46 [==============================] - 30s 655ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.1553 - val_mse: 0.1553 Epoch 7/50 46/46 [==============================] - 31s 675ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.1295 - val_mse: 0.1295 Epoch 8/50 46/46 [==============================] - 31s 668ms/step - loss: 0.0067 - mse: 0.0067 - val_loss: 0.1095 - val_mse: 0.1095 Epoch 9/50 46/46 [==============================] - 31s 674ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0974 - val_mse: 0.0974 Epoch 10/50 46/46 [==============================] - 30s 666ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0834 - val_mse: 0.0834 Epoch 11/50 46/46 [==============================] - 30s 668ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0748 - val_mse: 0.0748 Epoch 12/50 46/46 [==============================] - 31s 686ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0654 - val_mse: 0.0654 Epoch 13/50 46/46 [==============================] - 31s 670ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0577 - val_mse: 0.0577 Epoch 14/50 46/46 [==============================] - 30s 661ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0523 - val_mse: 0.0523 Epoch 15/50 46/46 [==============================] - 31s 673ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0502 - val_mse: 0.0502 Epoch 16/50 46/46 [==============================] - 30s 666ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0426 - val_mse: 0.0426 Epoch 17/50 46/46 [==============================] - 30s 668ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0399 - val_mse: 0.0399 Epoch 18/50 46/46 [==============================] - 31s 680ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0366 - val_mse: 0.0366 Epoch 19/50 46/46 [==============================] - 30s 668ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0341 - val_mse: 0.0341 Epoch 20/50 46/46 [==============================] - 33s 715ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0331 - val_mse: 0.0331 Epoch 21/50 46/46 [==============================] - 31s 668ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0317 - val_mse: 0.0317 Epoch 22/50 46/46 [==============================] - 31s 673ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0298 - val_mse: 0.0298 Epoch 23/50 46/46 [==============================] - 30s 661ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0287 - val_mse: 0.0287 Epoch 24/50 46/46 [==============================] - 31s 669ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0273 - val_mse: 0.0273 Epoch 25/50 46/46 [==============================] - 31s 676ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0273 - val_mse: 0.0273 Epoch 26/50 46/46 [==============================] - 31s 676ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0260 - val_mse: 0.0260 Epoch 27/50 46/46 [==============================] - 31s 672ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0256 - val_mse: 0.0256 Epoch 28/50 46/46 [==============================] - 32s 706ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0243 - val_mse: 0.0243 Epoch 29/50 46/46 [==============================] - 31s 685ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0241 - val_mse: 0.0241 Epoch 30/50 46/46 [==============================] - 31s 671ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0228 - val_mse: 0.0228 Epoch 31/50 46/46 [==============================] - 31s 675ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0228 - val_mse: 0.0228 Epoch 32/50 46/46 [==============================] - 30s 667ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0215 - val_mse: 0.0215 Epoch 33/50 46/46 [==============================] - 31s 670ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0213 - val_mse: 0.0213 Epoch 34/50 46/46 [==============================] - 31s 676ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0219 - val_mse: 0.0219 Epoch 35/50 46/46 [==============================] - 31s 670ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0209 - val_mse: 0.0209 Epoch 36/50 46/46 [==============================] - 31s 683ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0208 - val_mse: 0.0208 Epoch 37/50 46/46 [==============================] - 31s 671ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0207 - val_mse: 0.0207 Epoch 38/50 46/46 [==============================] - 31s 671ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0202 - val_mse: 0.0202 Epoch 39/50 46/46 [==============================] - 31s 677ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0196 - val_mse: 0.0196 Epoch 40/50 46/46 [==============================] - 31s 674ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0194 - val_mse: 0.0194 Epoch 41/50 46/46 [==============================] - 31s 675ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0195 - val_mse: 0.0195 Epoch 42/50 46/46 [==============================] - 31s 673ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0186 - val_mse: 0.0186 Epoch 43/50 46/46 [==============================] - 31s 673ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0190 - val_mse: 0.0190 Epoch 44/50 46/46 [==============================] - 31s 678ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0182 - val_mse: 0.0182 Epoch 45/50 46/46 [==============================] - 30s 667ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0185 - val_mse: 0.0185 Epoch 46/50 46/46 [==============================] - 31s 688ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0179 - val_mse: 0.0179 Epoch 47/50 46/46 [==============================] - 31s 670ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.0176 - val_mse: 0.0176 Epoch 48/50 46/46 [==============================] - 31s 673ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0177 - val_mse: 0.0177 Epoch 49/50 46/46 [==============================] - 30s 666ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.0175 - val_mse: 0.0175 Epoch 50/50 46/46 [==============================] - 31s 677ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0175 - val_mse: 0.0175 . . 5. Validate the Model . 5.1 Loss . You can now evaluate your trained model&#39;s performance by checking its loss value on the validation set. . loss = model.evaluate(validation_dataset, steps=validation_steps) print(&quot;Loss: &quot;, loss) . 48/48 [==============================] - 12s 242ms/step - loss: 0.0175 - mse: 0.0175 Loss: [0.017487820237874985, 0.017487820237874985] . . 5.2 Save your Model for Grading . When you have trained your model and are satisfied with your validation loss, please you save your model so that you can upload it to the Coursera classroom for grading. . model.save(&quot;birds.h5&quot;) . from google.colab import files files.download(&quot;birds.h5&quot;) . . 5.3 Plot Loss Function . You can also plot the loss metrics. . plot_metrics(&quot;loss&quot;, &quot;Bounding Box Loss&quot;, ylim=0.2) . . 5.4 Evaluate performance using IoU . You can see how well your model predicts bounding boxes on the validation set by calculating the Intersection-over-union (IoU) score for each image. . You&#39;ll find the IoU calculation implemented for you. | Predict on the validation set of images. | Apply the intersection_over_union on these predicted bounding boxes. | . def intersection_over_union(pred_box, true_box): xmin_pred, ymin_pred, xmax_pred, ymax_pred = np.split(pred_box, 4, axis = 1) xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1) #Calculate coordinates of overlap area between boxes xmin_overlap = np.maximum(xmin_pred, xmin_true) xmax_overlap = np.minimum(xmax_pred, xmax_true) ymin_overlap = np.maximum(ymin_pred, ymin_true) ymax_overlap = np.minimum(ymax_pred, ymax_true) #Calculates area of true and predicted boxes pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred) true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true) #Calculates overlap area and union area. overlap_area = np.maximum((xmax_overlap - xmin_overlap),0) * np.maximum((ymax_overlap - ymin_overlap), 0) union_area = (pred_box_area + true_box_area) - overlap_area # Defines a smoothing factor to prevent division by 0 smoothing_factor = 1e-10 #Updates iou score iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor) return iou #Makes predictions original_images, normalized_images, normalized_bboxes = dataset_to_numpy_with_original_bboxes_util(visualization_validation_dataset, N=500) predicted_bboxes = model.predict(normalized_images, batch_size=32) #Calculates IOU and reports true positives and false positives based on IOU threshold iou = intersection_over_union(predicted_bboxes, normalized_bboxes) iou_threshold = 0.5 print(&quot;Number of predictions where iou &gt; threshold(%s): %s&quot; % (iou_threshold, (iou &gt;= iou_threshold).sum())) print(&quot;Number of predictions where iou &lt; threshold(%s): %s&quot; % (iou_threshold, (iou &lt; iou_threshold).sum())) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. . Number of predictions where iou &gt; threshold(0.5): 246 Number of predictions where iou &lt; threshold(0.5): 254 . . 6. Visualize Predictions . Lastly, you&#39;ll plot the predicted and ground truth bounding boxes for a random set of images and visually see how well you did! . n = 10 indexes = np.random.choice(len(predicted_bboxes), size=n) iou_to_draw = iou[indexes] norm_to_draw = original_images[indexes] display_digits_with_boxes(original_images[indexes], predicted_bboxes[indexes], normalized_bboxes[indexes], iou[indexes], &quot;True and Predicted values&quot;, bboxes_normalized=True) .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/fastpages/jupyter/coursera/2022/06/20/object_localization.html",
            "relUrl": "/fastpages/jupyter/coursera/2022/06/20/object_localization.html",
            "date": " • Jun 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Part 1: Building Deep Learning Models by using Keras and Tensorflow",
            "content": "Introduction . Some random notes by reading Deep Learning with Python by Francois Chollet. This book focusses on developing Deep Learning Models using Keras and Tensorflow. I found this book is very good at explaining Deep learning.I try making notes for important chapters that i think it is essential for improving myself in developing deep learning models. I took some source code from the book as a reference. . Different ways of building models in Keras . Sequential model where the the layers are stacked each other to create the DL architecture. | The functional API where focussing on graph-like model architecture that represents a nice usabilibity and flexibility. | Model Subclassing where a low level option to write model from scratch. | . Sequential Models . from tensorflow import keras from tensorflow.keras import layers model =keras.Sequential([ layers.Dense(64,activation=&quot;relu), layers.Dense(10,activation=&quot;softmax&quot;) ]) . or . model = keras.Sequential() model.add(layers.Dense(64,activation=&quot;relu)) model.add(layers.Dense(10,activation=&quot;softmax&quot;)) . We see that sequential model create the 64 multidimensional representation based on input data to create 10 output value using softmax activation function to get predicted probabilities. You can check model.summary() for looking at the architecture of your model. You can also name the models and layers by dding name parameters in each process in your architecture by using string. . model = keras.Sequential(name=&quot;my_example_model&quot;) model.add(layers.Dense(64,activation=&quot;relu),name=&quot;my_first_layer&quot;) model.add(layers.Dense(10,activation=&quot;softmax&quot;,name=&quot;my_last_layer&quot;)) . Functional API . While sequential model constainst are for simple model that can express models with single input and single output where applying one layer after the other sequentially. There is a situation when we face a problem to encounter models with multiple inputs like image and its metadata and multiple output to predict different things about data. Imagine you are building a system to rank customer support tickets by priority and route them by departments based on 3 inputs . The title of the ticket(text input) | The text body of the ticket(text input) | Any tags added by the user(categorical input encode to one-hot encing) and create 2 outputs | The priority score of the ticket , a scalr between 0-1(Sigmoid ouput) | The deparment that should handle the ticket (Softax over the set of departments) | . vocabulary_size =10000 num_tags =100 num_departments =4 # Define model inputs title =keras,Input(shape=(vocabulary_size,),name=&quot;title&quot;) text_body = keras.Input(shape=(vocabulary_size,),name=&quot;text_body&quot;) tags = keras.Input(shape=(num_tags),name=&quot;tags&quot;) #Combine input features into a single tensor by concatenating them features = layers.Concatenate()([title,text_body,tags]) features =layers.Dense(64,activation=&quot;relu&quot;)(features) # Define Model outputs priority = layers.Dense(1,activation=&quot;sigmoid&quot;,names=&quot;priority&quot;)(features) department = layers.Dense(num_departments,activation=&quot;softmax&quot;,name=&quot;departments&quot;)(features) model = keras.Model(inputs=[title,text_body,tags], outputs=[priority,department]) . You can see the structure of your model in a nice graph by plotting the moarchitecture by using keras.utils.plt_model(instances of model object).The advantage of using functional API is enabling us to do feature extractions that reuse intermediate features from another model. If we want to add another output to the precious model by estimating how long a given issue ticket will take to ressolve by difficulty rating(quick,medium,difficult). we dont need to recreate the model from scratch. We can start fro intermediate features of previous model . features = model.layers[4].output difficulty = layers.Dense(3,activation =&quot;softmax&quot;,name=&quot;difficulty&quot;)(features) new_model = keras.Model(inputs =[title,text_body,tags], outputs=[priority,department,difficulty]) keras.utils.plot_model(new_model,&quot;updated_ticket_classifier.png&quot;,show_shapes=True) . Subclassing Model class . This is the advanced of building model pattern by subclassing Layer class to create custom layers. This is like subclassing nn.Model in pytorch(if you experienced in pytorch) . init for defining the layers the model will use | call() for defining the forward pass of the model by reusing previously layers created. | . class CustomerTicketModel(keras.Model): def __init__(self,num_departments): super.__init__() self.concat_layer = layers.Concatenate() self.mixing_layer = layers.Dense(64,activation=&quot;relu&quot;) self.priority_scorer = layers.Dense(1,activation=&quot;sigmoid&quot;) self.departments_classifier = layers.Dense(num_departments,activation=&quot;sofmax&quot;) def call(self,inputs): title = inputs[&quot;title&quot;] text_body = inputs[&quot;text_body&quot;] tags = inputs[&quot;tags&quot;] features =self.concat_layer[title,text_body,tags] features =self.mixing_layer(features) priority =self.priority_scorer)(features) department = self.department_classifier(features) return priority,department model =CustomerTicketModel(num_departments=4) priority,department =model({&quot;title&quot;:title_data,&quot;text_body&quot;:text_body_data,&quot;tags&quot;:ttags_data}) ## Do compile(),fit(),evalueate and predict() as usual. . Writing own callbacks (loss,EarlyStopping,ModelCheckPoint) . There are a few methods available in keras.callbacks.Callback class . on_epoch_begin(epoch,logs) for calling at the start of every epoch | on_epoch_end(epoch,logs) for calling at the end of every epoch | on_batch_begin(batch,logs) for calling tight before processing each batch | on_batch_end(batch,logs) for calling right after processing each batch | on_train_begin(logs) for calling at the start of training | on_train_end(logs) for calling at the end of training | . from matplotlib import pyplot as plt ## Transform Training fit() ModelCheckPoint and EarlyStopping python from tensorflow.keras.datasets import mnist def get_mnist_model(): inputs = keras.Input(shape=(28*28,)) features = layers.Dense(512,activation=&quot;relu&quot;)(inputs) features =layers.Dropout(0.5)(features) outputs =layers.Dense(10,activation=&quot;softmax&quot;)(features) model =keras.Model(inputs,outputs) return model (images,labels),(test_images,test_labels) =mnist.load_data() images = images.reshape((60000,28*28)).astype(&quot;float32&quot;)/255 test_images =test_images.reshape((10000,28*28)).astype(&quot;float32&quot;)/255 train_images,val_images = images[10000:],images[:10000] train_labels,val_labels = labels[10000:],labels[:10000] model = get_mnist_model() model.compile(optimizer=&quot;rmsprop&quot;,loss=&quot;sparse_categorical_crossentropy&quot;, metrics =[&quot;accuracy&quot;]) model.fit(train_images,train_labels,epoch=3,validation_data=[test_images,test_labels]) test_metrics =model.evaluate(test_images,val_label) predictions =model.predict(test_images) callbacks_list =[ keras.callbacks.EarlyStopping( monitor=&quot;val_accuracy&quot;, patience=2,), keras.callbacks.ModelCheckPoint( filepath=&quot;checkpoint_path.keras&quot;, monitor =&quot;val_loss&quot;, save_best_only=True ) ] model = get_mnist_model() model.compile(optimizer=&quot;rmsprop&quot;, loss =&quot;sparse_categorical_crossentropy&quot;, metrics =[&quot;accuracy&quot;]) model.fit(train_images,train_labels,epochs=10,calbacks=callbacks_list, validation_data=[val_images,val_labels]) #Load the model model = keras.models.load_model(&quot;checkpoint_path.keras&quot;) class lossHistory(keras.callbacks.Callback): def on_train_begin(self_logs): self.per_batch_losses=[] def on_batch_end(self,batch,logs): self.per_batch_losses.append(logs.get(&quot;loss&quot;)) def on_epoch_end(self,epoch,logs): plt.clf() plt.plot(range(len(self.per_batch_losses)),self.per_batch_losses,label=&quot;Training loss for each batch&quot;) plt.xlabel(f&quot;Batch (epoch{epoch})&quot;) self.per_batch_losses =[] model = model_get_mnist_model() model.compile(optimizer=&quot;rmsprop&quot;, loss =&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) model.fit(train_images,train_labels,epochs=10,callbacks=[LossHistory()], validation_data=[val_images,val_labels]) . A complete training and evaluation loop from scratch(fit() and evaluate() method) . # Training Process model = get_mnist_model() loss_fn =keras.losses.SparseCategoricalCrossentropy() optimizer =keras.optimizers.RMSprop() metrics = [keras.metrics.SparseCategoricalAccuracy()] loss_tracking_metrics =keras.metrics.Mean() def train_step(inputs,targets): with tf.GradientTape() as tape: # training=True during training predictions =model(inputs,training=True) loss =loss_fn(targets,predictions) gradients = tape.gradients(loss,,model.trainable_weights) optimizer.apply_gradients(zip(gradients,model.trainable_weights)) logs ={} for metric in metrics: metric.update_state(targets,predictions) logs[metric.name] =metric.result() loss_tracking_metric.update_state(loss) logs[&quot;loss&quot;] = loss_tracking_metric.result() return logs def reset_metrics(): for metric in metrics: metric.reset_state() loss_tracking_metric.reset_state() training_dataset =tf.data.Dataset.from_tensor_slices((train_images,train_labels)) training_dataset = training_dataset.batch(32) epochs =3 for epoch in range(epochs): reset_metrics() for input_batch,targets_batch in training_dataset: logs =train_step(input_batchs,targets_batch) print(f&quot;Results at the end of epoch {epoch}&quot;) for key,value in logs.items(): print(f&quot;....{key}: {value:.4f}&quot;) # Evaluation loop def test_step(inputs,targets): #training=False for evaluation predictions= model(inputs,training=False) loss =loss_fn(targets,predictions) logs ={} for metric in metrics: metric.update_state(targets,predictions) logs[&quot;val&quot; +metric.name] = metric.result() loss_tracking_metric.update_state(loss) logs[&quot;val_loss&quot;] = loss_tracking_metrics.result() return logs val_dataset = tf.data.Dataset.from_tesor_slices((val_images,val_labels)) val_dataset =val_dataset.batch(32) reset_metrics() for inputs_batch,targets_batch in val_dataset: logs = test_step(inputs_batch,targets_batch) print(&quot;Evaluation results.&quot;) for key,value in logs.items(): print(f&quot;...{key} : {value:.4f}&quot;) .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/tensorflow/keras/2022/05/31/tensofrflow-keras.html",
            "relUrl": "/tensorflow/keras/2022/05/31/tensofrflow-keras.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "NLP Approach using Word Embedding",
            "content": "This is a minimal yet very powerful approach of NLP problems that you can use on Kaggle competition. I got my attention to the book written by one of Kaggle Grandmaster in his book called Approaching (Almost) Any Machine Learning Problem. I am so interested in how the author approaches the problems for each case in Kaggle Competition started by Supervised and unsupervised problems. So, NLP is one of the problems in the competition in Kaggle. We know that many approaches that we can do to preprocess text data from Bag of Words, TFIDF to Word Embedding. I am interested to dive deeper into Word Embedding because this approach yield a better result compared to BOW or Tfidf approach based on this dataset. You can find the comparison by reading this book. I found this approach quite useful for my NLP competition. Before reading this NLP approach, I hope the readers have the fundamentals concepts of pytorch, data visualization libraries and NLP concept to know the code better. . Import Data . import pandas as pd movies = pd.read_csv(&quot;imdb.csv&quot;) movies.head() . review sentiment . 0 One of the other reviewers has mentioned that ... | positive | . 1 A wonderful little production. &lt;br /&gt;&lt;br /&gt;The... | positive | . 2 I thought this was a wonderful way to spend ti... | positive | . 3 Basically there&#39;s a family where a little boy ... | negative | . 4 Petter Mattei&#39;s &quot;Love in the Time of Money&quot; is... | positive | . Check Proportion of target . movies.sentiment.value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . Create Cross Validation . import pandas as pd from sklearn import model_selection if __name__==&quot;__main__&quot;: df = pd.read_csv(&quot;imdb.csv&quot;) df.sentiment = df.sentiment.apply(lambda x: 1 if x == &quot;positive&quot; else 0) df[&quot;kfold&quot;] =-1 df = df.sample(frac=1).reset_index(drop=True) y = df.sentiment.values kf = model_selection.StratifiedKFold(n_splits=5) for f,(t_,v_) in enumerate(kf.split(X=df,y=y)): df.loc[v_,&quot;kfold&quot;] =f df.to_csv(&quot;imdb_folds.csv&quot;,index=False) . movies_folds = pd.read_csv(&quot;imdb_folds.csv&quot;) movies_folds.head() . review sentiment kfold . 0 I enjoyed Erkan &amp; Stefan  a cool and fast sto... | 1 | 0 | . 1 The only reason I rated this film as 2 is beca... | 0 | 0 | . 2 One of those movies where you take bets on who... | 0 | 0 | . 3 This series was just like what you would expec... | 1 | 0 | . 4 While many people found this film simply too s... | 1 | 0 | . There is one additional features called kfold. . Word Embedding . import numpy as np def sentence_to_vec(s,embedding_dict,stop_words,tokenizer): words =str(s).lower() words =tokenizer(words) words = [ w for w in words if w not in stop_words] words = [w for w in words if w.alpha()] M =[] for w in words: if w in embedding_dict: M.append(embedding_dict[w]) if len(M)==0: return np.zeros(300) M = np.array(M) v = M.sum() return v/np.sqrt((v**2).sum()) . Create Dataset in pytorch based on model in our dataset . import torch class IMDBDataset: def __init__(self,reviews,targets): self.reviews =reviews self.targets = targets def __len__(self): return len(self.reviews) def __getitem__(self,item): review =self.reviews[item,:] target =self.target[item] return { &quot;review&quot;: torch.tensor(review,dtype=torch.long), &quot;target&quot;: torch.tensor(target,dtype=torch.float) } . Create Model . import torch.nn as nn class LSTM(nn.Module): def __init__(self,embedding_matrix): super(LSTM,self).__init__() num_words =embedding_matrix.shape[0] embed_dim= embedding_matrix.shape[1] self.embedding = nn.Embedding( num_embeddings = num_words, embedding_dim=embed_dim ) self.embedding.weight = nn.Parameter( torch.tensor( embedding_matrix, dtype=torch.float32 ) ) self.embedding.weight.requires_grad=False self.lstm = nn.LSTM( embed_dim, 128, bidirectional=True, batch_first=True ) self.out = nn.Linear(512,1) def forward(self,x): x = self.embedding(x) x,_ = self.lstm(x) avg_pool =torch.mean(x,1) max_pool, _ = torch.max(x,1) out = torch.cat((avg_pool,maxpool),1) out = self.out(out) return out . Create Training Function for Modelling . def train(data_loader,model,optimizer,device): model.train() for data in data_loader: reviews = data[&quot;review&quot;] targets = data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.float) optimizer.zero_grad() predictions = model(reviews) loss =nn.BCEWithLogitsLoss()( predictions, targets.view(-1,1) ) loss.bakward() optimizer.step() . Create Evaluation for Modelling . def evaluate(data_loader,model,device): final_predictions =[] final_targets = [] model.eval() with torch.no_grad(): for data in data_loader: reviews =data[&quot;review&quot;] targets =data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.long) predictions = model(reviews) predictions = predictions.cpu().numpy().tolist() targets = data[&quot;target&quot;].cpu().numpy.tolist() final_predictions.extend(predictions) final_targets.extend(targets) return final_predictions,final_targets . Word Embedding Creation . import io #from tensorflow.keras import import tensorflow as tf def load_vectors(fname): fin = io.open( fname, &quot;r&quot;, encoding=&quot;utf-8&quot;, newline=&quot; n&quot;, errors=&quot;ignore&quot; ) n,d = map(int,fin.readline().split()) data ={} for line in fin: tokens = line.rstrip().split(&#39; &#39;) data[tokens[0]] = list(map(float,tokens[1:])) return data def create_embedding_matrix(world_index,embedding_dict): embedding_matrix = np.zeros((len(word_index)+1,300)) for word , i in word_index.items(): if word in embedding_dict: embedding_dict[i] = embedding_dict[word] return embedding_matrix def run(df,fold): train_df = df[df.kfold != fold].reset_index(drop=True) valid_df = df[df.kfold ==fold].reset_index(drop=True) print(&quot;Fitting tokenizer&quot;) tokenizer = tf.keras.preprocessing.text.Tokenizer() tokenizer.fit_on_texts(df.review.values.tolist()) xtrain = tokenizer.texts_to_sequences(train_df.review.values) xtest = tokenizer.texts_to_sequences(valid_df.review.values) xtrain = tf.keras.preprocessing.sequence.pad_sequences( xtrain,maxlen=128 ) xtest = tf.keras.preprocessing.sequence.pad_sequences( xtest,maxlen=128 ) train_dataset = IMDBDataset( reviews =xtrain, targets = train_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( train_dataset, batch_size =16, num_workers=2 ) valid_dataset =IMDBDataset( reviews =xtest, targets = valid_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size =8, num_workers=1 ) print(&quot;Loading Embeddings&quot;) # you can suit based on where you put your vec fasttext embedding_dict = load_vectors(&quot;crawl-300d-2M.vec/crawl-300d-2M.vec&quot;) embedding_matrix = create_embedding_matrix( tokenizer.word_index,embedding_dict ) device =torch.device(&quot;cuda&quot;) model =LSTM(embedding_matrix) model.to(device) optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) print(&quot;Training Model&quot;) best_accuracy =0 early_stopping_counter =0 for epoch in range(10): train(train_data_loader,model,optimizer,device) outputs,targets = evaluate(valid_data_loader,model,device) outputs = np.array(outputs) &gt;=0.5 accuracy = metrics.accuracy_score(targets,outputs) print(f&quot;{fold}, Epoch {epoch}, Accuracy Score ={accuracy}&quot;) if accuracy &gt; best_accuracy: best_accuracy = accuracy else: early_stopping_counter +=1 if early_stopping_counter &gt; 2: break if __name__ == &quot;__main__&quot;: df = pd.read_csv(&quot;imdb_folds.csv&quot;) run(df,0) run(df,1) run(df,2) run(df,3) run(df,4) . Fitting tokenizer Loading Embeddings . The choice of Machine learning algorithms will determine the quality of our prediction score. However, The simple model will also determine how fast the training process compared to state of art of ML algorithms. If we have laptop/computer with better GPU can help the training process. But in real life, Simple models with better preprocessing will have prediction score that is not too dfferent with newest ML algorithms one. So, It is better to discuss with the stakeholders for improving the models based on business metrics. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/nlp/wordembedding/jupyter/2022/04/06/approachingnlpusingwordembedding.html",
            "relUrl": "/nlp/wordembedding/jupyter/2022/04/06/approachingnlpusingwordembedding.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Ensembling Implementation",
            "content": "Version 1.0.1 . import numpy as np import pandas as pd import sklearn import scipy.sparse import lightgbm for p in [np, pd, scipy, sklearn, lightgbm]: print (p.__name__, p.__version__) . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 lightgbm 2.0.6 . Important! There is a huge chance that the assignment will be impossible to pass if the versions of lighgbm and scikit-learn are wrong. The versions being tested: . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 ligthgbm 2.0.6 . To install an older version of lighgbm you may use the following command: . pip uninstall lightgbm pip install lightgbm==2.0.6 . Ensembling . In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking. . We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what&#39;s happening. . import pandas as pd import numpy as np import gc import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 600) pd.set_option(&#39;display.max_columns&#39;, 50) import lightgbm as lgb from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from tqdm import tqdm_notebook from itertools import product def downcast_dtypes(df): &#39;&#39;&#39; Changes column types in the dataframe: `float64` type to `float32` `int64` type to `int32` &#39;&#39;&#39; # Select columns to downcast float_cols = [c for c in df if df[c].dtype == &quot;float64&quot;] int_cols = [c for c in df if df[c].dtype == &quot;int64&quot;] # Downcast df[float_cols] = df[float_cols].astype(np.float32) df[int_cols] = df[int_cols].astype(np.int32) return df . Load data subset . Let&#39;s load the data from the hard drive first. . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) shops = pd.read_csv(&#39;../readonly/final_project_data/shops.csv&#39;) items = pd.read_csv(&#39;../readonly/final_project_data/items.csv&#39;) item_cats = pd.read_csv(&#39;../readonly/final_project_data/item_categories.csv&#39;) . And use only 3 shops for simplicity. . sales = sales[sales[&#39;shop_id&#39;].isin([26, 27, 28])] . Get a feature matrix . We now need to prepare the features. This part is all implemented for you. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;shop_id&#39;].unique() cur_items = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) # Turn the grid into a dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) # Groupby data to get shop-item-month aggregates gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) # Fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] # Join it to the grid all_data = pd.merge(grid, gb, how=&#39;left&#39;, on=index_cols).fillna(0) # Same as above but with shop-month aggregates gb = sales.groupby([&#39;shop_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_shop&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Same as above but with item-month aggregates gb = sales.groupby([&#39;item_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_item&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1] == &#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;item_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Downcast dtypes from 64 to 32 bit to save memory all_data = downcast_dtypes(all_data) del grid, gb gc.collect(); . /opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs) . After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago. . cols_to_rename = list(all_data.columns.difference(index_cols)) shift_range = [1, 2, 3, 4, 5, 12] for month_shift in tqdm_notebook(shift_range): train_shift = all_data[index_cols + cols_to_rename].copy() train_shift[&#39;date_block_num&#39;] = train_shift[&#39;date_block_num&#39;] + month_shift foo = lambda x: &#39;{}_lag_{}&#39;.format(x, month_shift) if x in cols_to_rename else x train_shift = train_shift.rename(columns=foo) all_data = pd.merge(all_data, train_shift, on=index_cols, how=&#39;left&#39;).fillna(0) del train_shift # Don&#39;t use old data from year 2013 all_data = all_data[all_data[&#39;date_block_num&#39;] &gt;= 12] # List of all lagged features fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] # We will drop these at fitting stage to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + [&#39;date_block_num&#39;] # Category for each item item_category_mapping = items[[&#39;item_id&#39;,&#39;item_category_id&#39;]].drop_duplicates() all_data = pd.merge(all_data, item_category_mapping, how=&#39;left&#39;, on=&#39;item_id&#39;) all_data = downcast_dtypes(all_data) gc.collect(); . . To this end, we&#39;ve created a feature matrix. It is stored in all_data variable. Take a look: . all_data.head() . shop_id item_id date_block_num target target_shop target_item target_lag_1 target_item_lag_1 target_shop_lag_1 target_lag_2 target_item_lag_2 target_shop_lag_2 target_lag_3 target_item_lag_3 target_shop_lag_3 target_lag_4 target_item_lag_4 target_shop_lag_4 target_lag_5 target_item_lag_5 target_shop_lag_5 target_lag_12 target_item_lag_12 target_shop_lag_12 item_category_id . 0 28 | 10994 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 1.0 | 6454.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37 | . 1 28 | 10992 | 12 | 3.0 | 6949.0 | 4.0 | 3.0 | 7.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 37 | . 2 28 | 10991 | 12 | 1.0 | 6949.0 | 5.0 | 1.0 | 3.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 2.0 | 4.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 40 | . 3 28 | 10988 | 12 | 1.0 | 6949.0 | 2.0 | 2.0 | 5.0 | 8499.0 | 4.0 | 5.0 | 6454.0 | 5.0 | 6.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . 4 28 | 11002 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . Train/test split . For a sake of the programming assignment, let&#39;s artificially split the data into train and test. We will treat last month data as the test set. . dates = all_data[&#39;date_block_num&#39;] last_block = dates.max() print(&#39;Test `date_block_num` is %d&#39; % last_block) . Test `date_block_num` is 33 . dates_train = dates[dates &lt; last_block] dates_test = dates[dates == last_block] X_train = all_data.loc[dates &lt; last_block].drop(to_drop_cols, axis=1) X_test = all_data.loc[dates == last_block].drop(to_drop_cols, axis=1) y_train = all_data.loc[dates &lt; last_block, &#39;target&#39;].values y_test = all_data.loc[dates == last_block, &#39;target&#39;].values . First level models . You need to implement a basic stacking scheme. We have a time component here, so we will use scheme f) from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let&#39;s see how we get test meta-features first. . Test meta-features . Firts, we will run linear regression on numeric columns and get predictions for the last month. . lr = LinearRegression() lr.fit(X_train.values, y_train) pred_lr = lr.predict(X_test.values) print(&#39;Test R-squared for linreg is %f&#39; % r2_score(y_test, pred_lr)) . Test R-squared for linreg is 0.743180 . And the we run LightGBM. . lgb_params = { &#39;feature_fraction&#39;: 0.75, &#39;metric&#39;: &#39;rmse&#39;, &#39;nthread&#39;:1, &#39;min_data_in_leaf&#39;: 2**7, &#39;bagging_fraction&#39;: 0.75, &#39;learning_rate&#39;: 0.03, &#39;objective&#39;: &#39;mse&#39;, &#39;bagging_seed&#39;: 2**7, &#39;num_leaves&#39;: 2**7, &#39;bagging_freq&#39;:1, &#39;verbose&#39;:0 } model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100) pred_lgb = model.predict(X_test) print(&#39;Test R-squared for LightGBM is %f&#39; % r2_score(y_test, pred_lgb)) . Test R-squared for LightGBM is 0.738391 . Finally, concatenate test predictions to get test meta-features. . X_test_level2 = np.c_[pred_lr, pred_lgb] . Train meta-features . Now it is your turn to write the code. You need to implement scheme f) from the reading material. Here, we will use duration T equal to month and M=15. . That is, you need to get predictions (meta-features) from linear regression and LightGBM for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models. . dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])] # That is how we get target for the 2nd level dataset y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])] . X_train_level2 = np.zeros([y_train_level2.shape[0], 2]) # Now fill `X_train_level2` with metafeatures for cur_block_num in [27, 28, 29, 30, 31, 32]: print(cur_block_num) &#39;&#39;&#39; 1. Split `X_train` into parts Remember, that corresponding dates are stored in `dates_train` 2. Fit linear regression 3. Fit LightGBM and put predictions 4. Store predictions from 2. and 3. in the right place of `X_train_level2`. You can use `dates_train_level2` for it Make sure the order of the meta-features is the same as in `X_test_level2` &#39;&#39;&#39; # YOUR CODE GOES HERE X_train_meta = all_data.loc[dates &lt; cur_block_num].drop(to_drop_cols, axis=1) X_test_meta = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1) y_train_meta = all_data.loc[dates &lt; cur_block_num, &#39;target&#39;].values y_test_meta = all_data.loc[dates == cur_block_num, &#39;target&#39;].values lr.fit(X_train_meta.values, y_train_meta) X_train_level2[dates_train_level2 == cur_block_num, 0] = lr.predict(X_test_meta.values) model = lgb.train(lgb_params, lgb.Dataset(X_train_meta, label=y_train_meta), 100) X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_meta) # Sanity check assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988, 1.38811989])) . 27 28 29 30 31 32 . Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig scatter plot between the two metafeatures. Plot the scatter plot below. . plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1]) . &lt;matplotlib.collections.PathCollection at 0x7fa38c41ca58&gt; . Ensembling . Now, when the meta-features are created, we can ensemble our first level models. . Simple convex mix . Let&#39;s start with simple linear convex mix: . $$ mix= alpha cdot text{linreg_prediction}+(1- alpha) cdot text{lgb_prediction} $$We need to find an optimal $ alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $ alpha$ out of alphas_to_try array. Remember, that you need to use train meta-features (not test) when searching for $ alpha$. . alphas_to_try = np.linspace(0, 1, 1001) # YOUR CODE GOES HERE r2_scores = np.array([r2_score(y_train_level2, np.dot(X_train_level2, [alpha, 1 - alpha])) for alpha in alphas_to_try]) best_alpha = alphas_to_try[r2_scores.argmax()] # YOUR CODE GOES HERE r2_train_simple_mix = r2_scores.max() # YOUR CODE GOES HERE print(&#39;Best alpha: %f; Corresponding r2 score on train: %f&#39; % (best_alpha, r2_train_simple_mix)) . Best alpha: 0.765000; Corresponding r2 score on train: 0.627255 . Now use the $ alpha$ you&#39;ve found to compute predictions for the test set . test_preds = best_alpha * pred_lr + (1 - best_alpha) * pred_lgb # YOUR CODE GOES HERE r2_test_simple_mix = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Test R-squared for simple mix is %f&#39; % r2_test_simple_mix) . Test R-squared for simple mix is 0.781144 . Stacking . Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above. . lr.fit(X_train_level2, y_train_level2) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) . Compute R-squared on the train and test sets. . train_preds = lr.predict(X_train_level2) # YOUR CODE GOES HERE r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE test_preds = lr.predict(np.vstack((pred_lr, pred_lgb)).T) # YOUR CODE GOES HERE r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Train R-squared for stacking is %f&#39; % r2_train_stacking) print(&#39;Test R-squared for stacking is %f&#39; % r2_test_stacking) . Train R-squared for stacking is 0.632176 Test R-squared for stacking is 0.771297 . Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. Examine and compare train and test scores for the two methods. . And of course this particular case does not mean simple mix is always better than stacking. . We all done! Submit everything we need to the grader now. . from grader import Grader grader = Grader() grader.submit_tag(&#39;best_alpha&#39;, best_alpha) grader.submit_tag(&#39;r2_train_simple_mix&#39;, r2_train_simple_mix) grader.submit_tag(&#39;r2_test_simple_mix&#39;, r2_test_simple_mix) grader.submit_tag(&#39;r2_train_stacking&#39;, r2_train_stacking) grader.submit_tag(&#39;r2_test_stacking&#39;, r2_test_stacking) . Current answer for task best_alpha is: 0.765 Current answer for task r2_train_simple_mix is: 0.627255043446 Current answer for task r2_test_simple_mix is: 0.781144169579 Current answer for task r2_train_stacking is: 0.632175561459 Current answer for task r2_test_stacking is: 0.771297132342 . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot; TOKEN HERE&quot;# TOKEN HERE grader.status() . You want to submit these numbers: Task best_alpha: 0.765 Task r2_train_simple_mix: 0.627255043446 Task r2_test_simple_mix: 0.781144169579 Task r2_train_stacking: 0.632175561459 Task r2_test_stacking: 0.771297132342 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/01/Ensembling_Implementation.html",
            "relUrl": "/2022/04/01/Ensembling_Implementation.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Z-unlock Challenge: Data Visualization",
            "content": "We will Analyze the correlation of temperatures changes on energy use, land cover,waste use and deforestoration by questioning these questions. . What are the areas with biggest/smallest change in temperature? | Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) | How does the seasonal temperature change look like? | How does this vary by continent? Particularly South America? | . # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) . df_temperature = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv&quot;) df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . temp_max = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].max().sort_values(ascending=False).reset_index() temp_min = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].min().sort_values().reset_index() d2 = temp_max[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Max temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . d2 = temp_min[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Min temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . Biggest/smallest change in temperature: . Svalbard and Jan Mayeb Island is the most change in temperature based on the chart above | . Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) . Look at all the possibilities from another dataset/tables | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . land_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv&quot;) land_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2001 | 2001 | 1000 ha | 88.1603 | FC | Calculated data | . 1 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2002 | 2002 | 1000 ha | 88.1818 | FC | Calculated data | . 2 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2003 | 2003 | 1000 ha | 88.2247 | FC | Calculated data | . 3 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2004 | 2004 | 1000 ha | 88.2462 | FC | Calculated data | . 4 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2005 | 2005 | 1000 ha | 88.3106 | FC | Calculated data | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . waste_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv&quot;) waste_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1990 | 1990 | kilotonnes | 0.0 | Fc | Calculated data | . 1 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1991 | 1991 | kilotonnes | 0.0 | Fc | Calculated data | . 2 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1992 | 1992 | kilotonnes | 0.0 | Fc | Calculated data | . 3 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1993 | 1993 | kilotonnes | 0.0 | Fc | Calculated data | . 4 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1994 | 1994 | kilotonnes | 0.0 | Fc | Calculated data | . fires_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv&quot;) fires_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Source Code Source Unit Value Flag Flag Description Note . 0 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1990 | 1990 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 1 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1991 | 1991 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 2 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1992 | 1992 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 3 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1993 | 1993 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 4 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1994 | 1994 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . temp_change= df_temperature.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, hue=&#39;Months&#39;, legend=&#39;full&#39;, data=temp_change, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temp_change.Months.unique()))) max_value_per_year = temp_change.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.title(&quot;The trend for temperature change annually over Months&quot;) plt.axvspan(2015, 2020,alpha=0.15) plt.show() . land_cover= land_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=land_cover, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(land_cover.Year.unique()))) max_value_per_year = land_cover.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Land Cover&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2004, 2006,alpha=0.15) plt.show() . energy_use= energy_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=energy_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(energy_use.Year.unique()))) max_value_per_year = energy_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Energy Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1985, 1989,alpha=0.15) plt.show() . waste_use= waste_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=waste_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(waste_use.Year.unique()))) max_value_per_year = waste_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Waste Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1990, 1993,alpha=0.15) plt.show() . fires_use= fires_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=fires_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(fires_use.Year.unique()))) max_value_per_year = fires_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Fires Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1999, 2003,alpha=0.15) plt.show() . Correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions and Fires.) . Insight Based on Aggregating the mean per year shows correlation among temperature, energy use, land cover, waste use, and fires. All country-Value indicator(Value feature based on each tables) combinations show an increase, but there are subtle differences: . In Land cover use, in 2004-2005, there was a signifant increase followed by a slighly increase in from 2011-2017. | In Energy use, in 1985-1989, there was a signifant increase followed by a slighly increase in from 2019-2020. | In Waste use, in 1999-1993, there was a signifant drop followed by a significant increase from 1994-2020. | In Fires use, in 1990-2003, there was a signifant increase followed by a slighly decrease from 2003-2020. . | Almost everywhere, the end-of-year show an correlation that the the temperature that increase yearly affect the use of waste, energy,deforestoration, and land cover yearly. . | . How does the seasonal temperature change look like? . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . df_temperature.groupby(&quot;Months&quot;)[&quot;Value&quot;].agg([&quot;sum&quot;,&quot;mean&quot;,&quot;max&quot;]) . sum mean max . Months . Dec–Jan–Feb 6113.952 | 0.467428 | 8.206 | . Jun–Jul–Aug 6951.271 | 0.531890 | 4.764 | . Mar–Apr–May 6872.110 | 0.525511 | 5.533 | . Meteorological year 6413.093 | 0.491651 | 5.328 | . Sep–Oct–Nov 5761.315 | 0.441108 | 6.084 | . plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_temperature.groupby([&#39;Months&#39;])): ax = plt.subplot(6, 3, i+1, ymargin=0.5) ax.plot(df.Value) ax.set_title(combi) #if i == 6: break plt.tight_layout(h_pad=3.0) plt.suptitle(&#39;Seasonal Temperature Change&#39;, y=1.03) plt.show() . Seasonal Temperature Change . We can see that on each month has different maximum temperrature. DEC-Jan-Feb has the hottest temperature with 8.206 followed by Sept-Oct-Nov. | . How does this vary by continent? Particularly South America? . south_america_countries =[&#39;Brazil&#39;,&#39;Argentina&#39;,&#39;Chile&#39;,&#39;Colombia&#39;, &#39;Ecuador&#39;,&#39;Venezuela (Bolivarian Republic of)&#39;, &#39;Bolivia (Plurinational State of)&#39;,&#39;Guyana&#39;, &#39;Uruguay&#39;,&#39;Suriname&#39;, &#39;Paraguay&#39;,&#39;Aruba&#39;,&#39;Trinidad and Tobago&#39;] temperature_sa =df_temperature[df_temperature[&quot;Area&quot;].isin(south_america_countries)] temperature_sa.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 2700 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | 0.035 | Fc | Calculated data | . 2701 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | -0.144 | Fc | Calculated data | . 2702 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 0.552 | Fc | Calculated data | . 2703 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | 0.052 | Fc | Calculated data | . 2704 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.034 | Fc | Calculated data | . temperature_sa.groupby([&quot;Area&quot;])[&quot;Value&quot;].agg([&quot;max&quot;,&quot;min&quot;]).plot(kind=&quot;bar&quot;,figsize=(12,8)) plt.ylabel(&quot;Temperature&quot;) . Text(0, 0.5, &#39;Temperature&#39;) . How about Meterological season temperature changes in South America? . temperature_sa= temperature_sa.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, hue=&#39;Months&#39;, data=temperature_sa, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temperature_sa.Months.unique()))) max_value_per_year = temperature_sa.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature Change&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2013, 2016,alpha=0.15) plt.show() . Ultimately, there is an uptrend for temperature change in South America annually in which the peak is around 2013-2016. | . For joining this competition, see Z-Unlocked_Challenge1. There is a chance to visit Barcelona for Kaggle Competition. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/03/30/data-visualization-challenge.html",
            "relUrl": "/2022/03/30/data-visualization-challenge.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Play for a chance to win 1 of 10 HP ZBook Studios & a trip to the Kaggle Days x Z by HP World Championship in Barcelona",
            "content": "Challenge 4 - (Image Classification) . The Task . The challenge is to build a machine learning model to classify images of &quot;La Eterna&quot;. This can be done in a variety of ways. For this challenge i implemented CNN using pytorch to classify the images. The data is split into a training and a submission set. The images includes two labeled folders in the Test folder. The folder labeled &quot;la_eterna&quot; includes the pictures of la eterna that Eva captured. The other folder labeled &quot;other_flowers&quot; includes pictures of other flowers that are not la eterna. We will use this data to build our classifier. Each of the images has been formatted to the dimensions (224,224, 3) for the analysis. You can check the episode for each chalenge in this video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;iycrQpWIMnQ&#39;, width=800, height=500) . Import libraries . %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import matplotlib.pyplot as plt import numpy as np import torch import torchvision from torch import nn from torch import optim import torch.nn.functional as F from torchvision import datasets, transforms, models . data_dir = &quot;data_cleaned/Train&quot; def load_split_train_test(datadir, valid_size = .2): train_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) test_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_data = datasets.ImageFolder(datadir, transform=train_transforms) test_data = datasets.ImageFolder(datadir, transform=test_transforms) num_train = len(train_data) indices = list(range(num_train)) split = int(np.floor(valid_size * num_train)) np.random.shuffle(indices) from torch.utils.data.sampler import SubsetRandomSampler train_idx, test_idx = indices[split:], indices[:split] train_sampler = SubsetRandomSampler(train_idx) test_sampler = SubsetRandomSampler(test_idx) trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=64) testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=64) return trainloader, testloader trainloader, testloader = load_split_train_test(data_dir, .2) print(trainloader.dataset.classes) . [&#39;la_eterna&#39;, &#39;other_flowers&#39;] . def img_display(img): img = img # unnormalize npimg = img.numpy() npimg = np.transpose(npimg, (1, 2, 0)) return npimg . Check trainloader Images . dataiter = iter(trainloader) images, labels = dataiter.next() arthopod_types = {0: &#39;la_eterna&#39;, 1: &#39;other_flowers&#39;} # Viewing data examples used for training fig, axis = plt.subplots(3, 5, figsize=(15, 10)) for i, ax in enumerate(axis.flat): with torch.no_grad(): image, label = images[i], labels[i] ax.imshow(img_display(image)) # add image ax.set(title = f&quot;{arthopod_types[label.item()]}&quot;) # add label . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Check the images shape in each steps of Neural Network Process to make us easier to form the CNN model . import matplotlib.pyplot as plt import torchvision dataiter = iter(trainloader) images,labels = dataiter.next() img_display(torchvision.utils.make_grid(images)) conv1 = nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) pool = nn.MaxPool2d(2, 2) conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) print(images.shape) x = conv1(images) print(x.shape) x =pool(x) print(x.shape) x =conv2(x) print(x.shape) x =conv3(x) print(x.shape) . torch.Size([64, 3, 224, 224]) torch.Size([64, 12, 224, 224]) torch.Size([64, 12, 112, 112]) torch.Size([64, 20, 112, 112]) torch.Size([64, 32, 112, 112]) . Create Convolutional Neural Network Architecture . class ConvNet(nn.Module): def __init__(self,num_classes=2): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (64,3,224,224) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) #Shape= (64,12,224,224) self.bn1=nn.BatchNorm2d(num_features=12) #Shape= (64,12,224,224) self.relu1=nn.ReLU() #Shape= (64,12,224,224) self.pool=nn.MaxPool2d(kernel_size=2) #Shape= (64,12,224,224) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) #Shape= (64,20,112,112) self.relu2=nn.ReLU() #Shape= (64,20,112,112) self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) #Shape= (64,32,112,112) self.bn3=nn.BatchNorm2d(num_features=32) #Shape= (64,32,112,112) self.relu3=nn.ReLU() #Shape= (64,32,112,112) self.fc=nn.Linear(in_features=112 * 112* 32,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,112,112) output=output.view(-1,32*112*112) output=self.fc(output) return output . model = ConvNet() # On CPU print(model) . ConvNet( (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu2): ReLU() (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu3): ReLU() (fc): Linear(in_features=401408, out_features=2, bias=True) ) . Initialize Loss function . criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.0001) . def accuracy(out, labels): _,pred = torch.max(out, dim=1) return torch.sum(pred==labels).item() . Training Process through Convolutional Neural Network . n_epochs = 12 print_every = 10 valid_loss_min = np.Inf val_loss = [] val_acc = [] train_loss = [] train_acc = [] total_step = len(trainloader) for epoch in range(1, n_epochs+1): running_loss = 0.0 # scheduler.step(epoch) correct = 0 total=0 print(f&#39;Epoch {epoch} n&#39;) for batch_idx, (data_, target_) in enumerate(trainloader): #data_, target_ = data_.to(device), target_.to(device)# on GPU # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(data_) loss = criterion(outputs, target_) loss.backward() optimizer.step() # print statistics running_loss += loss.item() _,pred = torch.max(outputs, dim=1) correct += torch.sum(pred==target_).item() total += target_.size(0) if (batch_idx) % 20 == 0: print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; .format(epoch, n_epochs, batch_idx, total_step, loss.item())) train_acc.append(100 * correct / total) train_loss.append(running_loss/total_step) print(f&#39; ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}&#39;) batch_loss = 0 total_t=0 correct_t=0 with torch.no_grad(): model.eval() for data_t, target_t in (testloader): #data_t, target_t = data_t.to(device), target_t.to(device)# on GPU outputs_t = model(data_t) loss_t = criterion(outputs_t, target_t) batch_loss += loss_t.item() _,pred_t = torch.max(outputs_t, dim=1) correct_t += torch.sum(pred_t==target_t).item() total_t += target_t.size(0) val_acc.append(100 * correct_t / total_t) val_loss.append(batch_loss/len(testloader)) network_learned = batch_loss &lt; valid_loss_min print(f&#39;validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f} n&#39;) # Saving the best weight if network_learned: valid_loss_min = batch_loss torch.save(model.state_dict(), &#39;model_classification_tutorial.pt&#39;) print(&#39;Detected network improvement, saving current model&#39;) model.train() . Epoch 1 Epoch [1/12], Step [0/7], Loss: 0.7828 train loss: 4.3233, train acc: 55.4779 validation loss: 0.5667, validation acc: 69.1589 Detected network improvement, saving current model Epoch 2 Epoch [2/12], Step [0/7], Loss: 2.0096 train loss: 3.6552, train acc: 75.0583 validation loss: 0.7703, validation acc: 42.0561 Epoch 3 Epoch [3/12], Step [0/7], Loss: 1.2640 train loss: 2.7823, train acc: 80.1865 validation loss: 0.7495, validation acc: 76.6355 Epoch 4 Epoch [4/12], Step [0/7], Loss: 0.8584 train loss: 2.2371, train acc: 89.7436 validation loss: 0.8120, validation acc: 60.7477 Epoch 5 Epoch [5/12], Step [0/7], Loss: 1.0303 train loss: 1.8770, train acc: 90.9091 validation loss: 0.7693, validation acc: 86.9159 Epoch 6 Epoch [6/12], Step [0/7], Loss: 0.2469 train loss: 1.5874, train acc: 97.2028 validation loss: 0.7170, validation acc: 89.7196 Detected network improvement, saving current model Epoch 7 Epoch [7/12], Step [0/7], Loss: 0.0705 train loss: 1.3695, train acc: 97.6690 validation loss: 0.6692, validation acc: 91.5888 Detected network improvement, saving current model Epoch 8 Epoch [8/12], Step [0/7], Loss: 0.0019 train loss: 1.1994, train acc: 99.5338 validation loss: 0.6778, validation acc: 87.8505 Epoch 9 Epoch [9/12], Step [0/7], Loss: 0.0163 train loss: 1.0672, train acc: 99.7669 validation loss: 0.6578, validation acc: 89.7196 Epoch 10 Epoch [10/12], Step [0/7], Loss: 0.0003 train loss: 0.9605, train acc: 100.0000 validation loss: 0.6351, validation acc: 91.5888 Epoch 11 Epoch [11/12], Step [0/7], Loss: 0.0018 train loss: 0.8734, train acc: 100.0000 validation loss: 0.6198, validation acc: 91.5888 Epoch 12 Epoch [12/12], Step [0/7], Loss: 0.0006 train loss: 0.8007, train acc: 100.0000 validation loss: 0.6055, validation acc: 91.5888 . Insights &#128161; . You can see that at epoch 5 we get the best result with accuracy 97.6 on training images and 91.6 on the validation images and loss is 1.3 on training images and 0.6 on validation images. . Train - Validation Loss . fig = plt.figure(figsize=(20,10)) plt.title(&quot;Train - Validation Loss&quot;) plt.plot( train_loss, label=&#39;train&#39;) plt.plot( val_loss, label=&#39;validation&#39;) plt.xlabel(&#39;num_epochs&#39;, fontsize=12) plt.ylabel(&#39;loss&#39;, fontsize=12) plt.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x15ad05fe1c0&gt; . Train - Validation Accuracy . fig = plt.figure(figsize=(20,10)) plt.title(&quot;Train - Validation Accuracy&quot;) plt.plot(train_acc, label=&#39;train&#39;) plt.plot(val_acc, label=&#39;validation&#39;) plt.xlabel(&#39;num_epochs&#39;, fontsize=12) plt.ylabel(&#39;accuracy&#39;, fontsize=12) plt.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x15ad0406130&gt; . model.load_state_dict(torch.load(&#39;model_classification_tutorial.pt&#39;)) . &lt;All keys matched successfully&gt; . Evaluation using submission images . submission_path =&quot;data_cleaned/scraped_images&quot; submission_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) submission_data = datasets.ImageFolder(submission_path,transform=submission_transforms) submissionloader = torch.utils.data.DataLoader(submission_data,shuffle=True, batch_size=64) . dataiter = iter(submissionloader) images, labels = dataiter.next() flower_types = {0:&#39;la_eterna&#39;, 1:&#39;other_flowers&#39;} # Viewing data examples used for training fig, axis = plt.subplots(3, 5, figsize=(15, 10)) submission =pd.DataFrame(columns = [&quot;labels&quot;,&quot;preditions&quot;]) with torch.no_grad(): model.eval() for ax, image, label in zip(axis.flat,images, labels): ax.imshow(img_display(image)) # add image image_tensor = image.unsqueeze_(0) output_ = model(image_tensor) output_ = output_.argmax() k = output_.item()==label.item() ax.set_title(str(flower_types[label.item()])+&quot;:&quot; +str(k)) # add label . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Insights &#128161; . You can see that based on the images given, the model can clasify whether it is la eterna or other flower. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/pytorch/neural%20network/images%20classification/2022/03/25/zunlock-Image-classification.html",
            "relUrl": "/pytorch/neural%20network/images%20classification/2022/03/25/zunlock-Image-classification.html",
            "date": " • Mar 25, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Model Design using Pytorch",
            "content": "Source: 3Blue1Brown . Neural Network has been evolving recently. Many applications have been developed by using neural network. The development of GPU provided by big company like NVIDIA has fasten how the training process in Architecture of Neural network that needs many parameters in order to get better result for any kind of problems. You can see on the GIf above how neural network works throguh many layers that involve many parameters that can create good output that can identify the real value. The practical way is image identification where Neural network through combining many layers and parameters, activation function, and loss that can be improved to identify the image based on the GIF above. We will learn the implementation through Pytorch in this tutorial. . Study Case 1 . You work as an assistant of the mayor of Somerville and the HR department has asked you to build a model capable of predicting whether a person is happy with the current administration based on their satisfaction with the city&#39;s services . import pandas as pd import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim import warnings warnings.filterwarnings(&quot;ignore&quot;) . df = pd.read_csv(&quot;SomervilleHappinessSurvey2015.csv&quot;) df.head() . D X1 X2 X3 X4 X5 X6 . 0 0 | 3 | 3 | 3 | 4 | 2 | 4 | . 1 0 | 3 | 2 | 3 | 5 | 4 | 3 | . 2 1 | 5 | 3 | 3 | 3 | 3 | 5 | . 3 0 | 5 | 4 | 3 | 3 | 3 | 5 | . 4 0 | 5 | 4 | 3 | 3 | 3 | 5 | . Columns Information: . D = decision attribute (D) with values 0 (unhappy) and 1 (happy) | X1 = the availability of information about the city services | X2 = the cost of housing | X3 = the overall quality of public schools | X4 = your trust in the local police | X5 = the maintenance of streets and sidewalks | X6 = the availability of social community events . | Attributes X1 to X6 have values 1 to 5. . | . X = torch.tensor(df.drop(&quot;D&quot;,axis=1).astype(np.float32).values) y = torch.tensor(df[&quot;D&quot;].astype(np.float32).values) . X[:10] . tensor([[3., 3., 3., 4., 2., 4.], [3., 2., 3., 5., 4., 3.], [5., 3., 3., 3., 3., 5.], [5., 4., 3., 3., 3., 5.], [5., 4., 3., 3., 3., 5.], [5., 5., 3., 5., 5., 5.], [3., 1., 2., 2., 1., 3.], [5., 4., 4., 4., 4., 5.], [4., 1., 4., 4., 4., 4.], [4., 4., 4., 2., 5., 5.]]) . # 6 from how many features we have # output whether happy or unhappy model = nn.Sequential(nn.Linear(6,1), nn.Sigmoid()) print(model) . Sequential( (0): Linear(in_features=6, out_features=1, bias=True) (1): Sigmoid() ) . loss_func = nn.MSELoss() optimizer = optim.Adam(model.parameters(),lr=1e-2) . losses =[] for i in range(20): y_pred = model(X) loss = loss_func(y_pred,y) # item() is used for getting value from tensor losses.append(loss.item()) optimizer.zero_grad() #do back propagation loss.backward() #update weights during backward propagation optimizer.step() if i%5 ==0: print(i,loss.item()) . 0 0.5094006061553955 5 0.46509113907814026 10 0.3730385899543762 15 0.26985085010528564 . Plot the loss for each epochs . plt.plot(range(0,20),losses) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Loss&quot;) . Text(0, 0.5, &#39;Loss&#39;) . This is just a simple sequential neural network by implementing pytorch. We will deep dive into a process of building model in the study case 2 where we implement the process of Data Science lifecycle from cleaning the data,splitting the data, making the prediction and evaluating the prediction. . Study Case 2 . Deep Learning in Bank . Deep Learning has been implementing in many sectors including Bank.The problem thas has been happening for this sector is to predict whether bank should grant loan for the customers who will be making credit card. This is essential for Bank because it can measure how they can validate how much money that they can provide and estimate the profit from customers who will use the credit card based on a period of time. We will detect the customers who will be potential to grant loan that can affect to the income of the bank through this dataset. . We will follow a few steps before modelling our data into ANN using Pytorch including : . Understand the data including dealing with quality of data | rescale the features (giving different scales for each features may result that a given features is more important thatn others as it has higher numerical values) | split the data | . df_credit = pd.read_excel(&quot;default of credit card clients.xls&quot;,skiprows=1) df_credit.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 1 | 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 2 | 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 3 | 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 4 | 50000 | 2 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 5 | 50000 | 1 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 25 columns . print(f&quot;Rows : {df_credit.shape[0]}, Columns:{df_credit.shape[1]}&quot;) . Rows : 30000, Columns:25 . data_clean =df_credit.drop([&quot;ID&quot;,&quot;SEX&quot;],axis=1) data_clean.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 50000 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 50000 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 23 columns . (data_clean.isnull().sum()/data_clean.shape[0]).plot() . &lt;AxesSubplot:&gt; . data_clean.describe() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 3.000000e+04 | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 167484.322667 | 1.853133 | 1.551867 | 35.485500 | -0.016700 | -0.133767 | -0.166200 | -0.220667 | -0.266200 | -0.291100 | ... | 43262.948967 | 40311.400967 | 38871.760400 | 5663.580500 | 5.921163e+03 | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | . std 129747.661567 | 0.790349 | 0.521970 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | 1.133187 | 1.149988 | ... | 64332.856134 | 60797.155770 | 59554.107537 | 16563.280354 | 2.304087e+04 | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | . min 10000.000000 | 0.000000 | 0.000000 | 21.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | ... | -170000.000000 | -81334.000000 | -339603.000000 | 0.000000 | 0.000000e+00 | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 50000.000000 | 1.000000 | 1.000000 | 28.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 2326.750000 | 1763.000000 | 1256.000000 | 1000.000000 | 8.330000e+02 | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | . 50% 140000.000000 | 2.000000 | 2.000000 | 34.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 19052.000000 | 18104.500000 | 17071.000000 | 2100.000000 | 2.009000e+03 | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | . 75% 240000.000000 | 2.000000 | 2.000000 | 41.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 54506.000000 | 50190.500000 | 49198.250000 | 5006.000000 | 5.000000e+03 | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | . max 1000000.000000 | 6.000000 | 3.000000 | 79.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | ... | 891586.000000 | 927171.000000 | 961664.000000 | 873552.000000 | 1.684259e+06 | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | . 8 rows × 23 columns . outliers ={} for i in range(data_clean.shape[1]): min_t = data_clean[data_clean.columns[i]].mean()-(3*data_clean[data_clean.columns[i]].std()) max_t = data_clean[data_clean.columns[i]].mean()+(3*data_clean[data_clean.columns[i]].std()) count =0 for j in data_clean[data_clean.columns[i]]: if j &lt; min_t or j &gt; max_t: count +=1 percentage = count/data_clean.shape[0] outliers[data_clean.columns[i]] = round(percentage,3) . from pprint import pprint pprint(outliers) . {&#39;AGE&#39;: 0.005, &#39;BILL_AMT1&#39;: 0.023, &#39;BILL_AMT2&#39;: 0.022, &#39;BILL_AMT3&#39;: 0.022, &#39;BILL_AMT4&#39;: 0.023, &#39;BILL_AMT5&#39;: 0.022, &#39;BILL_AMT6&#39;: 0.022, &#39;EDUCATION&#39;: 0.011, &#39;LIMIT_BAL&#39;: 0.004, &#39;MARRIAGE&#39;: 0.0, &#39;PAY_0&#39;: 0.005, &#39;PAY_2&#39;: 0.005, &#39;PAY_3&#39;: 0.005, &#39;PAY_4&#39;: 0.006, &#39;PAY_5&#39;: 0.005, &#39;PAY_6&#39;: 0.004, &#39;PAY_AMT1&#39;: 0.013, &#39;PAY_AMT2&#39;: 0.01, &#39;PAY_AMT3&#39;: 0.012, &#39;PAY_AMT4&#39;: 0.013, &#39;PAY_AMT5&#39;: 0.014, &#39;PAY_AMT6&#39;: 0.015, &#39;default payment next month&#39;: 0.0} . data_clean[&quot;default payment next month&quot;].value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . target = data_clean[&quot;default payment next month&quot;] yes = target[target == 1].count() no = target[target == 0].count() . data_yes = data_clean[data_clean[&quot;default payment next month&quot;] == 1] data_no = data_clean[data_clean[&quot;default payment next month&quot;] == 0] over_sampling = data_yes.sample(no, replace=True, random_state = 0) data_resampled = pd.concat([data_no, over_sampling], axis=0) . data_resampled[&quot;default payment next month&quot;].value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . data_resampled = data_resampled.reset_index(drop=True) X = data_resampled.drop(&quot;default payment next month&quot;,axis=1) y =data_resampled[&quot;default payment next month&quot;] . X = (X-X.min())/(X.max()-X.min()) X.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 . 0 0.080808 | 0.333333 | 0.666667 | 0.224138 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.093789 | 0.173637 | 0.095470 | 0.272928 | 0.001738 | 0.000891 | 0.001116 | 0.001610 | 0.002345 | 0.009458 | . 1 0.040404 | 0.333333 | 0.333333 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.113407 | 0.186809 | 0.109363 | 0.283685 | 0.002290 | 0.001199 | 0.001339 | 0.001771 | 0.002506 | 0.001892 | . 2 0.040404 | 0.333333 | 0.333333 | 0.620690 | 0.1 | 0.2 | 0.1 | 0.2 | 0.2 | 0.2 | ... | 0.106020 | 0.179863 | 0.099633 | 0.275681 | 0.002290 | 0.021779 | 0.011160 | 0.014493 | 0.001615 | 0.001284 | . 3 0.040404 | 0.166667 | 0.666667 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.117974 | 0.178407 | 0.100102 | 0.276367 | 0.002862 | 0.001078 | 0.000733 | 0.001610 | 0.002345 | 0.001513 | . 4 0.494949 | 0.166667 | 0.666667 | 0.137931 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.330672 | 0.671310 | 0.559578 | 0.625196 | 0.062961 | 0.023749 | 0.042409 | 0.032591 | 0.032237 | 0.026047 | . 5 rows × 22 columns . final_data =pd.concat([X,y],axis=1) final_data.to_csv(&quot;data_prepared.csv&quot;,index=False) . Build Model . import torch.nn.functional as F from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.metrics import accuracy_score . data = pd.read_csv(&quot;data_prepared.csv&quot;) data.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 0.080808 | 0.333333 | 0.666667 | 0.224138 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.173637 | 0.095470 | 0.272928 | 0.001738 | 0.000891 | 0.001116 | 0.001610 | 0.002345 | 0.009458 | 0 | . 1 0.040404 | 0.333333 | 0.333333 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.186809 | 0.109363 | 0.283685 | 0.002290 | 0.001199 | 0.001339 | 0.001771 | 0.002506 | 0.001892 | 0 | . 2 0.040404 | 0.333333 | 0.333333 | 0.620690 | 0.1 | 0.2 | 0.1 | 0.2 | 0.2 | 0.2 | ... | 0.179863 | 0.099633 | 0.275681 | 0.002290 | 0.021779 | 0.011160 | 0.014493 | 0.001615 | 0.001284 | 0 | . 3 0.040404 | 0.166667 | 0.666667 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.178407 | 0.100102 | 0.276367 | 0.002862 | 0.001078 | 0.000733 | 0.001610 | 0.002345 | 0.001513 | 0 | . 4 0.494949 | 0.166667 | 0.666667 | 0.137931 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.671310 | 0.559578 | 0.625196 | 0.062961 | 0.023749 | 0.042409 | 0.032591 | 0.032237 | 0.026047 | 0 | . 5 rows × 23 columns . X = data.drop(&quot;default payment next month&quot;,axis=1) y =data[&quot;default payment next month&quot;] . X_new , X_test,y_new,y_test =train_test_split(X,y,test_size=0.2,random_state=3) dev_per = X_test.shape[0]/X_new.shape[0] X_train,X_dev,y_train,y_dev = train_test_split(X_new,y_new,test_size=dev_per,random_state=3) . print(&quot;Training sets:&quot;,X_train.shape, y_train.shape) print(&quot;Validation sets:&quot;,X_dev.shape, y_dev.shape) print(&quot;Testing sets:&quot;,X_test.shape, y_test.shape) . Training sets: (28036, 22) (28036,) Validation sets: (9346, 22) (9346,) Testing sets: (9346, 22) (9346,) . X_dev_torch = torch.tensor(X_dev.values).float() y_dev_torch = torch.tensor(y_dev.values) X_test_torch = torch.tensor(X_test.values).float() y_test_torch = torch.tensor(y_test.values) . class Classifier(nn.Module): def __init__(self, input_size): super().__init__() self.hidden_1 = nn.Linear(input_size, 10) self.hidden_2 = nn.Linear(10, 10) self.hidden_3 = nn.Linear(10, 10) self.output = nn.Linear(10, 2) def forward(self, x): z = F.relu(self.hidden_1(x)) z = F.relu(self.hidden_2(z)) z = F.relu(self.hidden_3(z)) out = F.log_softmax(self.output(z), dim=1) return out . model = Classifier(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) epochs = 50 batch_size = 128 # for faster training procces/mini batch gradient descent . train_losses,dev_losses,train_acc,dev_acc =[],[],[],[] for epoch in range(epochs): X_,y_ =shuffle(X_train,y_train) running_loss=0 running_acc=0 iterations =0 for i in range(0,len(X_),batch_size): iterations +=1 b = i +batch_size X_batch = torch.tensor(X_.iloc[i:b,:].values).float() y_batch = torch.tensor(y_.iloc[i:b].values) pred = model(X_batch) loss = criterion(pred,y_batch) optimizer.zero_grad() loss.backward() optimizer.step() running_loss +=loss.item() ps = torch.exp(pred) top_p,top_class = ps.topk(1,dim=1) running_acc +=accuracy_score(y_batch,top_class) dev_loss =0 acc =0 with torch.no_grad(): pred_dev = model(X_dev_torch) dev_loss =criterion(pred_dev,y_dev_torch) ps_dev = torch.exp(pred_dev) top_p,top_class_dev = ps_dev.topk(1,dim=1) acc +=accuracy_score(y_dev_torch,top_class_dev) train_losses.append(running_loss/iterations) dev_losses.append(dev_loss) train_acc.append(running_acc/iterations) dev_acc.append(acc) print(&quot;Epoch: {}/{}.. &quot;.format(epoch+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/iterations), &quot;Validation Loss: {:.3f}.. &quot;.format(dev_loss), &quot;Training Accuracy: {:.3f}.. &quot;.format(running_acc/iterations), &quot;Validation Accuracy: {:.3f}&quot;.format(acc)) . Epoch: 1/50.. Training Loss: 0.675.. Validation Loss: 0.617.. Training Accuracy: 0.602.. Validation Accuracy: 0.663 Epoch: 2/50.. Training Loss: 0.607.. Validation Loss: 0.600.. Training Accuracy: 0.676.. Validation Accuracy: 0.686 Epoch: 3/50.. Training Loss: 0.598.. Validation Loss: 0.599.. Training Accuracy: 0.686.. Validation Accuracy: 0.688 Epoch: 4/50.. Training Loss: 0.595.. Validation Loss: 0.592.. Training Accuracy: 0.691.. Validation Accuracy: 0.695 Epoch: 5/50.. Training Loss: 0.592.. Validation Loss: 0.590.. Training Accuracy: 0.692.. Validation Accuracy: 0.693 Epoch: 6/50.. Training Loss: 0.589.. Validation Loss: 0.586.. Training Accuracy: 0.692.. Validation Accuracy: 0.693 Epoch: 7/50.. Training Loss: 0.585.. Validation Loss: 0.584.. Training Accuracy: 0.695.. Validation Accuracy: 0.696 Epoch: 8/50.. Training Loss: 0.583.. Validation Loss: 0.579.. Training Accuracy: 0.696.. Validation Accuracy: 0.700 Epoch: 9/50.. Training Loss: 0.576.. Validation Loss: 0.575.. Training Accuracy: 0.701.. Validation Accuracy: 0.703 Epoch: 10/50.. Training Loss: 0.572.. Validation Loss: 0.572.. Training Accuracy: 0.704.. Validation Accuracy: 0.707 Epoch: 11/50.. Training Loss: 0.572.. Validation Loss: 0.571.. Training Accuracy: 0.705.. Validation Accuracy: 0.709 Epoch: 12/50.. Training Loss: 0.570.. Validation Loss: 0.569.. Training Accuracy: 0.704.. Validation Accuracy: 0.709 Epoch: 13/50.. Training Loss: 0.569.. Validation Loss: 0.568.. Training Accuracy: 0.708.. Validation Accuracy: 0.709 Epoch: 14/50.. Training Loss: 0.567.. Validation Loss: 0.568.. Training Accuracy: 0.708.. Validation Accuracy: 0.709 Epoch: 15/50.. Training Loss: 0.566.. Validation Loss: 0.566.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 16/50.. Training Loss: 0.566.. Validation Loss: 0.567.. Training Accuracy: 0.708.. Validation Accuracy: 0.710 Epoch: 17/50.. Training Loss: 0.564.. Validation Loss: 0.566.. Training Accuracy: 0.711.. Validation Accuracy: 0.708 Epoch: 18/50.. Training Loss: 0.566.. Validation Loss: 0.564.. Training Accuracy: 0.707.. Validation Accuracy: 0.710 Epoch: 19/50.. Training Loss: 0.564.. Validation Loss: 0.564.. Training Accuracy: 0.712.. Validation Accuracy: 0.708 Epoch: 20/50.. Training Loss: 0.565.. Validation Loss: 0.564.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 21/50.. Training Loss: 0.562.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.707 Epoch: 22/50.. Training Loss: 0.563.. Validation Loss: 0.562.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 23/50.. Training Loss: 0.561.. Validation Loss: 0.564.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 24/50.. Training Loss: 0.562.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.709 Epoch: 25/50.. Training Loss: 0.561.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 26/50.. Training Loss: 0.562.. Validation Loss: 0.562.. Training Accuracy: 0.708.. Validation Accuracy: 0.707 Epoch: 27/50.. Training Loss: 0.562.. Validation Loss: 0.563.. Training Accuracy: 0.710.. Validation Accuracy: 0.709 Epoch: 28/50.. Training Loss: 0.560.. Validation Loss: 0.565.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 29/50.. Training Loss: 0.561.. Validation Loss: 0.562.. Training Accuracy: 0.711.. Validation Accuracy: 0.709 Epoch: 30/50.. Training Loss: 0.560.. Validation Loss: 0.564.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 31/50.. Training Loss: 0.561.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.709 Epoch: 32/50.. Training Loss: 0.560.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.706 Epoch: 33/50.. Training Loss: 0.559.. Validation Loss: 0.561.. Training Accuracy: 0.711.. Validation Accuracy: 0.709 Epoch: 34/50.. Training Loss: 0.559.. Validation Loss: 0.561.. Training Accuracy: 0.714.. Validation Accuracy: 0.710 Epoch: 35/50.. Training Loss: 0.560.. Validation Loss: 0.561.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 36/50.. Training Loss: 0.562.. Validation Loss: 0.559.. Training Accuracy: 0.710.. Validation Accuracy: 0.710 Epoch: 37/50.. Training Loss: 0.560.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.708 Epoch: 38/50.. Training Loss: 0.557.. Validation Loss: 0.563.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 39/50.. Training Loss: 0.561.. Validation Loss: 0.559.. Training Accuracy: 0.712.. Validation Accuracy: 0.711 Epoch: 40/50.. Training Loss: 0.560.. Validation Loss: 0.560.. Training Accuracy: 0.710.. Validation Accuracy: 0.708 Epoch: 41/50.. Training Loss: 0.559.. Validation Loss: 0.559.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 42/50.. Training Loss: 0.557.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 43/50.. Training Loss: 0.556.. Validation Loss: 0.559.. Training Accuracy: 0.714.. Validation Accuracy: 0.709 Epoch: 44/50.. Training Loss: 0.557.. Validation Loss: 0.563.. Training Accuracy: 0.714.. Validation Accuracy: 0.701 Epoch: 45/50.. Training Loss: 0.557.. Validation Loss: 0.558.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 46/50.. Training Loss: 0.560.. Validation Loss: 0.559.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 47/50.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 48/50.. Training Loss: 0.556.. Validation Loss: 0.558.. Training Accuracy: 0.714.. Validation Accuracy: 0.714 Epoch: 49/50.. Training Loss: 0.556.. Validation Loss: 0.564.. Training Accuracy: 0.714.. Validation Accuracy: 0.699 Epoch: 50/50.. Training Loss: 0.559.. Validation Loss: 0.560.. Training Accuracy: 0.712.. Validation Accuracy: 0.711 . fig = plt.subplots(figsize=(15,5)) plt.plot(train_losses,label=&quot;Training loss&quot;) plt.plot(dev_losses,label=&quot;Validation loss&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27ee31061c0&gt; . fig = plt.subplots(figsize=(15,5)) plt.plot(train_acc,label=&quot;Training accuracy&quot;) plt.plot(dev_acc,label=&quot;Validation accuracy&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efa576d30&gt; . We can see there is a change in the loss and accuracy accuracy during each epoch. We can do tune the learning rate for getting better result. You can do the experimentation by comparing the LR as well. This is a few steps in modelling using pytorch. You can see that we can do modelling by using Sequential or custom models using torch.nn. . # by adding hidden layer or epochs for training process class Classifier_Layer(nn.Module): def __init__(self, input_size): super().__init__() self.hidden_1 = nn.Linear(input_size, 10) self.hidden_2 = nn.Linear(10, 10) self.hidden_3 = nn.Linear(10, 10) self.hidden_4 = nn.Linear(10, 10) self.output = nn.Linear(10, 2) def forward(self, x): z = F.relu(self.hidden_1(x)) z = F.relu(self.hidden_2(z)) z = F.relu(self.hidden_3(z)) z = F.relu(self.hidden_4(z)) out = F.log_softmax(self.output(z), dim=1) return out . model = Classifier_Layer(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.01) epochs = 100 batch_size = 150 # for faster training procces/mini batch gradient descent . train_losses,dev_losses,train_acc,dev_acc =[],[],[],[] for epoch in range(epochs): X_,y_ =shuffle(X_train,y_train) running_loss=0 running_acc=0 iterations =0 for i in range(0,len(X_),batch_size): iterations +=1 b = i +batch_size X_batch = torch.tensor(X_.iloc[i:b,:].values).float() y_batch = torch.tensor(y_.iloc[i:b].values) pred = model(X_batch) loss = criterion(pred,y_batch) optimizer.zero_grad() loss.backward() optimizer.step() running_loss +=loss.item() ps = torch.exp(pred) top_p,top_class = ps.topk(1,dim=1) running_acc +=accuracy_score(y_batch,top_class) dev_loss =0 acc =0 with torch.no_grad(): pred_dev = model(X_dev_torch) dev_loss =criterion(pred_dev,y_dev_torch) ps_dev = torch.exp(pred_dev) top_p,top_class_dev = ps_dev.topk(1,dim=1) acc +=accuracy_score(y_dev_torch,top_class_dev) train_losses.append(running_loss/iterations) dev_losses.append(dev_loss) train_acc.append(running_acc/iterations) dev_acc.append(acc) print(&quot;Epoch: {}/{}.. &quot;.format(epoch+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/iterations), &quot;Validation Loss: {:.3f}.. &quot;.format(dev_loss), &quot;Training Accuracy: {:.3f}.. &quot;.format(running_acc/iterations), &quot;Validation Accuracy: {:.3f}&quot;.format(acc)) . Epoch: 1/100.. Training Loss: 0.618.. Validation Loss: 0.595.. Training Accuracy: 0.662.. Validation Accuracy: 0.688 Epoch: 2/100.. Training Loss: 0.594.. Validation Loss: 0.587.. Training Accuracy: 0.686.. Validation Accuracy: 0.690 Epoch: 3/100.. Training Loss: 0.588.. Validation Loss: 0.580.. Training Accuracy: 0.687.. Validation Accuracy: 0.693 Epoch: 4/100.. Training Loss: 0.584.. Validation Loss: 0.583.. Training Accuracy: 0.692.. Validation Accuracy: 0.692 Epoch: 5/100.. Training Loss: 0.583.. Validation Loss: 0.577.. Training Accuracy: 0.695.. Validation Accuracy: 0.694 Epoch: 6/100.. Training Loss: 0.580.. Validation Loss: 0.573.. Training Accuracy: 0.696.. Validation Accuracy: 0.703 Epoch: 7/100.. Training Loss: 0.575.. Validation Loss: 0.569.. Training Accuracy: 0.703.. Validation Accuracy: 0.704 Epoch: 8/100.. Training Loss: 0.574.. Validation Loss: 0.574.. Training Accuracy: 0.704.. Validation Accuracy: 0.709 Epoch: 9/100.. Training Loss: 0.571.. Validation Loss: 0.582.. Training Accuracy: 0.705.. Validation Accuracy: 0.708 Epoch: 10/100.. Training Loss: 0.571.. Validation Loss: 0.564.. Training Accuracy: 0.706.. Validation Accuracy: 0.712 Epoch: 11/100.. Training Loss: 0.569.. Validation Loss: 0.565.. Training Accuracy: 0.707.. Validation Accuracy: 0.712 Epoch: 12/100.. Training Loss: 0.569.. Validation Loss: 0.568.. Training Accuracy: 0.707.. Validation Accuracy: 0.705 Epoch: 13/100.. Training Loss: 0.566.. Validation Loss: 0.569.. Training Accuracy: 0.710.. Validation Accuracy: 0.706 Epoch: 14/100.. Training Loss: 0.566.. Validation Loss: 0.563.. Training Accuracy: 0.709.. Validation Accuracy: 0.713 Epoch: 15/100.. Training Loss: 0.566.. Validation Loss: 0.561.. Training Accuracy: 0.709.. Validation Accuracy: 0.711 Epoch: 16/100.. Training Loss: 0.564.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.715 Epoch: 17/100.. Training Loss: 0.563.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.713 Epoch: 18/100.. Training Loss: 0.566.. Validation Loss: 0.572.. Training Accuracy: 0.708.. Validation Accuracy: 0.701 Epoch: 19/100.. Training Loss: 0.564.. Validation Loss: 0.565.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 20/100.. Training Loss: 0.564.. Validation Loss: 0.561.. Training Accuracy: 0.712.. Validation Accuracy: 0.712 Epoch: 21/100.. Training Loss: 0.562.. Validation Loss: 0.566.. Training Accuracy: 0.710.. Validation Accuracy: 0.706 Epoch: 22/100.. Training Loss: 0.563.. Validation Loss: 0.571.. Training Accuracy: 0.711.. Validation Accuracy: 0.705 Epoch: 23/100.. Training Loss: 0.563.. Validation Loss: 0.563.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 24/100.. Training Loss: 0.559.. Validation Loss: 0.562.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 25/100.. Training Loss: 0.561.. Validation Loss: 0.564.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 26/100.. Training Loss: 0.561.. Validation Loss: 0.565.. Training Accuracy: 0.710.. Validation Accuracy: 0.704 Epoch: 27/100.. Training Loss: 0.560.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.714 Epoch: 28/100.. Training Loss: 0.560.. Validation Loss: 0.566.. Training Accuracy: 0.715.. Validation Accuracy: 0.701 Epoch: 29/100.. Training Loss: 0.559.. Validation Loss: 0.568.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 30/100.. Training Loss: 0.561.. Validation Loss: 0.566.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 31/100.. Training Loss: 0.559.. Validation Loss: 0.560.. Training Accuracy: 0.712.. Validation Accuracy: 0.716 Epoch: 32/100.. Training Loss: 0.558.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.715 Epoch: 33/100.. Training Loss: 0.560.. Validation Loss: 0.559.. Training Accuracy: 0.711.. Validation Accuracy: 0.713 Epoch: 34/100.. Training Loss: 0.558.. Validation Loss: 0.565.. Training Accuracy: 0.714.. Validation Accuracy: 0.705 Epoch: 35/100.. Training Loss: 0.560.. Validation Loss: 0.560.. Training Accuracy: 0.711.. Validation Accuracy: 0.713 Epoch: 36/100.. Training Loss: 0.557.. Validation Loss: 0.556.. Training Accuracy: 0.714.. Validation Accuracy: 0.715 Epoch: 37/100.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 38/100.. Training Loss: 0.557.. Validation Loss: 0.565.. Training Accuracy: 0.712.. Validation Accuracy: 0.714 Epoch: 39/100.. Training Loss: 0.559.. Validation Loss: 0.559.. Training Accuracy: 0.711.. Validation Accuracy: 0.714 Epoch: 40/100.. Training Loss: 0.559.. Validation Loss: 0.566.. Training Accuracy: 0.714.. Validation Accuracy: 0.713 Epoch: 41/100.. Training Loss: 0.556.. Validation Loss: 0.559.. Training Accuracy: 0.714.. Validation Accuracy: 0.713 Epoch: 42/100.. Training Loss: 0.557.. Validation Loss: 0.575.. Training Accuracy: 0.714.. Validation Accuracy: 0.716 Epoch: 43/100.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.715 Epoch: 44/100.. Training Loss: 0.555.. Validation Loss: 0.556.. Training Accuracy: 0.716.. Validation Accuracy: 0.715 Epoch: 45/100.. Training Loss: 0.557.. Validation Loss: 0.560.. Training Accuracy: 0.714.. Validation Accuracy: 0.715 Epoch: 46/100.. Training Loss: 0.556.. Validation Loss: 0.560.. Training Accuracy: 0.715.. Validation Accuracy: 0.714 Epoch: 47/100.. Training Loss: 0.556.. Validation Loss: 0.561.. Training Accuracy: 0.715.. Validation Accuracy: 0.709 Epoch: 48/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.714 Epoch: 49/100.. Training Loss: 0.555.. Validation Loss: 0.561.. Training Accuracy: 0.712.. Validation Accuracy: 0.713 Epoch: 50/100.. Training Loss: 0.555.. Validation Loss: 0.564.. Training Accuracy: 0.715.. Validation Accuracy: 0.703 Epoch: 51/100.. Training Loss: 0.556.. Validation Loss: 0.566.. Training Accuracy: 0.713.. Validation Accuracy: 0.699 Epoch: 52/100.. Training Loss: 0.555.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 53/100.. Training Loss: 0.553.. Validation Loss: 0.554.. Training Accuracy: 0.713.. Validation Accuracy: 0.716 Epoch: 54/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 55/100.. Training Loss: 0.555.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 56/100.. Training Loss: 0.552.. Validation Loss: 0.558.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 57/100.. Training Loss: 0.555.. Validation Loss: 0.557.. Training Accuracy: 0.713.. Validation Accuracy: 0.712 Epoch: 58/100.. Training Loss: 0.554.. Validation Loss: 0.565.. Training Accuracy: 0.716.. Validation Accuracy: 0.699 Epoch: 59/100.. Training Loss: 0.554.. Validation Loss: 0.560.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 60/100.. Training Loss: 0.554.. Validation Loss: 0.557.. Training Accuracy: 0.713.. Validation Accuracy: 0.711 Epoch: 61/100.. Training Loss: 0.552.. Validation Loss: 0.554.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 62/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 63/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.716 Epoch: 64/100.. Training Loss: 0.554.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.717 Epoch: 65/100.. Training Loss: 0.555.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 66/100.. Training Loss: 0.552.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.716 Epoch: 67/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.717.. Validation Accuracy: 0.713 Epoch: 68/100.. Training Loss: 0.552.. Validation Loss: 0.563.. Training Accuracy: 0.717.. Validation Accuracy: 0.705 Epoch: 69/100.. Training Loss: 0.552.. Validation Loss: 0.561.. Training Accuracy: 0.715.. Validation Accuracy: 0.705 Epoch: 70/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.714.. Validation Accuracy: 0.716 Epoch: 71/100.. Training Loss: 0.554.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 72/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 73/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 74/100.. Training Loss: 0.552.. Validation Loss: 0.562.. Training Accuracy: 0.716.. Validation Accuracy: 0.696 Epoch: 75/100.. Training Loss: 0.553.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.715 Epoch: 76/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.717.. Validation Accuracy: 0.715 Epoch: 77/100.. Training Loss: 0.550.. Validation Loss: 0.556.. Training Accuracy: 0.718.. Validation Accuracy: 0.714 Epoch: 78/100.. Training Loss: 0.551.. Validation Loss: 0.565.. Training Accuracy: 0.717.. Validation Accuracy: 0.700 Epoch: 79/100.. Training Loss: 0.552.. Validation Loss: 0.558.. Training Accuracy: 0.718.. Validation Accuracy: 0.714 Epoch: 80/100.. Training Loss: 0.552.. Validation Loss: 0.555.. Training Accuracy: 0.717.. Validation Accuracy: 0.712 Epoch: 81/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 82/100.. Training Loss: 0.551.. Validation Loss: 0.557.. Training Accuracy: 0.715.. Validation Accuracy: 0.710 Epoch: 83/100.. Training Loss: 0.550.. Validation Loss: 0.553.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 84/100.. Training Loss: 0.550.. Validation Loss: 0.557.. Training Accuracy: 0.718.. Validation Accuracy: 0.711 Epoch: 85/100.. Training Loss: 0.551.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.715 Epoch: 86/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.716.. Validation Accuracy: 0.704 Epoch: 87/100.. Training Loss: 0.553.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.717 Epoch: 88/100.. Training Loss: 0.551.. Validation Loss: 0.554.. Training Accuracy: 0.716.. Validation Accuracy: 0.714 Epoch: 89/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 90/100.. Training Loss: 0.549.. Validation Loss: 0.554.. Training Accuracy: 0.717.. Validation Accuracy: 0.718 Epoch: 91/100.. Training Loss: 0.551.. Validation Loss: 0.556.. Training Accuracy: 0.716.. Validation Accuracy: 0.717 Epoch: 92/100.. Training Loss: 0.550.. Validation Loss: 0.554.. Training Accuracy: 0.717.. Validation Accuracy: 0.715 Epoch: 93/100.. Training Loss: 0.549.. Validation Loss: 0.559.. Training Accuracy: 0.720.. Validation Accuracy: 0.705 Epoch: 94/100.. Training Loss: 0.549.. Validation Loss: 0.558.. Training Accuracy: 0.717.. Validation Accuracy: 0.706 Epoch: 95/100.. Training Loss: 0.549.. Validation Loss: 0.559.. Training Accuracy: 0.717.. Validation Accuracy: 0.714 Epoch: 96/100.. Training Loss: 0.549.. Validation Loss: 0.553.. Training Accuracy: 0.719.. Validation Accuracy: 0.718 Epoch: 97/100.. Training Loss: 0.551.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.718 Epoch: 98/100.. Training Loss: 0.551.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.711 Epoch: 99/100.. Training Loss: 0.549.. Validation Loss: 0.557.. Training Accuracy: 0.719.. Validation Accuracy: 0.716 Epoch: 100/100.. Training Loss: 0.547.. Validation Loss: 0.563.. Training Accuracy: 0.719.. Validation Accuracy: 0.714 . fig = plt.subplots(figsize=(15,5)) plt.plot(train_losses,label=&quot;Training loss&quot;) plt.plot(dev_losses,label=&quot;Validation loss&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efa57d460&gt; . fig = plt.subplots(figsize=(15,5)) plt.plot(train_acc,label=&quot;Training accuracy&quot;) plt.plot(dev_acc,label=&quot;Validation accuracy&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efb09d940&gt; . You can experiment by changing the architecture of the model .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/pytorch/neural%20network/activation%20function/loss/2022/03/23/NN-for-new-user-of-Pytorch.html",
            "relUrl": "/pytorch/neural%20network/activation%20function/loss/2022/03/23/NN-for-new-user-of-Pytorch.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "just random datacamp competition using advanced data visualization to extract electricity price in Australia",
            "content": "Understanding the local electricity market . 📖 Background You work for an energy company in Australia. Your company builds solar panel arrays and then sells the energy they produce to industrial customers. The company wants to expand to the city of Melbourne in the state of Victoria. . Prices and demand for electricity change every day. Customers pay for the energy received using a formula based on the local energy market&#39;s daily price. . Your company&#39;s pricing committee wants your team to estimate energy prices for the next 12-18 months to use those prices as the basis for contract negotiations. . In addition, the VP of strategy is researching investing in storage capacity (i.e., batteries) as a new source of revenue. The plan is to store some of the energy produced by the solar panels when pricing conditions are unfavorable and sell it by the next day on the open market if the prices are higher. . &#128190; The data . You have access to over five years of energy price and demand data (source): . &quot;date&quot; - from January 1, 2015, to October 6, 2020. | &quot;demand&quot; - daily electricity demand in MWh. | &quot;price&quot; - recommended retail price in AUD/MWh. | &quot;demand_pos_price&quot; - total daily demand at a positive price in MWh. | &quot;price_positive&quot; - average positive price, weighted by the corresponding intraday demand in AUD/MWh. | &quot;demand_neg_price&quot; - total daily demand at a negative price in MWh. | &quot;price_negative&quot; - average negative price, weighted by the corresponding intraday demand in AUD/MWh. | &quot;frac_neg_price&quot; - the fraction of the day when the demand traded at a negative price. | &quot;min_temperature&quot; - minimum temperature during the day in Celsius. | &quot;max_temperature&quot; - maximum temperature during the day in Celsius. | &quot;solar_exposure&quot; - total daily sunlight energy in MJ/m^2. | &quot;rainfall&quot; - daily rainfall in mm. | &quot;school_day&quot; - &quot;Y&quot; if that day was a school day, &quot;N&quot; otherwise. | &quot;holiday&quot; - &quot;Y&quot; if the day was a state or national holiday, &quot;N&quot; otherwise. | . Note: The price was negative during some intraday intervals, so energy producers were paying buyers rather than vice-versa. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import xgboost as xgb from xgboost import plot_importance, plot_tree from sklearn.metrics import mean_squared_error, mean_absolute_error from sklearn.model_selection import RandomizedSearchCV # Set defaults of the notebook sns.set(font=&quot;&#39;Source Code Pro&#39;, monospace&quot;) plt.rcParams[&quot;font.family&quot;] = &quot;&#39;Source Code Pro&#39;, monospace&quot; import warnings warnings.filterwarnings(&quot;ignore&quot;) # Color Palettes treasure_colors = [&quot;#703728&quot;, &quot;#c86b25&quot;, &quot;#dc9555&quot;, &quot;#fed56f&quot;, &quot;#c89a37&quot;] pirate_colors = [&quot;#010307&quot;, &quot;#395461&quot;, &quot;#449FAF&quot;, &quot;#B1F4FC&quot;, &quot;#F4D499&quot;, &quot;#835211&quot;] . df_electricity = pd.read_csv(&#39;./data/energy_demand.csv&#39;, parse_dates=[&#39;date&#39;],index_col =[&quot;date&quot;]) df_electricity.head() . demand price demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day holiday . date . 2015-01-01 99635.030 | 25.633696 | 97319.240 | 26.415953 | 2315.790 | -7.240000 | 0.020833 | 13.3 | 26.9 | 23.6 | 0.0 | N | Y | . 2015-01-02 129606.010 | 33.138988 | 121082.015 | 38.837661 | 8523.995 | -47.809777 | 0.062500 | 15.4 | 38.8 | 26.8 | 0.0 | N | N | . 2015-01-03 142300.540 | 34.564855 | 142300.540 | 34.564855 | 0.000 | 0.000000 | 0.000000 | 20.0 | 38.2 | 26.5 | 0.0 | N | N | . 2015-01-04 104330.715 | 25.005560 | 104330.715 | 25.005560 | 0.000 | 0.000000 | 0.000000 | 16.3 | 21.4 | 25.2 | 4.2 | N | N | . 2015-01-05 118132.200 | 26.724176 | 118132.200 | 26.724176 | 0.000 | 0.000000 | 0.000000 | 15.0 | 22.0 | 30.7 | 0.0 | N | N | . &#128170; Competition challenge . Create a report that covers the following: . How do energy prices change throughout the year? Are there any patterns by season or month of the year? | Build a forecast of daily energy prices the company can use as the basis of its financial planning. | Provide guidance on how much revenue the energy storage venture could generate per year using retail prices and a 70MWh storage system. | &#8987;&#65039; Exploratory Data Analysis! . color_pal = [&quot;#F8766D&quot;, &quot;#D39200&quot;, &quot;#93AA00&quot;, &quot;#00BA38&quot;, &quot;#00C19F&quot;, &quot;#00B9E3&quot;, &quot;#619CFF&quot;, &quot;#DB72FB&quot;] _ = df_electricity[&quot;price&quot;].plot(style=&#39;.&#39;, figsize=(15,5), color=color_pal[0], title=&#39;Daily electricity price in Victoria from January 2015 to October 2020&#39;) plt.ylim(0,300) plt.ylabel(&quot;Price&quot;) . Text(0, 0.5, &#39;Price&#39;) . Insights &#128205; : The distribution of daily electricity price . Based on the plot above show: . The distribution of daily electricity price in Victoria from January 2015 to October 2020 . | There is a significant electricity price change especially in 2016 to 2020. | (df_electricity.describe() .style .highlight_max(axis=0,color=&quot;#c07fef&quot;) .highlight_min(axis=0,color=&quot;#00FF00&quot;) .set_caption(&quot;Statistics of Electricity in Australia for 2015-2020&quot;)) . Statistics of Electricity in Australia for 2015-2020 demand price demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall . count 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2105.000000 | 2103.000000 | . mean 120035.476503 | 76.079554 | 119252.305055 | 76.553847 | 783.171448 | -2.686052 | 0.008547 | 11.582289 | 20.413200 | 14.743373 | 1.505944 | . std 13747.993761 | 130.246805 | 14818.631319 | 130.114184 | 3578.920686 | 19.485432 | 0.039963 | 4.313711 | 6.288693 | 7.945527 | 4.307897 | . min 85094.375000 | -6.076028 | 41988.240000 | 13.568986 | 0.000000 | -342.220000 | 0.000000 | 0.600000 | 9.000000 | 0.700000 | 0.000000 | . 25% 109963.650000 | 38.707040 | 109246.250000 | 39.117361 | 0.000000 | 0.000000 | 0.000000 | 8.500000 | 15.525000 | 8.200000 | 0.000000 | . 50% 119585.912500 | 66.596738 | 119148.082500 | 66.869058 | 0.000000 | 0.000000 | 0.000000 | 11.300000 | 19.100000 | 12.700000 | 0.000000 | . 75% 130436.006250 | 95.075012 | 130119.477500 | 95.130181 | 0.000000 | 0.000000 | 0.000000 | 14.600000 | 23.900000 | 20.700000 | 0.800000 | . max 170653.840000 | 4549.645105 | 170653.840000 | 4549.645105 | 57597.595000 | 0.000000 | 0.625000 | 28.000000 | 43.500000 | 33.300000 | 54.600000 | . fig, ax = plt.subplots(4, 1, figsize = (15, 20)) ax[0].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) ax[1].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) ax[2].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) ax[3].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;demand_pos_price&#39;, lw = 1, ax = ax[0]) ax[0].set_title(&quot;Daily electricity demand in Victoria from January 2015 to October 2020&quot;) ax[0].set_ylabel(&quot;Demand Positive Price [MWh]&quot;) ax[0].get_legend().remove() df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;demand_neg_price&#39;, lw = 1, color=&#39;red&#39;, ax = ax[1]) ax[1].set_ylabel(&quot;Demand Negative Price [MWh]&quot;) ax[1].get_legend().remove() df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;price_positive&#39;, lw = 1, color=&#39;red&#39;, ax = ax[2]) ax[2].set_ylabel(&quot;Price Positive [AUD$/MWh]&quot;) ax[2].get_legend().remove() df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;price_negative&#39;, lw = 1, color=&#39;red&#39;, ax = ax[3]) ax[3].set_ylabel(&quot;Price Negative [AUD$/MWh]&quot;) ax[3].get_legend().remove() . Insights &#128205; : Daily electricity demand in Victoria from January 2015 to October 2020 . Based on the plot above show: . Demand positive price shows some seasonality based on plot shown above&lt;/span&gt;. Maybe this feature may benefit our model. | Demand negative price We can see there is a significant difference electricity price through maximum price which is very far from the mean and standard deviation. | Positive price mostly with small values except for some values that is greater than 1000. | Price negative ocurred in each year with variation of values. | Make a correlation plot . def heatmap(x, y, size): fig, ax = plt.subplots() # Mapping from column names to integer coordinates x_labels = [v for v in sorted(x.unique())] y_labels = [v for v in sorted(y.unique())] x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} size_scale = 500 ax.scatter( x=x.map(x_to_num), # Use mapping for x y=y.map(y_to_num), # Use mapping for y s=size * size_scale, # Vector of square sizes, proportional to size parameter marker=&#39;s&#39; # Use square as scatterplot marker ) # Show column labels on the axes ax.set_xticks([x_to_num[v] for v in x_labels]) ax.set_xticklabels(x_labels, rotation=45, horizontalalignment=&#39;right&#39;) ax.set_yticks([y_to_num[v] for v in y_labels]) ax.set_yticklabels(y_labels) data = df_electricity columns = df_electricity.columns corr = data[columns].corr() corr = pd.melt(corr.reset_index(), id_vars=&#39;index&#39;) # Unpivot the dataframe, so we can get pair of arrays for x and y corr.columns = [&#39;x&#39;, &#39;y&#39;, &#39;value&#39;] heatmap( x=corr[&#39;x&#39;], y=corr[&#39;y&#39;], size=corr[&#39;value&#39;].abs() ) . Insights &#128205; : Correlation Matrix . Based on the plot above show: . Price target has higher correlation with price_positive, demand, demand_pos_price and frac_negative_price. | plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_electricity.groupby([df_electricity.index.year])): ax = plt.subplot(2, 3, i+1, ymargin=0.5) #print(df.num_sold.values.shape, df.num_sold.values) ax.plot(df.price) ax.set_title(combi) plt.xticks(rotation=45) plt.tight_layout(h_pad=3.0) plt.suptitle(&#39;Energy prices change for 2015-2020&#39;, y=1.03) plt.show() . Insights &#128205; : Electricity price change . Based on the plot above show: . There is a significant increase of electricity price in December 2015 after a stable price during January-September.&gt; 2. The electricity price tends to be stable in range 50-150 AUD except in February where there is an increase of energy price.&gt; 2. For 2016, significant increase of price occured in January and March followed by stable price during April-December&gt; 3. For 2018-2019, There is a significant increase of electricity price around January-February. | plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_electricity.groupby([df_electricity.index.year,&quot;holiday&quot;])): ax = plt.subplot(3,4, i+1, ymargin=0.5) #print(df.num_sold.values.shape, df.num_sold.values) ax.plot(df.price) ax.set_title(combi) plt.xticks(rotation=45) plt.suptitle(&#39;Energy prices change for 2015-2020 on holiday Season&#39;, y=1.03) . Text(0.5, 1.03, &#39;Energy prices change for 2015-2020 on holiday Season&#39;) . Insights &#128205; : Electricity prices change for 2015-2020 on holiday Season . Based on the plot above show: . There is an unstable price pattern of energy price in holiday season.&gt; 2. For annual Non-Holiday Season, There is an increase of energy price in the beginning of the year | plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_electricity.groupby([df_electricity.index.year,&quot;school_day&quot;])): ax = plt.subplot(3,4, i+1, ymargin=0.5) #print(df.num_sold.values.shape, df.num_sold.values) ax.plot(df.price) ax.set_title(combi) plt.xticks(rotation=40) plt.suptitle(&#39;Energy prices change for 2015-2020 on School Season&#39;, y=1.03) . Text(0.5, 1.03, &#39;Energy prices change for 2015-2020 on School Season&#39;) . Insights &#128205; : Electricity price change for 2015-2020 on School Season . Based on the plot above show: . There is a significant increase of electricity price in the end of 2015 on school season&gt; 2. There is a similar pattern of the increase of electricity price on school season in February-March in 2018, 2019, 2020&gt; 2. For annual non-holiday Season, There is an increase of electricity price in the beginning of the year in 2016, 2018, 2019 | Splitting the data based on time . split_date = &#39;01-Jan-2019&#39; df_train = df_electricity.loc[df_electricity.index &lt;= split_date].copy() df_test = df_electricity.loc[df_electricity.index &gt; split_date].copy() . def create_features(df, label=None): &quot;&quot;&quot; Creates time series features from datetime index &quot;&quot;&quot; df[&#39;date&#39;] = df.index df[&#39;hour&#39;] = df[&#39;date&#39;].dt.hour df[&#39;dayofweek&#39;] = df[&#39;date&#39;].dt.dayofweek df[&#39;quarter&#39;] = df[&#39;date&#39;].dt.quarter df[&#39;month&#39;] = df[&#39;date&#39;].dt.month df[&#39;year&#39;] = df[&#39;date&#39;].dt.year df[&#39;dayofyear&#39;] = df[&#39;date&#39;].dt.dayofyear df[&#39;dayofmonth&#39;] = df[&#39;date&#39;].dt.day df[&#39;weekofyear&#39;] = df[&#39;date&#39;].dt.isocalendar().week.astype(np.int64) df[&quot;school_day&quot;] = pd.get_dummies(df[&quot;school_day&quot;]) df[&quot;school_day&quot;] = pd.get_dummies(df[&quot;school_day&quot;]) df[&#39;price_7_days_lag&#39;] = df[&#39;price&#39;].shift(7) df[&#39;price_15_days_lag&#39;] = df[&#39;price&#39;].shift(15) df[&#39;price_30_days_lag&#39;] = df[&#39;price&#39;].shift(30) df[&#39;price_7_days_mean&#39;] = df[&#39;price&#39;].rolling(window = 7).mean() df[&#39;price_15_days_mean&#39;] = df[&#39;price&#39;].rolling(window = 15).mean() df[&#39;price_30_days_mean&#39;] = df[&#39;price&#39;].rolling(window = 30).mean() df[&#39;price_7_days_std&#39;] = df[&#39;price&#39;].rolling(window = 7).std() df[&#39;price_15_days_std&#39;] = df[&#39;price&#39;].rolling(window = 15).std() df[&#39;price_30_days_std&#39;] = df[&#39;price&#39;].rolling(window = 30).std() df[&#39;price_7_days_max&#39;] = df[&#39;price&#39;].rolling(window = 7).max() df[&#39;price_15_days_max&#39;] = df[&#39;price&#39;].rolling(window = 15).max() df[&#39;price_30_days_max&#39;] = df[&#39;price&#39;].rolling(window = 30).max() df[&#39;price_7_days_min&#39;] = df[&#39;price&#39;].rolling(window = 7).min() df[&#39;price_15_days_min&#39;] = df[&#39;price&#39;].rolling(window = 15).min() df[&#39;price_30_days_min&#39;] = df[&#39;price&#39;].rolling(window = 30).min() cols = [&#39;hour&#39;,&#39;dayofweek&#39;,&#39;dayofyear&#39;,&#39;quarter&#39;,&#39;month&#39;,&#39;year&#39;,&#39;dayofmonth&#39;,&#39;weekofyear&#39;,&#39;price_positive&#39;,&#39;demand_pos_price&#39;,&#39;demand&#39;,&#39;demand_neg_price&#39;,&#39;price_negative&#39;,&#39;frac_neg_price&#39;] #cols = [&#39;hour&#39;,&#39;dayofweek&#39;,&#39;quarter&#39;,&#39;month&#39;,&#39;year&#39;,&#39;dayofyear&#39;,&#39;dayofmonth&#39;,&#39;weekofyear&#39;,&#39;demand_pos_price&#39;,] for d in (&#39;7&#39;, &#39;15&#39;, &#39;30&#39;): for c in (&#39;lag&#39;, &#39;mean&#39;, &#39;std&#39;, &#39;max&#39;, &#39;min&#39;): cols.append(f&#39;price_{d}_days_{c}&#39;) X = df[cols] if label: y = df[label] return X, y return X . X_train, y_train = create_features(df_train, label=&#39;price&#39;) X_test, y_test = create_features(df_test, label=&#39;price&#39;) . reg = xgb.XGBRegressor(n_estimators=100,eta=0.1) fitted_xgb_model = reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=50, verbose=True) . [0] validation_0-rmse:78.10894 validation_1-rmse:232.43314 [1] validation_0-rmse:71.00056 validation_1-rmse:226.20828 [2] validation_0-rmse:64.59680 validation_1-rmse:220.67780 [3] validation_0-rmse:58.83662 validation_1-rmse:215.75482 [4] validation_0-rmse:53.66943 validation_1-rmse:211.30708 [5] validation_0-rmse:49.02540 validation_1-rmse:207.36438 [6] validation_0-rmse:44.84807 validation_1-rmse:203.70424 [7] validation_0-rmse:41.09291 validation_1-rmse:200.41937 [8] validation_0-rmse:37.70780 validation_1-rmse:197.01787 [9] validation_0-rmse:34.63074 validation_1-rmse:193.86409 [10] validation_0-rmse:31.85113 validation_1-rmse:190.97063 [11] validation_0-rmse:29.33829 validation_1-rmse:188.28787 [12] validation_0-rmse:27.05840 validation_1-rmse:185.79985 [13] validation_0-rmse:24.98889 validation_1-rmse:183.49159 [14] validation_0-rmse:23.10552 validation_1-rmse:181.34113 [15] validation_0-rmse:21.39121 validation_1-rmse:179.33682 [16] validation_0-rmse:19.83474 validation_1-rmse:178.55098 [17] validation_0-rmse:18.41598 validation_1-rmse:177.83699 [18] validation_0-rmse:17.11959 validation_1-rmse:177.18620 [19] validation_0-rmse:15.93410 validation_1-rmse:176.59129 [20] validation_0-rmse:14.84714 validation_1-rmse:176.04675 [21] validation_0-rmse:13.85011 validation_1-rmse:175.54684 [22] validation_0-rmse:12.93431 validation_1-rmse:175.08743 [23] validation_0-rmse:12.09328 validation_1-rmse:174.66530 [24] validation_0-rmse:11.32025 validation_1-rmse:173.82794 [25] validation_0-rmse:10.60783 validation_1-rmse:173.03954 [26] validation_0-rmse:9.95008 validation_1-rmse:172.29640 [27] validation_0-rmse:9.34216 validation_1-rmse:171.59558 [28] validation_0-rmse:8.77918 validation_1-rmse:170.93398 [29] validation_0-rmse:8.25816 validation_1-rmse:170.30927 [30] validation_0-rmse:7.77358 validation_1-rmse:169.71977 [31] validation_0-rmse:7.32364 validation_1-rmse:169.16220 [32] validation_0-rmse:6.90481 validation_1-rmse:168.63498 [33] validation_0-rmse:6.51409 validation_1-rmse:168.13646 [34] validation_0-rmse:6.14908 validation_1-rmse:167.66449 [35] validation_0-rmse:5.80786 validation_1-rmse:167.21773 [36] validation_0-rmse:5.48851 validation_1-rmse:166.79488 [37] validation_0-rmse:5.18929 validation_1-rmse:166.39388 [38] validation_0-rmse:4.90825 validation_1-rmse:166.01436 [39] validation_0-rmse:4.64466 validation_1-rmse:165.65491 [40] validation_0-rmse:4.39683 validation_1-rmse:165.31377 [41] validation_0-rmse:4.16372 validation_1-rmse:164.99037 [42] validation_0-rmse:3.94428 validation_1-rmse:164.68390 [43] validation_0-rmse:3.73747 validation_1-rmse:164.39325 [44] validation_0-rmse:3.54249 validation_1-rmse:164.11774 [45] validation_0-rmse:3.35867 validation_1-rmse:163.85646 [46] validation_0-rmse:3.18519 validation_1-rmse:163.60869 [47] validation_0-rmse:3.02139 validation_1-rmse:163.37366 [48] validation_0-rmse:2.86655 validation_1-rmse:163.15067 [49] validation_0-rmse:2.72040 validation_1-rmse:162.93970 [50] validation_0-rmse:2.58236 validation_1-rmse:162.73958 [51] validation_0-rmse:2.45181 validation_1-rmse:162.54944 [52] validation_0-rmse:2.32823 validation_1-rmse:162.36912 [53] validation_0-rmse:2.21122 validation_1-rmse:162.19797 [54] validation_0-rmse:2.10046 validation_1-rmse:162.03554 [55] validation_0-rmse:1.99561 validation_1-rmse:161.88142 [56] validation_0-rmse:1.89622 validation_1-rmse:161.73517 [57] validation_0-rmse:1.80210 validation_1-rmse:161.59613 [58] validation_0-rmse:1.71306 validation_1-rmse:161.46413 [59] validation_0-rmse:1.62860 validation_1-rmse:161.33887 [60] validation_0-rmse:1.54868 validation_1-rmse:161.21994 [61] validation_0-rmse:1.47296 validation_1-rmse:161.10706 [62] validation_0-rmse:1.40104 validation_1-rmse:160.99974 [63] validation_0-rmse:1.33303 validation_1-rmse:160.89801 [64] validation_0-rmse:1.26847 validation_1-rmse:160.80133 [65] validation_0-rmse:1.20725 validation_1-rmse:160.70961 [66] validation_0-rmse:1.14925 validation_1-rmse:160.62248 [67] validation_0-rmse:1.09428 validation_1-rmse:160.53976 [68] validation_0-rmse:1.04232 validation_1-rmse:160.46124 [69] validation_0-rmse:0.99303 validation_1-rmse:160.38670 [70] validation_0-rmse:0.94642 validation_1-rmse:160.31592 [71] validation_0-rmse:0.90220 validation_1-rmse:160.24872 [72] validation_0-rmse:0.86030 validation_1-rmse:160.18489 [73] validation_0-rmse:0.82075 validation_1-rmse:160.12427 [74] validation_0-rmse:0.78316 validation_1-rmse:160.06668 [75] validation_0-rmse:0.74781 validation_1-rmse:160.01205 [76] validation_0-rmse:0.71458 validation_1-rmse:159.96014 [77] validation_0-rmse:0.68281 validation_1-rmse:159.91087 [78] validation_0-rmse:0.65270 validation_1-rmse:159.86403 [79] validation_0-rmse:0.62447 validation_1-rmse:159.81957 [80] validation_0-rmse:0.59772 validation_1-rmse:159.77733 [81] validation_0-rmse:0.57273 validation_1-rmse:159.73723 [82] validation_0-rmse:0.54923 validation_1-rmse:159.69914 [83] validation_0-rmse:0.52638 validation_1-rmse:159.66295 [84] validation_0-rmse:0.50531 validation_1-rmse:159.62856 [85] validation_0-rmse:0.48540 validation_1-rmse:159.59593 [86] validation_0-rmse:0.46696 validation_1-rmse:159.56493 [87] validation_0-rmse:0.44928 validation_1-rmse:159.53546 [88] validation_0-rmse:0.43296 validation_1-rmse:159.50746 [89] validation_0-rmse:0.41758 validation_1-rmse:159.48086 [90] validation_0-rmse:0.40334 validation_1-rmse:159.45570 [91] validation_0-rmse:0.38988 validation_1-rmse:159.43173 [92] validation_0-rmse:0.37735 validation_1-rmse:159.40901 [93] validation_0-rmse:0.36581 validation_1-rmse:159.38736 [94] validation_0-rmse:0.35467 validation_1-rmse:159.36688 [95] validation_0-rmse:0.34369 validation_1-rmse:159.34740 [96] validation_0-rmse:0.33422 validation_1-rmse:159.32890 [97] validation_0-rmse:0.32500 validation_1-rmse:159.31133 [98] validation_0-rmse:0.31654 validation_1-rmse:159.29463 [99] validation_0-rmse:0.30846 validation_1-rmse:159.27872 . plt.plot(fitted_xgb_model.evals_result()[&#39;validation_0&#39;] [&#39;rmse&#39;]) plt.plot(fitted_xgb_model.evals_result()[&#39;validation_1&#39;] [&#39;rmse&#39;]) plt.ylabel(&#39;RMSE&#39;, fontsize=14) plt.xlabel(&#39;Price&#39;, fontsize=14) plt.legend([&#39;Train&#39;, &#39;Val&#39;], loc=&#39;upper right&#39;) . &lt;matplotlib.legend.Legend at 0x7f0267b412e0&gt; . We can see the comparison between training rmse and validation rmse score where the is an underfitting problem in our case where maybe due to outlier electricity price in 2019-2020 . feature_important = reg.get_booster().get_score(importance_type=&#39;weight&#39;) keys = list(feature_important.keys()) values = list(feature_important.values()) data = pd.DataFrame(data=values, index=keys, columns=[&quot;score&quot;]).sort_values(by = &quot;score&quot;, ascending=False) data.nlargest(10, columns=&quot;score&quot;).plot(kind=&#39;barh&#39;, figsize = (20,10)) ## plot top 10 features . &lt;AxesSubplot:&gt; . Based on the feature importances extracted from the model, price_positive feature, demand_neg_price, dayofweek,dayofyear and dayofyear show the most 5th of feature importances. . df_test[&#39;price_prediction&#39;] = reg.predict(X_test) price_all = pd.concat([df_test, df_train], sort=False) . _ = price_all[[&#39;price&#39;,&#39;price_prediction&#39;]].plot(figsize=(15, 5)) plt.ylim(0,1000) . (0.0, 1000.0) . f, ax = plt.subplots(1) f.set_figheight(5) f.set_figwidth(15) _ = price_all[[&#39;price&#39;,&#39;price_prediction&#39;]].plot(ax=ax,style=[&#39;-&#39;,&#39;.&#39;]) ax.set_xbound(lower=&#39;01-05-2019&#39;, upper=&#39;02-10-2020&#39;) ax.set_ylim(0, 2000) plot = plt.suptitle(&#39;May 2019 Forecast vs Actuals&#39;) . Insights &#128205; : Prediction error price between real price and prediction price . Our model is generalizable for the next 12-18 months based on the plot above. In Machine Learninig, It is a mandatory to make our model to be generalizable as possible so that it can predict the real/actual value in real situation. There is a few errors around february and in the beginining of January 2020. If it can predict the same value for actual demand, it is a overfitting problem. We have to take into account this case to make better prediction that can mimic the actual value with fewer errors. . mean_squared_error(y_true=df_test[&#39;price&#39;],y_pred=df_test[&#39;price_prediction&#39;]) . 25369.714397883905 . mean_absolute_error(y_true=df_test[&#39;price&#39;],y_pred=df_test[&#39;price_prediction&#39;]) . 10.73348890658493 . df_test[&#39;error&#39;] = df_test[&#39;price&#39;] - df_test[&#39;price_prediction&#39;] df_test[&#39;abs_error&#39;] = df_test[&#39;error&#39;].apply(np.abs) error_by_day = df_test.groupby([&#39;year&#39;,&#39;month&#39;,&#39;dayofmonth&#39;]).mean()[[&#39;price&#39;,&#39;price_prediction&#39;,&#39;error&#39;,&#39;abs_error&#39;]] . (error_by_day.sort_values(&#39;error&#39;, ascending=True).head(10) .style .bar(subset=&quot;error&quot;,color=&quot;#c07fef&quot;,vmax=error_by_day.error.quantile(0.95)) .highlight_max(axis=0,color=&quot;#fef70c&quot;) ) . price price_prediction error abs_error . year month dayofmonth . 2019 1 22 278.777743 | 350.801331 | -72.023587 | 72.023587 | . 12 30 295.829202 | 342.720764 | -46.891562 | 46.891562 | . 2020 1 30 1044.447303 | 1078.613281 | -34.165979 | 34.165979 | . 10 2 -6.076028 | 21.710295 | -27.786323 | 27.786323 | . 3 -1.983471 | 21.918243 | -23.901714 | 23.901714 | . 1 23 -1.761423 | 19.056923 | -20.818346 | 20.818346 | . 2019 2 2 240.954524 | 257.686920 | -16.732396 | 16.732396 | . 8 31 79.347198 | 91.819000 | -12.471802 | 12.471802 | . 2020 8 30 9.421019 | 20.588486 | -11.167467 | 11.167467 | . 1 4 28.042231 | 38.049507 | -10.007276 | 10.007276 | . (error_by_day.sort_values(&#39;abs_error&#39;, ascending=False).head(10) .style .bar(subset=&quot;abs_error&quot;,color=&quot;#c07fef&quot;,vmax=error_by_day.abs_error.quantile(0.95)) .highlight_max(axis=0,color=&quot;#fef70c&quot;) ) . price price_prediction error abs_error . year month dayofmonth . 2019 1 24 4549.645105 | 1078.613281 | 3471.031824 | 3471.031824 | . 2020 1 31 2809.437516 | 812.882507 | 1996.555008 | 1996.555008 | . 2019 3 1 1284.799876 | 812.882507 | 471.917369 | 471.917369 | . 1 25 906.437232 | 645.617493 | 260.819740 | 260.819740 | . 22 278.777743 | 350.801331 | -72.023587 | 72.023587 | . 12 30 295.829202 | 342.720764 | -46.891562 | 46.891562 | . 2020 1 30 1044.447303 | 1078.613281 | -34.165979 | 34.165979 | . 10 2 -6.076028 | 21.710295 | -27.786323 | 27.786323 | . 3 -1.983471 | 21.918243 | -23.901714 | 23.901714 | . 2019 8 13 267.347650 | 246.398193 | 20.949457 | 20.949457 | . Insights &#128205; : Notice anything about the over forecasted days? . #1 worst day - February 2nd, 2020. | #6 worst day - March 25, 2019. | . Looks like our model influenced by outliers . . (error_by_day.sort_values(&#39;abs_error&#39;, ascending=True).head(10) .style .bar(subset=&quot;abs_error&quot;,color=&quot;#c07fef&quot;,vmax=error_by_day.abs_error.quantile(0.95)) .highlight_max(axis=0,color=&quot;#fef70c&quot;) ) . price price_prediction error abs_error . year month dayofmonth . 2019 7 29 95.873753 | 95.874405 | -0.000652 | 0.000652 | . 8 20 61.926547 | 61.927246 | -0.000699 | 0.000699 | . 2020 8 6 69.433709 | 69.434418 | -0.000708 | 0.000708 | . 9 9 37.015434 | 37.016582 | -0.001149 | 0.001149 | . 8 15 52.515043 | 52.512608 | 0.002436 | 0.002436 | . 2019 10 20 87.533212 | 87.530640 | 0.002573 | 0.002573 | . 2020 4 2 61.537020 | 61.540165 | -0.003145 | 0.003145 | . 14 40.838554 | 40.842064 | -0.003510 | 0.003510 | . 2019 6 9 69.561306 | 69.557457 | 0.003849 | 0.003849 | . 2020 3 23 44.639439 | 44.643665 | -0.004226 | 0.004226 | . Revenue generated per year using retail prices and a 70MWh storage system by the energy storage venture . predicton_vs_Actual = df_test.rename(columns={&quot;price&quot;:&quot;PRICE&quot;,&quot;price_prediction&quot;:&quot;PRICE_PREDICTION&quot;}) predicton_vs_Actual.head() . demand PRICE demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure ... price_30_days_std price_7_days_max price_15_days_max price_30_days_max price_7_days_min price_15_days_min price_30_days_min PRICE_PREDICTION error abs_error . date . 2019-01-02 106470.675 | 92.202011 | 106470.675 | 92.202011 | 0.0 | 0.0 | 0.0 | 18.4 | 22.2 | 26.3 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 92.063560 | 0.138451 | 0.138451 | . 2019-01-03 118789.605 | 127.380303 | 118789.605 | 127.380303 | 0.0 | 0.0 | 0.0 | 15.9 | 29.5 | 27.6 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 126.187721 | 1.192581 | 1.192581 | . 2019-01-04 133288.460 | 121.020997 | 133288.460 | 121.020997 | 0.0 | 0.0 | 0.0 | 18.0 | 42.6 | 27.4 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 121.093582 | -0.072585 | 0.072585 | . 2019-01-05 97262.790 | 83.493520 | 97262.790 | 83.493520 | 0.0 | 0.0 | 0.0 | 17.4 | 21.2 | 12.9 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 83.704323 | -0.210802 | 0.210802 | . 2019-01-06 93606.215 | 65.766407 | 93606.215 | 65.766407 | 0.0 | 0.0 | 0.0 | 14.6 | 22.1 | 30.9 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 65.620796 | 0.145611 | 0.145611 | . 5 rows × 40 columns . (predicton_vs_Actual.sample(5) .style .background_gradient(axis=0,subset=[&quot;PRICE&quot;,&quot;PRICE_PREDICTION&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real price and Prediction price&quot;) ) . Comparison of Real price and Prediction price demand PRICE demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day holiday date hour dayofweek quarter month year dayofyear dayofmonth weekofyear price_7_days_lag price_15_days_lag price_30_days_lag price_7_days_mean price_15_days_mean price_30_days_mean price_7_days_std price_15_days_std price_30_days_std price_7_days_max price_15_days_max price_30_days_max price_7_days_min price_15_days_min price_30_days_min PRICE_PREDICTION error abs_error . date . 2019-10-17 00:00:00 116690.040000 | 92.498044 | 116690.040000 | 92.498044 | 0.000000 | 0.000000 | 0.000000 | 9.500000 | 15.200000 | 9.600000 | 4.400000 | 1 | N | 2019-10-17 00:00:00 | 0 | 3 | 4 | 10 | 2019 | 290 | 17 | 42 | 167.981136 | 108.661677 | 149.599176 | 107.013610 | 111.958930 | 115.165759 | 14.385468 | 26.376231 | 28.347460 | 123.261328 | 167.981136 | 192.485035 | 90.164236 | 54.720115 | 54.720115 | 92.161629 | 0.336415 | 0.336415 | . 2020-02-28 00:00:00 110006.970000 | 50.306353 | 110006.970000 | 50.306353 | 0.000000 | 0.000000 | 0.000000 | 13.100000 | 20.100000 | 17.600000 | 0.000000 | 1 | N | 2020-02-28 00:00:00 | 0 | 4 | 1 | 2 | 2020 | 59 | 28 | 9 | 48.658203 | 79.018494 | 78.272342 | 50.750555 | 54.706821 | 174.695236 | 6.434453 | 9.322584 | 529.975511 | 60.554702 | 79.502644 | 2809.437516 | 42.236094 | 42.236094 | 14.235635 | 50.108658 | 0.197695 | 0.197695 | . 2019-05-26 00:00:00 106153.430000 | 63.903082 | 106153.430000 | 63.903082 | 0.000000 | 0.000000 | 0.000000 | 9.500000 | 15.100000 | 8.600000 | 6.600000 | 1 | N | 2019-05-26 00:00:00 | 0 | 6 | 2 | 5 | 2019 | 146 | 26 | 21 | 76.294168 | 87.036533 | 60.918383 | 90.270987 | 89.690794 | 95.045620 | 17.449081 | 13.195141 | 13.561886 | 115.894456 | 115.894456 | 121.029642 | 63.903082 | 63.903082 | 63.903082 | 63.706802 | 0.196280 | 0.196280 | . 2019-03-19 00:00:00 123290.835000 | 113.644774 | 123290.835000 | 113.644774 | 0.000000 | 0.000000 | 0.000000 | 16.900000 | 23.400000 | 12.700000 | 0.000000 | 1 | N | 2019-03-19 00:00:00 | 0 | 1 | 1 | 3 | 2019 | 78 | 19 | 12 | 76.971548 | 135.206447 | 110.489593 | 110.805143 | 100.527496 | 148.568061 | 11.182552 | 17.085113 | 215.716053 | 126.785957 | 126.785957 | 1284.799876 | 95.251188 | 74.374932 | 74.374932 | 113.493477 | 0.151297 | 0.151297 | . 2019-04-02 00:00:00 114927.205000 | 119.397551 | 114927.205000 | 119.397551 | 0.000000 | 0.000000 | 0.000000 | 7.400000 | 25.900000 | 16.600000 | 0.000000 | 1 | N | 2019-04-02 00:00:00 | 0 | 1 | 2 | 4 | 2019 | 92 | 2 | 14 | 95.567894 | 111.149795 | 153.784296 | 105.271113 | 105.146212 | 103.555577 | 17.997891 | 15.698139 | 17.232821 | 131.333531 | 131.333531 | 135.206447 | 83.989538 | 80.292927 | 74.374932 | 118.808243 | 0.589308 | 0.589308 | . (predicton_vs_Actual.describe() .style .highlight_max(axis=0,color=&quot;#c07fef&quot;) .highlight_min(axis=0,color=&quot;#00FF00&quot;) .set_caption(&quot;Price for the next 12-18 months dataset&quot;) ) . Price for the next 12-18 months dataset demand PRICE demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day hour dayofweek quarter month year dayofyear dayofmonth weekofyear price_7_days_lag price_15_days_lag price_30_days_lag price_7_days_mean price_15_days_mean price_30_days_mean price_7_days_std price_15_days_std price_30_days_std price_7_days_max price_15_days_max price_30_days_max price_7_days_min price_15_days_min price_30_days_min PRICE_PREDICTION error abs_error . count 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 637.000000 | 629.000000 | 614.000000 | 638.000000 | 630.000000 | 615.000000 | 638.000000 | 630.000000 | 615.000000 | 638.000000 | 630.000000 | 615.000000 | 638.000000 | 630.000000 | 615.000000 | 644.000000 | 644.000000 | 644.000000 | . mean 117643.164884 | 92.987010 | 115960.088610 | 93.920500 | 1683.076273 | -2.628288 | 0.018763 | 11.258851 | 20.047516 | 14.121429 | 1.571118 | 0.579193 | 0.000000 | 3.000000 | 2.310559 | 5.920807 | 2019.434783 | 164.804348 | 15.633540 | 24.015528 | 93.691424 | 94.312974 | 95.825203 | 93.212968 | 93.538990 | 91.768173 | 54.150249 | 72.852308 | 87.392116 | 200.741524 | 322.505861 | 494.818502 | 52.455595 | 43.836577 | 36.683321 | 83.745338 | 9.241676 | 10.733489 | . std 13719.536422 | 221.045838 | 16058.351649 | 220.815258 | 5468.830434 | 14.501710 | 0.061677 | 4.178613 | 6.275934 | 7.527379 | 4.622587 | 0.494072 | 0.000000 | 2.001555 | 1.038818 | 3.204041 | 0.496114 | 97.726483 | 8.830309 | 13.916315 | 222.139909 | 223.477412 | 225.979776 | 105.016925 | 78.052549 | 55.971449 | 204.237108 | 204.005225 | 183.989880 | 555.624866 | 798.388712 | 1010.230907 | 26.400018 | 23.519784 | 21.738763 | 83.260063 | 159.133995 | 159.040192 | . min 86891.230000 | -6.076028 | 41988.240000 | 14.558266 | 0.000000 | -304.150000 | 0.000000 | 1.700000 | 9.600000 | 1.300000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 2019.000000 | 1.000000 | 1.000000 | 1.000000 | -1.761423 | -1.761423 | -1.761423 | 23.314900 | 26.399112 | 30.011994 | 2.643165 | 5.364451 | 8.586690 | 33.031411 | 37.881752 | 53.063441 | -6.076028 | -6.076028 | -6.076028 | 14.941895 | -72.023587 | 0.000652 | . 25% 107860.791250 | 47.951427 | 106688.311250 | 48.534078 | 0.000000 | 0.000000 | 0.000000 | 8.300000 | 15.300000 | 8.200000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 3.000000 | 2019.000000 | 81.750000 | 8.000000 | 12.000000 | 48.336165 | 48.542669 | 49.852245 | 48.849650 | 50.822983 | 52.430808 | 10.068998 | 12.243217 | 15.547400 | 64.270729 | 75.818450 | 103.916022 | 31.208768 | 21.755296 | 18.875436 | 47.338225 | -0.289771 | 0.100342 | . 50% 115552.020000 | 70.942903 | 115005.677500 | 72.009555 | 0.000000 | 0.000000 | 0.000000 | 11.000000 | 18.700000 | 11.700000 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | 2.000000 | 6.000000 | 2019.000000 | 162.000000 | 16.000000 | 24.000000 | 72.065599 | 72.815311 | 74.405598 | 75.313011 | 83.410650 | 87.232646 | 15.213009 | 18.972156 | 23.018682 | 100.676262 | 125.099031 | 134.747615 | 49.910197 | 44.176312 | 42.328652 | 71.584919 | -0.011184 | 0.268921 | . 75% 127321.842500 | 101.439833 | 127115.777500 | 101.665942 | 0.000000 | 0.000000 | 0.000000 | 13.800000 | 23.200000 | 19.300000 | 1.050000 | 1.000000 | 0.000000 | 5.000000 | 3.000000 | 8.000000 | 2020.000000 | 242.250000 | 23.000000 | 35.000000 | 101.944171 | 102.608515 | 103.262500 | 103.681588 | 105.118976 | 105.963637 | 24.736949 | 32.092289 | 38.685609 | 134.488422 | 162.491995 | 243.810160 | 75.512742 | 63.903082 | 52.525320 | 101.335741 | 0.233059 | 0.588213 | . max 170653.840000 | 4549.645105 | 170653.840000 | 4549.645105 | 57597.595000 | 0.000000 | 0.625000 | 25.100000 | 43.500000 | 32.000000 | 54.600000 | 1.000000 | 0.000000 | 6.000000 | 4.000000 | 12.000000 | 2020.000000 | 365.000000 | 31.000000 | 52.000000 | 4549.645105 | 4549.645105 | 4549.645105 | 902.337815 | 506.831433 | 318.015562 | 1663.267896 | 1140.727552 | 817.222368 | 4549.645105 | 4549.645105 | 4549.645105 | 118.261945 | 89.427805 | 80.292927 | 1078.613281 | 3471.031824 | 3471.031824 | . train_df_result =df_train.groupby(&quot;year&quot;)[&quot;price&quot;,&quot;demand_pos_price&quot;,&quot;demand_neg_price&quot;,&quot;frac_neg_price&quot;,&quot;price_negative&quot;,&quot;demand&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).stack() (train_df_result .style .background_gradient(axis=0,subset=[&quot;demand&quot;,&quot;price&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real Demand and Prediction Demand&quot;) ) . Comparison of Real Demand and Prediction Demand price demand_pos_price demand_neg_price frac_neg_price price_negative demand . year . 2015 mean 35.068136 | 124491.479493 | 170.690384 | 0.001598 | -2.305297 | 124662.169877 | . max 188.086125 | 158052.890000 | 19480.250000 | 0.229167 | 0.000000 | 158052.890000 | . min 13.279841 | 84331.030000 | 0.000000 | 0.000000 | -318.660000 | 95093.295000 | . 2016 mean 50.094252 | 121028.504249 | 961.034508 | 0.010303 | -4.438947 | 121989.538757 | . max 545.737820 | 160285.015000 | 29110.575000 | 0.333333 | 0.000000 | 160285.015000 | . min 6.869135 | 65215.145000 | 0.000000 | 0.000000 | -289.190000 | 90227.480000 | . 2017 mean 94.740161 | 118910.605411 | 42.443274 | 0.000400 | -2.460516 | 118953.048685 | . max 213.339432 | 154632.335000 | 4666.465000 | 0.041667 | 0.000000 | 155060.610000 | . min 19.865114 | 85094.375000 | 0.000000 | 0.000000 | -342.220000 | 85094.375000 | . 2018 mean 94.648823 | 118438.166507 | 372.398726 | 0.003881 | -1.643921 | 118810.565233 | . max 1210.137920 | 165070.595000 | 22839.285000 | 0.250000 | 0.000000 | 165070.595000 | . min 14.673588 | 73658.870000 | 0.000000 | 0.000000 | -157.920000 | 88903.065000 | . 2019 mean 78.560979 | 98933.060000 | 0.000000 | 0.000000 | 0.000000 | 98933.060000 | . max 78.560979 | 98933.060000 | 0.000000 | 0.000000 | 0.000000 | 98933.060000 | . min 78.560979 | 98933.060000 | 0.000000 | 0.000000 | 0.000000 | 98933.060000 | . test_df_result =df_test.groupby(&quot;year&quot;)[&quot;price&quot;,&quot;demand_pos_price&quot;,&quot;demand_neg_price&quot;,&quot;frac_neg_price&quot;,&quot;price_negative&quot;,&quot;demand&quot;,&quot;price_prediction&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).stack() (test_df_result .style .background_gradient(axis=0,subset=[&quot;price&quot;,&quot;price_prediction&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real Demand and Prediction Demand&quot;) ) . Comparison of Real Demand and Prediction Demand price demand_pos_price demand_neg_price frac_neg_price price_negative demand price_prediction . year . 2019 mean 117.387745 | 117288.371772 | 703.457431 | 0.007612 | -1.718283 | 117991.829203 | 106.173088 | . max 4549.645105 | 168894.845000 | 17410.610000 | 0.166667 | 0.000000 | 168894.845000 | 1078.613281 | . min 19.170951 | 80859.020000 | 0.000000 | 0.000000 | -89.465035 | 90145.615000 | 21.854008 | . 2020 mean 61.266055 | 114233.320500 | 2956.580768 | 0.033259 | -3.811295 | 117189.901268 | 54.589256 | . max 2809.437516 | 170653.840000 | 57597.595000 | 0.625000 | 0.000000 | 170653.840000 | 1078.613281 | . min -6.076028 | 41988.240000 | 0.000000 | 0.000000 | -304.150000 | 86891.230000 | 14.941895 | . test_df_result =df_test.groupby(&quot;year&quot;)[&quot;price&quot;,&quot;demand_pos_price&quot;,&quot;demand_neg_price&quot;,&quot;frac_neg_price&quot;,&quot;price_negative&quot;,&quot;demand&quot;,&quot;price_prediction&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).stack() (test_df_result .style .background_gradient(axis=0,subset=[&quot;price&quot;,&quot;price_prediction&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real Demand and Prediction Demand&quot;) ) . Comparison of Real Demand and Prediction Demand price demand_pos_price demand_neg_price frac_neg_price price_negative demand price_prediction . year . 2019 mean 117.387745 | 117288.371772 | 703.457431 | 0.007612 | -1.718283 | 117991.829203 | 106.173088 | . max 4549.645105 | 168894.845000 | 17410.610000 | 0.166667 | 0.000000 | 168894.845000 | 1078.613281 | . min 19.170951 | 80859.020000 | 0.000000 | 0.000000 | -89.465035 | 90145.615000 | 21.854008 | . 2020 mean 61.266055 | 114233.320500 | 2956.580768 | 0.033259 | -3.811295 | 117189.901268 | 54.589256 | . max 2809.437516 | 170653.840000 | 57597.595000 | 0.625000 | 0.000000 | 170653.840000 | 1078.613281 | . min -6.076028 | 41988.240000 | 0.000000 | 0.000000 | -304.150000 | 86891.230000 | 14.941895 | . df_electricity[&quot;total_price&quot;] = df_electricity[&quot;demand&quot;]*df_electricity[&quot;price&quot;] . df_electricity.groupby(df_electricity.index.year)[&quot;price&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).unstack() . date mean 2015 35.068136 2016 50.094252 2017 94.740161 2018 94.648823 2019 117.281370 2020 61.266055 max 2015 188.086125 2016 545.737820 2017 213.339432 2018 1210.137920 2019 4549.645105 2020 2809.437516 min 2015 13.279841 2016 6.869135 2017 19.865114 2018 14.673588 2019 19.170951 2020 -6.076028 dtype: float64 . demand_negative_price = df_electricity[df_electricity[&quot;price_negative&quot;] &lt; 0] demand_negative_price . demand price demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day holiday total_price . date . 2015-01-01 99635.030 | 25.633696 | 97319.240 | 26.415953 | 2315.790 | -7.240000 | 0.020833 | 13.3 | 26.9 | 23.6 | 0.0 | N | Y | 2.554014e+06 | . 2015-01-02 129606.010 | 33.138988 | 121082.015 | 38.837661 | 8523.995 | -47.809777 | 0.062500 | 15.4 | 38.8 | 26.8 | 0.0 | N | N | 4.295012e+06 | . 2015-01-07 153514.820 | 48.312309 | 149498.715 | 49.639712 | 4016.105 | -1.100000 | 0.020833 | 18.9 | 37.4 | 20.7 | 0.0 | N | N | 7.416655e+06 | . 2015-01-18 97728.750 | 17.008681 | 95473.965 | 20.911790 | 2254.785 | -148.260000 | 0.020833 | 15.3 | 19.5 | 23.4 | 0.0 | N | N | 1.662237e+06 | . 2015-02-13 136070.620 | 18.736971 | 133078.540 | 26.322857 | 2992.080 | -318.660000 | 0.020833 | 16.1 | 32.4 | 14.9 | 0.0 | Y | N | 2.549551e+06 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2020-10-01 106641.790 | 34.654671 | 95349.610 | 41.651658 | 11292.180 | -24.426925 | 0.125000 | 9.4 | 19.5 | 21.2 | 1.8 | N | N | 3.695636e+06 | . 2020-10-02 99585.835 | -6.076028 | 41988.240 | 26.980251 | 57597.595 | -30.173823 | 0.625000 | 12.8 | 26.0 | 22.0 | 0.0 | N | N | -6.050864e+05 | . 2020-10-03 92277.025 | -1.983471 | 44133.510 | 32.438156 | 48143.515 | -33.538025 | 0.583333 | 17.4 | 29.4 | 19.8 | 0.0 | N | N | -1.830288e+05 | . 2020-10-04 94081.565 | 25.008614 | 88580.995 | 26.571687 | 5500.570 | -0.163066 | 0.062500 | 13.5 | 29.5 | 8.4 | 0.0 | N | N | 2.352850e+06 | . 2020-10-05 113610.030 | 36.764701 | 106587.375 | 39.616015 | 7022.655 | -6.511550 | 0.083333 | 9.1 | 12.7 | 7.3 | 12.8 | N | N | 4.176839e+06 | . 181 rows × 14 columns . demand_positive_price = df_electricity[df_electricity[&quot;price_negative&quot;] &gt;= 0] demand_positive_price . demand_negative= (demand_negative_price.groupby(demand_negative_price.index.year)[&quot;total_price&quot;].agg([&quot;sum&quot;,&quot;max&quot;,&quot;min&quot;])).reset_index() demand_negative . This is the condition when the demand for negative price per year. You can see on 2020 where the minimum price show a negative value. . demand_positive = demand_positive_price.groupby(demand_positive_price.index.year)[&quot;total_price&quot;].agg([&quot;sum&quot;,&quot;max&quot;,&quot;min&quot;]).reset_index() demand_positive . This is the condition when the demand for positive price per year. You can see on per year where the minimum price show a an appreciation from the market that is shown on the positive electiricity price. . result = demand_negative.merge(demand_positive,on=&quot;date&quot;,suffixes =[&quot;_demand_pos&quot;,&quot;_demand_neg&quot;]) result . result[&quot;profit&quot;] =result[&quot;sum_demand_pos&quot;]-result[&quot;sum_demand_neg&quot;] result . We then substract the result when the demand is positive and demand is negative based on the total price feature that we have been created. Most of the years shows a good result. . (result.sort_values(&quot;profit&quot;,ascending=False) .style .background_gradient(axis=0,subset=[&quot;profit&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Revenue Generated per year in the daily electiricity in Victoria Australia&quot;) ) . This is the result of revenue generated per year. We can see a good revenue generated per year. If you find this notebook usefull, Please show your appreciation by upvoting this notebook. Thank you. . References . Here is a few references for creating advanced visualization using matplotlib 1. . 1. You can see a lot of data visualization plots here Data Visualization with Python!↩ . 2. Towards Data Science Article by Shiu Tang Li !↩ . 3. TPSJAN22-01 EDA which makes sense !↩ . .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/datavisualization/matplotlib/2022/01/01/datavisualization-advanced.html",
            "relUrl": "/datavisualization/matplotlib/2022/01/01/datavisualization-advanced.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "My key takeaways from reading Trustworthy Online Controlled Experiments Book",
            "content": "Achievement unlocked: Our book is the #1 Best Seller in Data Mining: Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing (https://t.co/aseJyTKbDh).I&#39;m humbled.#abtesting #hippo #experimentguide pic.twitter.com/5CckQcXnPH . &mdash; Ronny Kohavi (@ronnyk) June 29, 2021 This article contains about A/B testing, make sure the reader understands fundamental concepts about control and treatment, p-values, confidence interval, statistical significance, practical significance, randomization, sample, and population in order to get the idea about A/B testing. You can get the fundamental statistics by looking at this video. . . Experimentation/Testing has been everywhere. It is widely adopted by startups to the corporate firm to detect how good the simple or bigger changes of the project or additional feature(be it a website, mobile app, etc.) of the project to give impact to the real-life/business. In Data Science, Experimentation is widely used to predict how good our experimentation is based on a few metrics by using statistical approaches. Online Trustworthy Controlled Experiment is the book I wish I had when I started learning A/B testing/Experimentation. This book covered all the fundamental concepts to advanced concepts about A/B testing through a step-by-step walkthrough such as designing the experimentation, running the experimentation and getting data, interpreting the results and results to decision-making. The author explained clearly the pitfalls and solutions to the problems that could exist during the experimentation. . A step-by-step walkthrough of A/B Testing . Designing The Experiment . The first step of doing online experimentation is to ascertain our hypothesis, a practical significance boundary, and a few metrics before running the experimentation. We should check the randomization of the sample that we will use for the control and treatment. We also should pay attention to how large the sample is to be used for running the experimentation. If we are concerned about detecting a small change or being more confident about the conclusion, we have to consider using more samples and a lower p-value threshold to get a more accurate result. However, If we no longer care about small changes, we could reduce the sample to detect the practical significance. . Getting the Data and Running The Experimentation . In this section, you are going to get some data pertaining to the experimentation that we will be analyzing such as analyzing how many samples should be used, the day of week effect due to everyone having different behavior on weekdays over the weekend, and also seasonality where users behave differently on holiday. We also consider looking at primacy and novelty effects where users have a tendency to use new features, . Interpreting the Results . One thing we should consider when interpreting results is how our experimentation will run properly and avoid some bugs that could invalidate the experiment result(guardrail metrics). For instance, we can check the latency which is essential to check that can affect the control and treatment, or expect the control and treatment sample to be equal to the configuration we set for A/B testing. These factors must be fulfilled to get better results that can affect the metrics we are going to achieve. . From Results to Decisions . Getting a result from the experiment is not the end of the experimentation. Getting a result that can make an impact on the business will be a good way of implementing experimentation. In A/B Testing, good results can be considered good if they are repeatable and trustworthy. However, there are a few factors that should consider regarding whether we need to make a tradeoff between metrics for instance user engagement and revenue. Should we launch if there is no correlation between this metric?. We also consider launching costs whether the revenue will cover the launch cost or get more expected revenue even need much cost to launch the product. Furthermore, We also consider statistical and practical significance thresholds of whether to launch or not launch the product. . Statistical and Practical Significance Thresholds . Source: Online Trustworthy Controlled Experiment . The figure shown above depicts the statistical and practical significance threshold where the two dashed lines are the practical significance boundary and the black box is the statistical significance threshold along with the confidence interval. We know from statistics theory that the statistical significance threshold is less than or equal to 5% to quantify that we should reject the null hypothesis and the practical significance is managed based on the condition of our objective that we wanted to achieve. Based on the practical and statistical significance, we can take a step either choosing to launch or not. However, we can even take a follow-up test to test our hypothesis in order to translate practical and statistical significance boundaries based on some consideration of our experiment to get statistical power(a condition where the p-value is less than(more extreme) or equal to 0.05 implying there is a difference between control and treatment mean assuming the null hypothesis is true). . Conclusion . This book is really essential for every data scientist who specialized in product analytics in order to cover our understanding of data better through A/B testing. You will get a lot of enlightenment after reading this book. Read it, buy it. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/abtesting/statistics/2021/06/20/Trustworthy-Online-Controlled-Experiments.html",
            "relUrl": "/abtesting/statistics/2021/06/20/Trustworthy-Online-Controlled-Experiments.html",
            "date": " • Jun 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Josua is a business development analyst who turns into a self-taught Machine Learning Engineer. His interests include statistical learning, predictive modeling, and causal inference. He loves running and it teaches him against giving up doing anything, even when implementing the Machine Learning Lifecycle(MLOps). . Apart from pursuing his passion for Machine Learning, he is keen on investing in the Indonesian Stock Exchange and Cryptocurrency. He has been running a full marathon in Jakarta Marathon in 2015 and Osaka Marathon in 2019. His next dreams are to run a marathon in Boston Marathon, TCS New York City Marathon and Virgin Money London Marathon. . My Certification . I am not a guy who love collecting certifications in Data Science. Perhaps, you will find somewhere that i have completed a few certifications in Data Science. I love learning and implement what i learn in data science project is paramount. . Papers I am currently interested in arXiv . “Why Should I Trust You?” Explaining the Predictions of Any Classifier | Evaluation Gaps in Machine Learning Practice | 50 Years of Data Science | Cyclical Learning Rates for Training Neural Networks | Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle | On Artificial Intelligence - A European approach to excellence and trust | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | The Unreasonable Effectiveness of Data | Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX) | Microsoft COCO: Common Objects in Context | is all you need: On the influence of random seeds in deep learning architectures for computer vision | Advancing mathematics by guiding human intuition with AI | Machine Learning Operations (MLOps): Overview, Definition, and Architecture | . Contact Me . josuadotnaiborhu94atgmaildotcom . © Josua Antonius Naiborhu, 2020-2022.These posts are meant to be used for educational purposes. Excerpts and links from this site may be used, provided that full and clear credit is given to Josua Naiborhu with appropriate and specific direction to the original content. .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}