{
  
    
        "post0": {
            "title": "NLP Approach using Word Embedding",
            "content": "This is a minimal yet very powerful approach of NLP problems that you can use on Kaggle competition. I got my attention to the book written by one of Kaggle Grandmaster in his book called Approaching (Almost) Any Machine Learning Problem. I am so interested in how the author approaches the problems for each case in Kaggle Competition started by Supervised and Unsuperviced problems. So, NLP is one of the problems in the competition in Kaggle. We know that many approaches that we can do to preprocess text data from Bag of Words, TFIDF to Word Embedding. I am interested to dive deeper into Word Embedding because this approach yield a better result compared to BOW or Tfidf approach based on this dataset. You can find the comparison by reading this book. I found this approach quite useful for my NLP competition. Before reading this NLP approach, I hope the readers have the fundamentals concepts of pytorch, data visualization libraries and NLP concept to know the code better. . Import Data . import pandas as pd movies = pd.read_csv(&quot;imdb.csv&quot;) movies.head() . review sentiment . 0 One of the other reviewers has mentioned that ... | positive | . 1 A wonderful little production. &lt;br /&gt;&lt;br /&gt;The... | positive | . 2 I thought this was a wonderful way to spend ti... | positive | . 3 Basically there&#39;s a family where a little boy ... | negative | . 4 Petter Mattei&#39;s &quot;Love in the Time of Money&quot; is... | positive | . Check Proportion of target . movies.sentiment.value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . Create Cross Validation . import pandas as pd from sklearn import model_selection if __name__==&quot;__main__&quot;: df = pd.read_csv(&quot;imdb.csv&quot;) df.sentiment = df.sentiment.apply(lambda x: 1 if x == &quot;positive&quot; else 0) df[&quot;kfold&quot;] =-1 df = df.sample(frac=1).reset_index(drop=True) y = df.sentiment.values kf = model_selection.StratifiedKFold(n_splits=5) for f,(t_,v_) in enumerate(kf.split(X=df,y=y)): df.loc[v_,&quot;kfold&quot;] =f df.to_csv(&quot;imdb_folds.csv&quot;,index=False) . movies_folds = pd.read_csv(&quot;imdb_folds.csv&quot;) movies_folds.head() . review sentiment kfold . 0 I enjoyed Erkan &amp; Stefan  a cool and fast sto... | 1 | 0 | . 1 The only reason I rated this film as 2 is beca... | 0 | 0 | . 2 One of those movies where you take bets on who... | 0 | 0 | . 3 This series was just like what you would expec... | 1 | 0 | . 4 While many people found this film simply too s... | 1 | 0 | . There is one additional features called kfold. . Word Embedding . import numpy as np def sentence_to_vec(s,embedding_dict,stop_words,tokenizer): words =str(s).lower() words =tokenizer(words) words = [ w for w in words if w not in stop_words] words = [w for w in words if w.alpha()] M =[] for w in words: if w in embedding_dict: M.append(embedding_dict[w]) if len(M)==0: return np.zeros(300) M = np.array(M) v = M.sum() return v/np.sqrt((v**2).sum()) . Create Dataset in pytorch based on model in our dataset . import torch class IMDBDataset: def __init__(self,reviews,targets): self.reviews =reviews self.targets = targets def __len__(self): return len(self.reviews) def __getitem__(self,item): review =self.reviews[item,:] target =self.target[item] return { &quot;review&quot;: torch.tensor(review,dtype=torch.long), &quot;target&quot;: torch.tensor(target,dtype=torch.float) } . Create Model . import torch.nn as nn class LSTM(nn.Module): def __init__(self,embedding_matrix): super(LSTM,self).__init__() num_words =embedding_matrix.shape[0] embed_dim= embedding_matrix.shape[1] self.embedding = nn.Embedding( num_embeddings = num_words, embedding_dim=embed_dim ) self.embedding.weight = nn.Parameter( torch.tensor( embedding_matrix, dtype=torch.float32 ) ) self.embedding.weight.requires_grad=False self.lstm = nn.LSTM( embed_dim, 128, bidirectional=True, batch_first=True ) self.out = nn.Linear(512,1) def forward(self,x): x = self.embedding(x) x,_ = self.lstm(x) avg_pool =torch.mean(x,1) max_pool, _ = torch.max(x,1) out = torch.cat((avg_pool,maxpool),1) out = self.out(out) return out . Create Training Function for Modelling . def train(data_loader,model,optimizer,device): model.train() for data in data_loader: reviews = data[&quot;review&quot;] targets = data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.float) optimizer.zero_grad() predictions = model(reviews) loss =nn.BCEWithLogitsLoss()( predictions, targets.view(-1,1) ) loss.bakward() optimizer.step() . Create Evaluation for Modelling . def evaluate(data_loader,model,device): final_predictions =[] final_targets = [] model.eval() with torch.no_grad(): for data in data_loader: reviews =data[&quot;review&quot;] targets =data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.long) predictions = model(reviews) predictions = predictions.cpu().numpy().tolist() targets = data[&quot;target&quot;].cpu().numpy.tolist() final_predictions.extend(predictions) final_targets.extend(targets) return final_predictions,final_targets . Word Embedding Creation . import io #from tensorflow.keras import import tensorflow as tf def load_vectors(fname): fin = io.open( fname, &quot;r&quot;, encoding=&quot;utf-8&quot;, newline=&quot; n&quot;, errors=&quot;ignore&quot; ) n,d = map(int,fin.readline().split()) data ={} for line in fin: tokens = line.rstrip().split(&#39; &#39;) data[tokens[0]] = list(map(float,tokens[1:])) return data def create_embedding_matrix(world_index,embedding_dict): embedding_matrix = np.zeros((len(word_index)+1,300)) for word , i in word_index.items(): if word in embedding_dict: embedding_dict[i] = embedding_dict[word] return embedding_matrix def run(df,fold): train_df = df[df.kfold != fold].reset_index(drop=True) valid_df = df[df.kfold ==fold].reset_index(drop=True) print(&quot;Fitting tokenizer&quot;) tokenizer = tf.keras.preprocessing.text.Tokenizer() tokenizer.fit_on_texts(df.review.values.tolist()) xtrain = tokenizer.texts_to_sequences(train_df.review.values) xtest = tokenizer.texts_to_sequences(valid_df.review.values) xtrain = tf.keras.preprocessing.sequence.pad_sequences( xtrain,maxlen=128 ) xtest = tf.keras.preprocessing.sequence.pad_sequences( xtest,maxlen=128 ) train_dataset = IMDBDataset( reviews =xtrain, targets = train_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( train_dataset, batch_size =16, num_workers=2 ) valid_dataset =IMDBDataset( reviews =xtest, targets = valid_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size =8, num_workers=1 ) print(&quot;Loading Embeddings&quot;) # you can suit based on where you put your vec fasttext embedding_dict = load_vectors(&quot;crawl-300d-2M.vec/crawl-300d-2M.vec&quot;) embedding_matrix = create_embedding_matrix( tokenizer.word_index,embedding_dict ) device =torch.device(&quot;cuda&quot;) model =LSTM(embedding_matrix) model.to(device) optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) print(&quot;Training Model&quot;) best_accuracy =0 early_stopping_counter =0 for epoch in range(10): train(train_data_loader,model,optimizer,device) outputs,targets = evaluate(valid_data_loader,model,device) outputs = np.array(outputs) &gt;=0.5 accuracy = metrics.accuracy_score(targets,outputs) print(f&quot;{fold}, Epoch {epoch}, Accuracy Score ={accuracy}&quot;) if accuracy &gt; best_accuracy: best_accuracy = accuracy else: early_stopping_counter +=1 if early_stopping_counter &gt; 2: break if __name__ == &quot;__main__&quot;: df = pd.read_csv(&quot;imdb_folds.csv&quot;) run(df,0) run(df,1) run(df,2) run(df,3) run(df,4) . Fitting tokenizer Loading Embeddings . The choice of Machine learning algorithms will determine the quality of our predictions score. However, The simple model will also determine how fast the training process compared to state of art of ML algorithms. If we have laptop/computer with better GPU can help the training process. But in real life, Simple models with better preprocessing will have predictions score that is not too dfferent with newest ML algorithms. So, It is better to discuss with the stakeholder for improving the models based on business metrics. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/06/approachingnlpusingwordembedding.html",
            "relUrl": "/2022/04/06/approachingnlpusingwordembedding.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Mean encodings",
            "content": "Version 1.1.0 . In this programming assignment you will be working with 1C dataset from the final competition. You are asked to encode item_id in 4 different ways: . 1) Via KFold scheme; 2) Via Leave-one-out scheme; 3) Via smoothing scheme; 4) Via expanding mean scheme. . You will need to submit the correlation coefficient between resulting encoding and target variable up to 4 decimal places. . General tips . Fill NANs in the encoding with 0.3343. | Some encoding schemes depend on sorting order, so in order to avoid confusion, please use the following code snippet to construct the data frame. This snippet also implements mean encoding without regularization. | . import pandas as pd import numpy as np from itertools import product import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) from grader import Grader . Read data . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) . sales.head() . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . sales.shape . (2935849, 6) . Aggregate data . Since the competition task is to make a monthly prediction, we need to aggregate the data to montly level before doing any encodings. The following code-cell serves just that purpose. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales[sales[&#39;date_block_num&#39;]==block_num][&#39;shop_id&#39;].unique() cur_items = sales[sales[&#39;date_block_num&#39;]==block_num][&#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) #turn the grid into pandas dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) #get aggregated values for (shop_id, item_id, month) gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) #fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] #join aggregated data to the grid all_data = pd.merge(grid,gb,how=&#39;left&#39;,on=index_cols).fillna(0) #sort the data all_data.sort_values([&#39;date_block_num&#39;,&#39;shop_id&#39;,&#39;item_id&#39;],inplace=True) . Mean encodings without regularization . After we did the techinical work, we are ready to actually mean encode the desired item_id variable. . Here are two ways to implement mean encoding features without any regularization. You can use this code as a starting point to implement regularized techniques. . Method 1 . item_id_target_mean = all_data.groupby(&#39;item_id&#39;).target.mean() # In our non-regularized case we just *map* the computed means to the `item_id`&#39;s all_data[&#39;item_target_enc&#39;] = all_data[&#39;item_id&#39;].map(item_id_target_mean) # Fill NaNs all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) # Print correlation encoded_feature = all_data[&#39;item_target_enc&#39;].values print(np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1]) . 0.483038698862 . Method 2 . &#39;&#39;&#39; Differently to `.target.mean()` function `transform` will return a dataframe with an index like in `all_data`. Basically this single line of code is equivalent to the first two lines from of Method 1. &#39;&#39;&#39; all_data[&#39;item_target_enc&#39;] = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;mean&#39;) # Fill NaNs all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) # Print correlation encoded_feature = all_data[&#39;item_target_enc&#39;].values print(np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1]) . 0.483038698862 . See the printed value? It is the correlation coefficient between the target variable and your new encoded feature. You need to compute correlation coefficient between the encodings, that you will implement and submit those to coursera. . grader = Grader() . 1. KFold scheme . Explained starting at 41 sec of Regularization video. . Now it&#39;s your turn to write the code! . You may use &#39;Regularization&#39; video as a reference for all further tasks. . First, implement KFold scheme with five folds. Use KFold(5) from sklearn.model_selection. . Split your data in 5 folds with sklearn.model_selection.KFold with shuffle=False argument. | Iterate through folds: use all but the current fold to calculate mean target for each level item_id, and fill the current fold. . See the Method 1 from the example implementation. In particular learn what map and pd.Series.map functions do. They are pretty handy in many situations. | . | from sklearn.model_selection import KFold kf = KFold(n_splits=5,shuffle=False) for tr_ind, val_ind in kf.split(all_data): X_tr, X_val = all_data.iloc[tr_ind], all_data.iloc[val_ind] X_val[&#39;item_target_enc&#39;] = X_val[&#39;item_id&#39;].map(X_tr.groupby(&#39;item_id&#39;).target.mean()) all_data.iloc[val_ind] = X_val all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values # You will need to compute correlation like that corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;KFold_scheme&#39;, corr) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy . 0.41645907128 Current answer for task KFold_scheme is: 0.41645907128 . 2. Leave-one-out scheme . Now, implement leave-one-out scheme. Note that if you just simply set the number of folds to the number of samples and run the code from the KFold scheme, you will probably wait for a very long time. . To implement a faster version, note, that to calculate mean target value using all the objects but one given object, you can: . Calculate sum of the target values using all the objects. | Then subtract the target of the given object and divide the resulting value by n_objects - 1. | Note that you do not need to perform 1. for every object. And 2. can be implemented without any for loop. . It is the most convenient to use .transform function as in Method 2. . target_sum = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;sum&#39;) n_objects = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;count&#39;) all_data[&#39;item_target_enc&#39;] = (target_sum - all_data[&#39;target&#39;]) / (n_objects - 1) all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Leave-one-out_scheme&#39;, corr) . 0.480384831129 Current answer for task Leave-one-out_scheme is: 0.480384831129 . 3. Smoothing . Explained starting at 4:03 of Regularization video. . Next, implement smoothing scheme with $ alpha = 100$. Use the formula from the first slide in the video and $0.3343$ as globalmean. Note that nrows is the number of objects that belong to a certain category (not the number of rows in the dataset). . item_id_target_mean = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;mean&#39;) n_objects = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;count&#39;) all_data[&#39;item_target_enc&#39;] = (item_id_target_mean * n_objects + 0.3343 * 100) / (n_objects + 100) all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Smoothing_scheme&#39;, corr) . 0.48181987971 Current answer for task Smoothing_scheme is: 0.48181987971 . 4. Expanding mean scheme . Explained starting at 5:50 of Regularization video. . Finally, implement the expanding mean scheme. It is basically already implemented for you in the video, but you can challenge yourself and try to implement it yourself. You will need cumsum and cumcount functions from pandas. . cumsum = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].cumsum() - all_data[&#39;target&#39;] cumcnt = all_data.groupby(&#39;item_id&#39;).cumcount() all_data[&#39;item_target_enc&#39;] = cumsum / cumcnt all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Expanding_mean_scheme&#39;, corr) . 0.502524521108 Current answer for task Expanding_mean_scheme is: 0.502524521108 . Authorization &amp; Submission . To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. Note: Token expires 30 minutes after generation. . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot;TOKEN HERE&quot; # TOKEN HERE grader.status() . You want to submit these numbers: Task KFold_scheme: 0.41645907128 Task Leave-one-out_scheme: 0.480384831129 Task Smoothing_scheme: 0.48181987971 Task Expanding_mean_scheme: 0.502524521108 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/02/Mean-Encodings.html",
            "relUrl": "/2022/04/02/Mean-Encodings.html",
            "date": " • Apr 2, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Josua is a business development analyst who turns into a self-taught Data Scientist. His interests include statistical learning,predictive modeling, network experimentation, and causal inference. He loves running and it teaches him against giving up doing anything, even when implementing the Data Science lifecycle. . Apart from pursuing his passion for data science, he is keen on investing in the Indonesian Stock Exchange and Cryptocurrency. He has been running a full marathon in Jakarta Marathon in 2015 and Osaka Marathon in 2019. His next dreams are to run a marathon in TCS New York City Marathon and Virgin Money London Marathon. . For more, please reach out on LinkedIn! .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}