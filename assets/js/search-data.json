{
  
    
        "post0": {
            "title": "NLP Approach using Word Embedding",
            "content": "This is a minimal yet very powerful approach of NLP problems that you can use on Kaggle competition. I got my attention to the book written by one of Kaggle Grandmaster in his book called Approaching (Almost) Any Machine Learning Problem. I am so interested in how the author approaches the problems for each case in Kaggle Competition started by Supervised and Unsuperviced problems. So, NLP is one of the problems in the competition in Kaggle. We know that many approaches that we can do to preprocess text data from Bag of Words, TFIDF to Word Embedding. I am interested to dive deeper into Word Embedding because this approach yield a better result compared to BOW or Tfidf approach based on this dataset. You can find the comparison by reading this book. I found this approach quite useful for my NLP competition. Before reading this NLP approach, I hope the readers have the fundamentals concepts of pytorch, data visualization libraries and NLP concept to know the code better. . Import Data . import pandas as pd movies = pd.read_csv(&quot;imdb.csv&quot;) movies.head() . review sentiment . 0 One of the other reviewers has mentioned that ... | positive | . 1 A wonderful little production. &lt;br /&gt;&lt;br /&gt;The... | positive | . 2 I thought this was a wonderful way to spend ti... | positive | . 3 Basically there&#39;s a family where a little boy ... | negative | . 4 Petter Mattei&#39;s &quot;Love in the Time of Money&quot; is... | positive | . Check Proportion of target . movies.sentiment.value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . Create Cross Validation . import pandas as pd from sklearn import model_selection if __name__==&quot;__main__&quot;: df = pd.read_csv(&quot;imdb.csv&quot;) df.sentiment = df.sentiment.apply(lambda x: 1 if x == &quot;positive&quot; else 0) df[&quot;kfold&quot;] =-1 df = df.sample(frac=1).reset_index(drop=True) y = df.sentiment.values kf = model_selection.StratifiedKFold(n_splits=5) for f,(t_,v_) in enumerate(kf.split(X=df,y=y)): df.loc[v_,&quot;kfold&quot;] =f df.to_csv(&quot;imdb_folds.csv&quot;,index=False) . movies_folds = pd.read_csv(&quot;imdb_folds.csv&quot;) movies_folds.head() . review sentiment kfold . 0 I enjoyed Erkan &amp; Stefan  a cool and fast sto... | 1 | 0 | . 1 The only reason I rated this film as 2 is beca... | 0 | 0 | . 2 One of those movies where you take bets on who... | 0 | 0 | . 3 This series was just like what you would expec... | 1 | 0 | . 4 While many people found this film simply too s... | 1 | 0 | . There is one additional features called kfold. . Word Embedding . import numpy as np def sentence_to_vec(s,embedding_dict,stop_words,tokenizer): words =str(s).lower() words =tokenizer(words) words = [ w for w in words if w not in stop_words] words = [w for w in words if w.alpha()] M =[] for w in words: if w in embedding_dict: M.append(embedding_dict[w]) if len(M)==0: return np.zeros(300) M = np.array(M) v = M.sum() return v/np.sqrt((v**2).sum()) . Create Dataset in pytorch based on model in our dataset . import torch class IMDBDataset: def __init__(self,reviews,targets): self.reviews =reviews self.targets = targets def __len__(self): return len(self.reviews) def __getitem__(self,item): review =self.reviews[item,:] target =self.target[item] return { &quot;review&quot;: torch.tensor(review,dtype=torch.long), &quot;target&quot;: torch.tensor(target,dtype=torch.float) } . Create Model . import torch.nn as nn class LSTM(nn.Module): def __init__(self,embedding_matrix): super(LSTM,self).__init__() num_words =embedding_matrix.shape[0] embed_dim= embedding_matrix.shape[1] self.embedding = nn.Embedding( num_embeddings = num_words, embedding_dim=embed_dim ) self.embedding.weight = nn.Parameter( torch.tensor( embedding_matrix, dtype=torch.float32 ) ) self.embedding.weight.requires_grad=False self.lstm = nn.LSTM( embed_dim, 128, bidirectional=True, batch_first=True ) self.out = nn.Linear(512,1) def forward(self,x): x = self.embedding(x) x,_ = self.lstm(x) avg_pool =torch.mean(x,1) max_pool, _ = torch.max(x,1) out = torch.cat((avg_pool,maxpool),1) out = self.out(out) return out . Create Training Function for Modelling . def train(data_loader,model,optimizer,device): model.train() for data in data_loader: reviews = data[&quot;review&quot;] targets = data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.float) optimizer.zero_grad() predictions = model(reviews) loss =nn.BCEWithLogitsLoss()( predictions, targets.view(-1,1) ) loss.bakward() optimizer.step() . Create Evaluation for Modelling . def evaluate(data_loader,model,device): final_predictions =[] final_targets = [] model.eval() with torch.no_grad(): for data in data_loader: reviews =data[&quot;review&quot;] targets =data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.long) predictions = model(reviews) predictions = predictions.cpu().numpy().tolist() targets = data[&quot;target&quot;].cpu().numpy.tolist() final_predictions.extend(predictions) final_targets.extend(targets) return final_predictions,final_targets . Word Embedding Creation . import io #from tensorflow.keras import import tensorflow as tf def load_vectors(fname): fin = io.open( fname, &quot;r&quot;, encoding=&quot;utf-8&quot;, newline=&quot; n&quot;, errors=&quot;ignore&quot; ) n,d = map(int,fin.readline().split()) data ={} for line in fin: tokens = line.rstrip().split(&#39; &#39;) data[tokens[0]] = list(map(float,tokens[1:])) return data def create_embedding_matrix(world_index,embedding_dict): embedding_matrix = np.zeros((len(word_index)+1,300)) for word , i in word_index.items(): if word in embedding_dict: embedding_dict[i] = embedding_dict[word] return embedding_matrix def run(df,fold): train_df = df[df.kfold != fold].reset_index(drop=True) valid_df = df[df.kfold ==fold].reset_index(drop=True) print(&quot;Fitting tokenizer&quot;) tokenizer = tf.keras.preprocessing.text.Tokenizer() tokenizer.fit_on_texts(df.review.values.tolist()) xtrain = tokenizer.texts_to_sequences(train_df.review.values) xtest = tokenizer.texts_to_sequences(valid_df.review.values) xtrain = tf.keras.preprocessing.sequence.pad_sequences( xtrain,maxlen=128 ) xtest = tf.keras.preprocessing.sequence.pad_sequences( xtest,maxlen=128 ) train_dataset = IMDBDataset( reviews =xtrain, targets = train_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( train_dataset, batch_size =16, num_workers=2 ) valid_dataset =IMDBDataset( reviews =xtest, targets = valid_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size =8, num_workers=1 ) print(&quot;Loading Embeddings&quot;) # you can suit based on where you put your vec fasttext embedding_dict = load_vectors(&quot;crawl-300d-2M.vec/crawl-300d-2M.vec&quot;) embedding_matrix = create_embedding_matrix( tokenizer.word_index,embedding_dict ) device =torch.device(&quot;cuda&quot;) model =LSTM(embedding_matrix) model.to(device) optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) print(&quot;Training Model&quot;) best_accuracy =0 early_stopping_counter =0 for epoch in range(10): train(train_data_loader,model,optimizer,device) outputs,targets = evaluate(valid_data_loader,model,device) outputs = np.array(outputs) &gt;=0.5 accuracy = metrics.accuracy_score(targets,outputs) print(f&quot;{fold}, Epoch {epoch}, Accuracy Score ={accuracy}&quot;) if accuracy &gt; best_accuracy: best_accuracy = accuracy else: early_stopping_counter +=1 if early_stopping_counter &gt; 2: break if __name__ == &quot;__main__&quot;: df = pd.read_csv(&quot;imdb_folds.csv&quot;) run(df,0) run(df,1) run(df,2) run(df,3) run(df,4) . Fitting tokenizer Loading Embeddings . The choice of Machine learning algorithms will determine the quality of our predictions score. However, The simple model will also determine how fast the training process compared to state of art of ML algorithms. If we have laptop/computer with better GPU can help the training process. But in real life, Simple models with better preprocessing will have predictions score that is not too dfferent with newest ML algorithms. So, It is better to discuss with the stakeholder for improving the models based on business metrics. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/06/approachingnlpusingwordembedding.html",
            "relUrl": "/2022/04/06/approachingnlpusingwordembedding.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Mean encodings",
            "content": "Version 1.1.0 . In this programming assignment you will be working with 1C dataset from the final competition. You are asked to encode item_id in 4 different ways: . 1) Via KFold scheme; 2) Via Leave-one-out scheme; 3) Via smoothing scheme; 4) Via expanding mean scheme. . You will need to submit the correlation coefficient between resulting encoding and target variable up to 4 decimal places. . General tips . Fill NANs in the encoding with 0.3343. | Some encoding schemes depend on sorting order, so in order to avoid confusion, please use the following code snippet to construct the data frame. This snippet also implements mean encoding without regularization. | . import pandas as pd import numpy as np from itertools import product import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) from grader import Grader . Read data . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) . sales.head() . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . sales.shape . (2935849, 6) . Aggregate data . Since the competition task is to make a monthly prediction, we need to aggregate the data to montly level before doing any encodings. The following code-cell serves just that purpose. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales[sales[&#39;date_block_num&#39;]==block_num][&#39;shop_id&#39;].unique() cur_items = sales[sales[&#39;date_block_num&#39;]==block_num][&#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) #turn the grid into pandas dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) #get aggregated values for (shop_id, item_id, month) gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) #fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] #join aggregated data to the grid all_data = pd.merge(grid,gb,how=&#39;left&#39;,on=index_cols).fillna(0) #sort the data all_data.sort_values([&#39;date_block_num&#39;,&#39;shop_id&#39;,&#39;item_id&#39;],inplace=True) . Mean encodings without regularization . After we did the techinical work, we are ready to actually mean encode the desired item_id variable. . Here are two ways to implement mean encoding features without any regularization. You can use this code as a starting point to implement regularized techniques. . Method 1 . item_id_target_mean = all_data.groupby(&#39;item_id&#39;).target.mean() # In our non-regularized case we just *map* the computed means to the `item_id`&#39;s all_data[&#39;item_target_enc&#39;] = all_data[&#39;item_id&#39;].map(item_id_target_mean) # Fill NaNs all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) # Print correlation encoded_feature = all_data[&#39;item_target_enc&#39;].values print(np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1]) . 0.483038698862 . Method 2 . &#39;&#39;&#39; Differently to `.target.mean()` function `transform` will return a dataframe with an index like in `all_data`. Basically this single line of code is equivalent to the first two lines from of Method 1. &#39;&#39;&#39; all_data[&#39;item_target_enc&#39;] = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;mean&#39;) # Fill NaNs all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) # Print correlation encoded_feature = all_data[&#39;item_target_enc&#39;].values print(np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1]) . 0.483038698862 . See the printed value? It is the correlation coefficient between the target variable and your new encoded feature. You need to compute correlation coefficient between the encodings, that you will implement and submit those to coursera. . grader = Grader() . 1. KFold scheme . Explained starting at 41 sec of Regularization video. . Now it&#39;s your turn to write the code! . You may use &#39;Regularization&#39; video as a reference for all further tasks. . First, implement KFold scheme with five folds. Use KFold(5) from sklearn.model_selection. . Split your data in 5 folds with sklearn.model_selection.KFold with shuffle=False argument. | Iterate through folds: use all but the current fold to calculate mean target for each level item_id, and fill the current fold. . See the Method 1 from the example implementation. In particular learn what map and pd.Series.map functions do. They are pretty handy in many situations. | . | from sklearn.model_selection import KFold kf = KFold(n_splits=5,shuffle=False) for tr_ind, val_ind in kf.split(all_data): X_tr, X_val = all_data.iloc[tr_ind], all_data.iloc[val_ind] X_val[&#39;item_target_enc&#39;] = X_val[&#39;item_id&#39;].map(X_tr.groupby(&#39;item_id&#39;).target.mean()) all_data.iloc[val_ind] = X_val all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values # You will need to compute correlation like that corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;KFold_scheme&#39;, corr) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy . 0.41645907128 Current answer for task KFold_scheme is: 0.41645907128 . 2. Leave-one-out scheme . Now, implement leave-one-out scheme. Note that if you just simply set the number of folds to the number of samples and run the code from the KFold scheme, you will probably wait for a very long time. . To implement a faster version, note, that to calculate mean target value using all the objects but one given object, you can: . Calculate sum of the target values using all the objects. | Then subtract the target of the given object and divide the resulting value by n_objects - 1. | Note that you do not need to perform 1. for every object. And 2. can be implemented without any for loop. . It is the most convenient to use .transform function as in Method 2. . target_sum = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;sum&#39;) n_objects = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;count&#39;) all_data[&#39;item_target_enc&#39;] = (target_sum - all_data[&#39;target&#39;]) / (n_objects - 1) all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Leave-one-out_scheme&#39;, corr) . 0.480384831129 Current answer for task Leave-one-out_scheme is: 0.480384831129 . 3. Smoothing . Explained starting at 4:03 of Regularization video. . Next, implement smoothing scheme with $ alpha = 100$. Use the formula from the first slide in the video and $0.3343$ as globalmean. Note that nrows is the number of objects that belong to a certain category (not the number of rows in the dataset). . item_id_target_mean = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;mean&#39;) n_objects = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;count&#39;) all_data[&#39;item_target_enc&#39;] = (item_id_target_mean * n_objects + 0.3343 * 100) / (n_objects + 100) all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Smoothing_scheme&#39;, corr) . 0.48181987971 Current answer for task Smoothing_scheme is: 0.48181987971 . 4. Expanding mean scheme . Explained starting at 5:50 of Regularization video. . Finally, implement the expanding mean scheme. It is basically already implemented for you in the video, but you can challenge yourself and try to implement it yourself. You will need cumsum and cumcount functions from pandas. . cumsum = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].cumsum() - all_data[&#39;target&#39;] cumcnt = all_data.groupby(&#39;item_id&#39;).cumcount() all_data[&#39;item_target_enc&#39;] = cumsum / cumcnt all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Expanding_mean_scheme&#39;, corr) . 0.502524521108 Current answer for task Expanding_mean_scheme is: 0.502524521108 . Authorization &amp; Submission . To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. Note: Token expires 30 minutes after generation. . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot;TOKEN HERE&quot; # TOKEN HERE grader.status() . You want to submit these numbers: Task KFold_scheme: 0.41645907128 Task Leave-one-out_scheme: 0.480384831129 Task Smoothing_scheme: 0.48181987971 Task Expanding_mean_scheme: 0.502524521108 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/02/Mean-Encodings.html",
            "relUrl": "/2022/04/02/Mean-Encodings.html",
            "date": " • Apr 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Ensembling Implementation",
            "content": "Version 1.0.1 . import numpy as np import pandas as pd import sklearn import scipy.sparse import lightgbm for p in [np, pd, scipy, sklearn, lightgbm]: print (p.__name__, p.__version__) . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 lightgbm 2.0.6 . Important! There is a huge chance that the assignment will be impossible to pass if the versions of lighgbm and scikit-learn are wrong. The versions being tested: . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 ligthgbm 2.0.6 . To install an older version of lighgbm you may use the following command: . pip uninstall lightgbm pip install lightgbm==2.0.6 . Ensembling . In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking. . We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what&#39;s happening. . import pandas as pd import numpy as np import gc import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 600) pd.set_option(&#39;display.max_columns&#39;, 50) import lightgbm as lgb from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from tqdm import tqdm_notebook from itertools import product def downcast_dtypes(df): &#39;&#39;&#39; Changes column types in the dataframe: `float64` type to `float32` `int64` type to `int32` &#39;&#39;&#39; # Select columns to downcast float_cols = [c for c in df if df[c].dtype == &quot;float64&quot;] int_cols = [c for c in df if df[c].dtype == &quot;int64&quot;] # Downcast df[float_cols] = df[float_cols].astype(np.float32) df[int_cols] = df[int_cols].astype(np.int32) return df . Load data subset . Let&#39;s load the data from the hard drive first. . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) shops = pd.read_csv(&#39;../readonly/final_project_data/shops.csv&#39;) items = pd.read_csv(&#39;../readonly/final_project_data/items.csv&#39;) item_cats = pd.read_csv(&#39;../readonly/final_project_data/item_categories.csv&#39;) . And use only 3 shops for simplicity. . sales = sales[sales[&#39;shop_id&#39;].isin([26, 27, 28])] . Get a feature matrix . We now need to prepare the features. This part is all implemented for you. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;shop_id&#39;].unique() cur_items = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) # Turn the grid into a dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) # Groupby data to get shop-item-month aggregates gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) # Fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] # Join it to the grid all_data = pd.merge(grid, gb, how=&#39;left&#39;, on=index_cols).fillna(0) # Same as above but with shop-month aggregates gb = sales.groupby([&#39;shop_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_shop&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Same as above but with item-month aggregates gb = sales.groupby([&#39;item_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_item&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1] == &#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;item_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Downcast dtypes from 64 to 32 bit to save memory all_data = downcast_dtypes(all_data) del grid, gb gc.collect(); . /opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs) . After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago. . cols_to_rename = list(all_data.columns.difference(index_cols)) shift_range = [1, 2, 3, 4, 5, 12] for month_shift in tqdm_notebook(shift_range): train_shift = all_data[index_cols + cols_to_rename].copy() train_shift[&#39;date_block_num&#39;] = train_shift[&#39;date_block_num&#39;] + month_shift foo = lambda x: &#39;{}_lag_{}&#39;.format(x, month_shift) if x in cols_to_rename else x train_shift = train_shift.rename(columns=foo) all_data = pd.merge(all_data, train_shift, on=index_cols, how=&#39;left&#39;).fillna(0) del train_shift # Don&#39;t use old data from year 2013 all_data = all_data[all_data[&#39;date_block_num&#39;] &gt;= 12] # List of all lagged features fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] # We will drop these at fitting stage to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + [&#39;date_block_num&#39;] # Category for each item item_category_mapping = items[[&#39;item_id&#39;,&#39;item_category_id&#39;]].drop_duplicates() all_data = pd.merge(all_data, item_category_mapping, how=&#39;left&#39;, on=&#39;item_id&#39;) all_data = downcast_dtypes(all_data) gc.collect(); . . To this end, we&#39;ve created a feature matrix. It is stored in all_data variable. Take a look: . all_data.head() . shop_id item_id date_block_num target target_shop target_item target_lag_1 target_item_lag_1 target_shop_lag_1 target_lag_2 target_item_lag_2 target_shop_lag_2 target_lag_3 target_item_lag_3 target_shop_lag_3 target_lag_4 target_item_lag_4 target_shop_lag_4 target_lag_5 target_item_lag_5 target_shop_lag_5 target_lag_12 target_item_lag_12 target_shop_lag_12 item_category_id . 0 28 | 10994 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 1.0 | 6454.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37 | . 1 28 | 10992 | 12 | 3.0 | 6949.0 | 4.0 | 3.0 | 7.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 37 | . 2 28 | 10991 | 12 | 1.0 | 6949.0 | 5.0 | 1.0 | 3.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 2.0 | 4.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 40 | . 3 28 | 10988 | 12 | 1.0 | 6949.0 | 2.0 | 2.0 | 5.0 | 8499.0 | 4.0 | 5.0 | 6454.0 | 5.0 | 6.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . 4 28 | 11002 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . Train/test split . For a sake of the programming assignment, let&#39;s artificially split the data into train and test. We will treat last month data as the test set. . dates = all_data[&#39;date_block_num&#39;] last_block = dates.max() print(&#39;Test `date_block_num` is %d&#39; % last_block) . Test `date_block_num` is 33 . dates_train = dates[dates &lt; last_block] dates_test = dates[dates == last_block] X_train = all_data.loc[dates &lt; last_block].drop(to_drop_cols, axis=1) X_test = all_data.loc[dates == last_block].drop(to_drop_cols, axis=1) y_train = all_data.loc[dates &lt; last_block, &#39;target&#39;].values y_test = all_data.loc[dates == last_block, &#39;target&#39;].values . First level models . You need to implement a basic stacking scheme. We have a time component here, so we will use scheme f) from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let&#39;s see how we get test meta-features first. . Test meta-features . Firts, we will run linear regression on numeric columns and get predictions for the last month. . lr = LinearRegression() lr.fit(X_train.values, y_train) pred_lr = lr.predict(X_test.values) print(&#39;Test R-squared for linreg is %f&#39; % r2_score(y_test, pred_lr)) . Test R-squared for linreg is 0.743180 . And the we run LightGBM. . lgb_params = { &#39;feature_fraction&#39;: 0.75, &#39;metric&#39;: &#39;rmse&#39;, &#39;nthread&#39;:1, &#39;min_data_in_leaf&#39;: 2**7, &#39;bagging_fraction&#39;: 0.75, &#39;learning_rate&#39;: 0.03, &#39;objective&#39;: &#39;mse&#39;, &#39;bagging_seed&#39;: 2**7, &#39;num_leaves&#39;: 2**7, &#39;bagging_freq&#39;:1, &#39;verbose&#39;:0 } model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100) pred_lgb = model.predict(X_test) print(&#39;Test R-squared for LightGBM is %f&#39; % r2_score(y_test, pred_lgb)) . Test R-squared for LightGBM is 0.738391 . Finally, concatenate test predictions to get test meta-features. . X_test_level2 = np.c_[pred_lr, pred_lgb] . Train meta-features . Now it is your turn to write the code. You need to implement scheme f) from the reading material. Here, we will use duration T equal to month and M=15. . That is, you need to get predictions (meta-features) from linear regression and LightGBM for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models. . dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])] # That is how we get target for the 2nd level dataset y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])] . X_train_level2 = np.zeros([y_train_level2.shape[0], 2]) # Now fill `X_train_level2` with metafeatures for cur_block_num in [27, 28, 29, 30, 31, 32]: print(cur_block_num) &#39;&#39;&#39; 1. Split `X_train` into parts Remember, that corresponding dates are stored in `dates_train` 2. Fit linear regression 3. Fit LightGBM and put predictions 4. Store predictions from 2. and 3. in the right place of `X_train_level2`. You can use `dates_train_level2` for it Make sure the order of the meta-features is the same as in `X_test_level2` &#39;&#39;&#39; # YOUR CODE GOES HERE X_train_meta = all_data.loc[dates &lt; cur_block_num].drop(to_drop_cols, axis=1) X_test_meta = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1) y_train_meta = all_data.loc[dates &lt; cur_block_num, &#39;target&#39;].values y_test_meta = all_data.loc[dates == cur_block_num, &#39;target&#39;].values lr.fit(X_train_meta.values, y_train_meta) X_train_level2[dates_train_level2 == cur_block_num, 0] = lr.predict(X_test_meta.values) model = lgb.train(lgb_params, lgb.Dataset(X_train_meta, label=y_train_meta), 100) X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_meta) # Sanity check assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988, 1.38811989])) . 27 28 29 30 31 32 . Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig scatter plot between the two metafeatures. Plot the scatter plot below. . plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1]) . &lt;matplotlib.collections.PathCollection at 0x7fa38c41ca58&gt; . Ensembling . Now, when the meta-features are created, we can ensemble our first level models. . Simple convex mix . Let&#39;s start with simple linear convex mix: . $$ mix= alpha cdot text{linreg_prediction}+(1- alpha) cdot text{lgb_prediction} $$We need to find an optimal $ alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $ alpha$ out of alphas_to_try array. Remember, that you need to use train meta-features (not test) when searching for $ alpha$. . alphas_to_try = np.linspace(0, 1, 1001) # YOUR CODE GOES HERE r2_scores = np.array([r2_score(y_train_level2, np.dot(X_train_level2, [alpha, 1 - alpha])) for alpha in alphas_to_try]) best_alpha = alphas_to_try[r2_scores.argmax()] # YOUR CODE GOES HERE r2_train_simple_mix = r2_scores.max() # YOUR CODE GOES HERE print(&#39;Best alpha: %f; Corresponding r2 score on train: %f&#39; % (best_alpha, r2_train_simple_mix)) . Best alpha: 0.765000; Corresponding r2 score on train: 0.627255 . Now use the $ alpha$ you&#39;ve found to compute predictions for the test set . test_preds = best_alpha * pred_lr + (1 - best_alpha) * pred_lgb # YOUR CODE GOES HERE r2_test_simple_mix = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Test R-squared for simple mix is %f&#39; % r2_test_simple_mix) . Test R-squared for simple mix is 0.781144 . Stacking . Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above. . lr.fit(X_train_level2, y_train_level2) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) . Compute R-squared on the train and test sets. . train_preds = lr.predict(X_train_level2) # YOUR CODE GOES HERE r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE test_preds = lr.predict(np.vstack((pred_lr, pred_lgb)).T) # YOUR CODE GOES HERE r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Train R-squared for stacking is %f&#39; % r2_train_stacking) print(&#39;Test R-squared for stacking is %f&#39; % r2_test_stacking) . Train R-squared for stacking is 0.632176 Test R-squared for stacking is 0.771297 . Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. Examine and compare train and test scores for the two methods. . And of course this particular case does not mean simple mix is always better than stacking. . We all done! Submit everything we need to the grader now. . from grader import Grader grader = Grader() grader.submit_tag(&#39;best_alpha&#39;, best_alpha) grader.submit_tag(&#39;r2_train_simple_mix&#39;, r2_train_simple_mix) grader.submit_tag(&#39;r2_test_simple_mix&#39;, r2_test_simple_mix) grader.submit_tag(&#39;r2_train_stacking&#39;, r2_train_stacking) grader.submit_tag(&#39;r2_test_stacking&#39;, r2_test_stacking) . Current answer for task best_alpha is: 0.765 Current answer for task r2_train_simple_mix is: 0.627255043446 Current answer for task r2_test_simple_mix is: 0.781144169579 Current answer for task r2_train_stacking is: 0.632175561459 Current answer for task r2_test_stacking is: 0.771297132342 . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot; TOKEN HERE&quot;# TOKEN HERE grader.status() . You want to submit these numbers: Task best_alpha: 0.765 Task r2_train_simple_mix: 0.627255043446 Task r2_test_simple_mix: 0.781144169579 Task r2_train_stacking: 0.632175561459 Task r2_test_stacking: 0.771297132342 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/01/Ensembling_Implementation.html",
            "relUrl": "/2022/04/01/Ensembling_Implementation.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Z-unlock Challenge: Data Visualization",
            "content": "We will Analyze the correlation of temperatures changes on energy use, land cover,waste use and deforestoration by questioning these questions. . What are the areas with biggest/smallest change in temperature? | Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) | How does the seasonal temperature change look like? | How does this vary by continent? Particularly South America? | . # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) . df_temperature = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv&quot;) df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . temp_max = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].max().sort_values(ascending=False).reset_index() temp_min = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].min().sort_values().reset_index() d2 = temp_max[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Max temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . d2 = temp_min[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Min temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . Biggest/smallest change in temperature: . Svalbard and Jan Mayeb Island is the most change in temperature based on the chart above | . Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) . Look at all the possibilities from another dataset/tables | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . land_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv&quot;) land_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2001 | 2001 | 1000 ha | 88.1603 | FC | Calculated data | . 1 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2002 | 2002 | 1000 ha | 88.1818 | FC | Calculated data | . 2 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2003 | 2003 | 1000 ha | 88.2247 | FC | Calculated data | . 3 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2004 | 2004 | 1000 ha | 88.2462 | FC | Calculated data | . 4 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2005 | 2005 | 1000 ha | 88.3106 | FC | Calculated data | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . waste_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv&quot;) waste_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1990 | 1990 | kilotonnes | 0.0 | Fc | Calculated data | . 1 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1991 | 1991 | kilotonnes | 0.0 | Fc | Calculated data | . 2 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1992 | 1992 | kilotonnes | 0.0 | Fc | Calculated data | . 3 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1993 | 1993 | kilotonnes | 0.0 | Fc | Calculated data | . 4 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1994 | 1994 | kilotonnes | 0.0 | Fc | Calculated data | . fires_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv&quot;) fires_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Source Code Source Unit Value Flag Flag Description Note . 0 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1990 | 1990 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 1 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1991 | 1991 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 2 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1992 | 1992 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 3 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1993 | 1993 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 4 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1994 | 1994 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . temp_change= df_temperature.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, hue=&#39;Months&#39;, legend=&#39;full&#39;, data=temp_change, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temp_change.Months.unique()))) max_value_per_year = temp_change.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.title(&quot;The trend for temperature change annually over Months&quot;) plt.axvspan(2015, 2020,alpha=0.15) plt.show() . land_cover= land_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=land_cover, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(land_cover.Year.unique()))) max_value_per_year = land_cover.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Land Cover&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2004, 2006,alpha=0.15) plt.show() . energy_use= energy_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=energy_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(energy_use.Year.unique()))) max_value_per_year = energy_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Energy Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1985, 1989,alpha=0.15) plt.show() . waste_use= waste_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=waste_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(waste_use.Year.unique()))) max_value_per_year = waste_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Waste Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1990, 1993,alpha=0.15) plt.show() . fires_use= fires_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=fires_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(fires_use.Year.unique()))) max_value_per_year = fires_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Fires Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1999, 2003,alpha=0.15) plt.show() . Correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions and Fires.) . Insight Based on Aggregating the mean per year shows correlation among temperature, energy use, land cover, waste use, and fires. All country-Value indicator(Value feature based on each tables) combinations show an increase, but there are subtle differences: . In Land cover use, in 2004-2005, there was a signifant increase followed by a slighly increase in from 2011-2017. | In Energy use, in 1985-1989, there was a signifant increase followed by a slighly increase in from 2019-2020. | In Waste use, in 1999-1993, there was a signifant drop followed by a significant increase from 1994-2020. | In Fires use, in 1990-2003, there was a signifant increase followed by a slighly decrease from 2003-2020. . | Almost everywhere, the end-of-year show an correlation that the the temperature that increase yearly affect the use of waste, energy,deforestoration, and land cover yearly. . | . How does the seasonal temperature change look like? . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . df_temperature.groupby(&quot;Months&quot;)[&quot;Value&quot;].agg([&quot;sum&quot;,&quot;mean&quot;,&quot;max&quot;]) . sum mean max . Months . Dec–Jan–Feb 6113.952 | 0.467428 | 8.206 | . Jun–Jul–Aug 6951.271 | 0.531890 | 4.764 | . Mar–Apr–May 6872.110 | 0.525511 | 5.533 | . Meteorological year 6413.093 | 0.491651 | 5.328 | . Sep–Oct–Nov 5761.315 | 0.441108 | 6.084 | . plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_temperature.groupby([&#39;Months&#39;])): ax = plt.subplot(6, 3, i+1, ymargin=0.5) ax.plot(df.Value) ax.set_title(combi) #if i == 6: break plt.tight_layout(h_pad=3.0) plt.suptitle(&#39;Seasonal Temperature Change&#39;, y=1.03) plt.show() . Seasonal Temperature Change . We can see that on each month has different maximum temperrature. DEC-Jan-Feb has the hottest temperature with 8.206 followed by Sept-Oct-Nov. | . How does this vary by continent? Particularly South America? . south_america_countries =[&#39;Brazil&#39;,&#39;Argentina&#39;,&#39;Chile&#39;,&#39;Colombia&#39;, &#39;Ecuador&#39;,&#39;Venezuela (Bolivarian Republic of)&#39;, &#39;Bolivia (Plurinational State of)&#39;,&#39;Guyana&#39;, &#39;Uruguay&#39;,&#39;Suriname&#39;, &#39;Paraguay&#39;,&#39;Aruba&#39;,&#39;Trinidad and Tobago&#39;] temperature_sa =df_temperature[df_temperature[&quot;Area&quot;].isin(south_america_countries)] temperature_sa.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 2700 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | 0.035 | Fc | Calculated data | . 2701 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | -0.144 | Fc | Calculated data | . 2702 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 0.552 | Fc | Calculated data | . 2703 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | 0.052 | Fc | Calculated data | . 2704 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.034 | Fc | Calculated data | . temperature_sa.groupby([&quot;Area&quot;])[&quot;Value&quot;].agg([&quot;max&quot;,&quot;min&quot;]).plot(kind=&quot;bar&quot;,figsize=(12,8)) plt.ylabel(&quot;Temperature&quot;) . Text(0, 0.5, &#39;Temperature&#39;) . How about Meterological season temperature changes in South America? . temperature_sa= temperature_sa.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, hue=&#39;Months&#39;, data=temperature_sa, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temperature_sa.Months.unique()))) max_value_per_year = temperature_sa.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature Change&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2013, 2016,alpha=0.15) plt.show() . Ultimately, there is an uptrend for temperature change in South America annually in which the peak is around 2013-2016. | . For joining this competition, see Z-Unlocked_Challenge1. There is a chance to visit Barcelona for Kaggle Competition. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/03/30/data-visualization-challenge.html",
            "relUrl": "/2022/03/30/data-visualization-challenge.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Model Design using Pytorch",
            "content": "import torch.nn as nn import torch.nn.functional as F import torch from torch import optim . class SimpleNet(nn.Module): ## created layers as classattributes def __init__(self): ## call the base class to initialize params super(SimpleNet,self).__init__() self.fc1 = nn.Linear(2048,256) self.fc2 = nn.Linear(256,64) self.fc3 = nn.Linear(64,2) ## Required to define how the model process the parameters def forward(self,x): x = x.view(-1,2048) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.softmax(self.fc3(x),dim=1) return x . simplenet = SimpleNet() print(simplenet) . SimpleNet( (fc1): Linear(in_features=2048, out_features=256, bias=True) (fc2): Linear(in_features=256, out_features=64, bias=True) (fc3): Linear(in_features=64, out_features=2, bias=True) ) . nn.Module also supports for CNN, Dropout, and BatchNomarlization to implement in our model. . Training Loop . implemented training loop using LeNet5 model . class LeNet5(nn.Module): def __init__(self): super(LeNet5,self).__init__() self.conv1 = nn.Conv2d(3,6,5) self.conv2 = nn.Conv2d(6,16,5) self.fc1 = nn.Linear(16*5*5,120) self.fc2 = nn.Linear(120,84) self.fc3 = nn.Linear(84,10) def forward(self,x): x = F.max_pool2d(F.relu(self.conv1(x)),(2,2)) x = F.max_pool2d(F.relu(self.conv2(x)),2) x = x.view(-1,int(x.nelement()/x.shape[0])) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x device = (&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) ##move to GPU model = LeNet5().to(device=device) . from torchvision.datasets import CIFAR10 train_data = CIFAR10(root=&quot;./train/&quot;,train=True,download=True) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./train/cifar-10-python.tar.gz Extracting ./train/cifar-10-python.tar.gz to ./train/ . test_data = CIFAR10(root=&quot;./test/&quot;,train=False,download=True) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./test/cifar-10-python.tar.gz Extracting ./test/cifar-10-python.tar.gz to ./test/ . Transform the data . from torchvision import transforms train_transforms = transforms.Compose([ transforms.RandomCrop(32,padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize( mean = (0.4914,0.4822,0.4465), std = (0.2023,0.1994,0.2010))]) train_data = CIFAR10(root=&quot;./train/&quot;,train=True, download=True,transform=train_transforms) . Files already downloaded and verified . from torchvision import transforms test_transforms = transforms.Compose([ transforms.RandomCrop(32,padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize( mean = (0.4914,0.4822,0.4465), std = (0.2023,0.1994,0.2010))]) test_data = CIFAR10(root=&quot;./test/&quot;,train=False, download=True,transform=test_transforms) . Files already downloaded and verified . Data Batching using DataLoader . trainloader =torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True) . testloader =torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False) . criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(),lr=1e-2,momentum=0.9) N_EPOCHS = 10 for epoch in range(1,N_EPOCHS+1): epoch_loss = 0.0 for inputs,labels in trainloader: inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs,labels) loss.backward() optimizer.step() epoch_loss +=loss.item() print(f&quot;Epoch {epoch} Loss {epoch_loss/len(trainloader)}&quot;) . Epoch 1 Loss 1.8241477269363404 Epoch 2 Loss 1.6615231191253663 Epoch 3 Loss 1.6304866078948974 Epoch 4 Loss 1.6164952528762817 Epoch 5 Loss 1.5978561991500855 Epoch 6 Loss 1.5923947086524963 Epoch 7 Loss 1.5816101968955993 Epoch 8 Loss 1.591676569519043 Epoch 9 Loss 1.5898757279396056 Epoch 10 Loss 1.5873737773704528 . Validation . from torch.utils.data import random_split train_set,val_set = random_split(train_data,[40000,10000]) trainloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True) valloader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True) print(len(trainloader),len(valloader)) . 2500 625 . model = LeNet5().to(device) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(),lr=1e-2,momentum=0.9) . N_EPOCHS = 10 for epoch in range(1,N_EPOCHS+1): train_loss = 0.0 model.train() for inputs,labels in trainloader: inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs,labels) loss.backward() optimizer.step() epoch_loss +=loss.item() val_loss =0.0 model.eval() for inputs,labels in valloader: inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) loss = criterion(outputs,labels) val_loss +=loss.item() print(f&quot;Epoch {epoch} Train Loss {epoch_loss/len(trainloader)} Val loss {val_loss/len(valloader)}&quot;) . Epoch 1 Train Loss 3.8346362232685087 Val loss 1.6857107292175293 Epoch 2 Train Loss 5.50487792634964 Val loss 1.6504140425682068 Epoch 3 Train Loss 7.115121179986 Val loss 1.5639411679267883 Epoch 4 Train Loss 8.702473026013374 Val loss 1.5383020911216736 Epoch 5 Train Loss 10.265977776813507 Val loss 1.5413660417556763 Epoch 6 Train Loss 11.820966996860504 Val loss 1.5524281386375427 Epoch 7 Train Loss 13.36783570830822 Val loss 1.6050877237319947 Epoch 8 Train Loss 14.901475507044792 Val loss 1.6089960625648498 Epoch 9 Train Loss 16.44660836327076 Val loss 1.573909682750702 Epoch 10 Train Loss 17.996182167482377 Val loss 1.588604866695404 . Validation occurs at every epoch after the training has been processed . During validation, the model is passed data which has not seen before. Only forward pass during validation. . Testing . num_correct = 0.0 for x_test_batch,y_test_batch in testloader: model.eval() y_test_batch = y_test_batch.to(device) x_test_batch = x_test_batch.to(device) y_pred_batch = model(x_test_batch) _,predicted = torch.max(y_pred_batch,1) num_correct +=(predicted==y_test_batch).float().sum() accuracy =num_correct / (len(testloader)*testloader.batch_size) print(f&quot;Test Accuracy {accuracy}&quot;) . Test Accuracy 0.44679999351501465 . Saving Models . torch.save(model.state_dict(),&quot;./lenet5_model.pt&quot;) model = LeNet5().to(device) model.load_state_dict(torch.load(&quot;./lenet5_model.pt&quot;)) . &lt;All keys matched successfully&gt; .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/03/29/NN-for-new-user-of-Pytorch.html",
            "relUrl": "/2022/03/29/NN-for-new-user-of-Pytorch.html",
            "date": " • Mar 29, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Josua is a business development analyst who turns into a self-taught Data Scientist. His interests include statistical learning,predictive modeling, network experimentation, and causal inference. He loves running and it teaches him against giving up doing anything, even when implementing the Data Science lifecycle. . Apart from pursuing his passion for data science, he is keen on investing in the Indonesian Stock Exchange and Cryptocurrency. He has been running a full marathon in Jakarta Marathon in 2015 and Osaka Marathon in 2019. His next dreams are to run a marathon in TCS New York City Marathon and Virgin Money London Marathon. . For more, please reach out on LinkedIn! . View my certificates in Data Science . Books I am currently reading . Here is a snapshot of a (small) subset of all of my Coding, Data Science and Machine Learning books. This collection would get you close to 98%-99% of all the necessary core skills to be a good Data Scientists. 1/6 pic.twitter.com/c1M4wpEXWt . &mdash; Bojan Tunguz (@tunguz) March 23, 2022",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}