{
  
    
        "post0": {
            "title": "Part 1: Building Deep Learning Models by using Keras and Tensorflow",
            "content": "Introduction . Some random notes by reading Deep Learning with Python by Francois Chollet. This book focusses on developing Deep Learning Models using Keras and Tensorflow. I found this book is very good at explaining Deep learning.I try making notes for important chapters that i think it is essential for improving myself in developing deep learning models. I took some source code from the book as a reference. . Different ways of building models in Keras . Sequential model where the the layers are stacked each other to create the DL architecture. | The functional API where focussing on graph-like model architecture that represents a nice usabilibity and flexibility. | Model Subclassing where a low level option to write model from scratch. | . Sequential Models . from tensorflow import keras from tensorflow.keras import layers model =keras.Sequential([ layers.Dense(64,activation=&quot;relu), layers.Dense(10,activation=&quot;softmax&quot;) ]) . or . model = keras.Sequential() model.add(layers.Dense(64,activation=&quot;relu)) model.add(layers.Dense(10,activation=&quot;softmax&quot;)) . We see that sequential model create the 64 multidimensional representation based on input data to create 10 output value using softmax activation function to get predicted probabilities. You can check model.summary() for looking at the architecture of your model. You can also name the models and layers by dding name parameters in each process in your architecture by using string. . model = keras.Sequential(name=&quot;my_example_model&quot;) model.add(layers.Dense(64,activation=&quot;relu),name=&quot;my_first_layer&quot;) model.add(layers.Dense(10,activation=&quot;softmax&quot;,name=&quot;my_last_layer&quot;)) . Functional API . While sequential model constainst are for simple model that can express models with single input and single output where applying one layer after the other sequentially. There is a situation when we face a problem to encounter models with multiple inputs like image and its metadata and multiple output to predict different things about data. Imagine you are building a system to rank customer support tickets by priority and route them by departments based on 3 inputs . The title of the ticket(text input) | The text body of the ticket(text input) | Any tags added by the user(categorical input encode to one-hot encing) and create 2 outputs | The priority score of the ticket , a scalr between 0-1(Sigmoid ouput) | The deparment that should handle the ticket (Softax over the set of departments) | . vocabulary_size =10000 num_tags =100 num_departments =4 # Define model inputs title =keras,Input(shape=(vocabulary_size,),name=&quot;title&quot;) text_body = keras.Input(shape=(vocabulary_size,),name=&quot;text_body&quot;) tags = keras.Input(shape=(num_tags),name=&quot;tags&quot;) #Combine input features into a single tensor by concatenating them features = layers.Concatenate()([title,text_body,tags]) features =layers.Dense(64,activation=&quot;relu&quot;)(features) # Define Model outputs priority = layers.Dense(1,activation=&quot;sigmoid&quot;,names=&quot;priority&quot;)(features) department = layers.Dense(num_departments,activation=&quot;softmax&quot;,name=&quot;departments&quot;)(features) model = keras.Model(inputs=[title,text_body,tags], outputs=[priority,department]) . You can see the structure of your model in a nice graph by plotting the moarchitecture by using keras.utils.plt_model(instances of model object).The advantage of using functional API is enabling us to do feature extractions that reuse intermediate features from another model. If we want to add another output to the precious model by estimating how long a given issue ticket will take to ressolve by difficulty rating(quick,medium,difficult). we dont need to recreate the model from scratch. We can start fro intermediate features of previous model . features = model.layers[4].output difficulty = layers.Dense(3,activation =&quot;softmax&quot;,name=&quot;difficulty&quot;)(features) new_model = keras.Model(inputs =[title,text_body,tags], outputs=[priority,department,difficulty]) keras.utils.plot_model(new_model,&quot;updated_ticket_classifier.png&quot;,show_shapes=True) . Subclassing Model class . This is the advanced of building model pattern by subclassing Layer class to create custom layers. This is like subclassing nn.Model in pytorch(if you experienced in pytorch) . init for defining the layers the model will use | call() for defining the forward pass of the model by reusing previously layers created. | . class CustomerTicketModel(keras.Model): def __init__(self,num_departments): super.__init__() self.concat_layer = layers.Concatenate() self.mixing_layer = layers.Dense(64,activation=&quot;relu&quot;) self.priority_scorer = layers.Dense(1,activation=&quot;sigmoid&quot;) self.departments_classifier = layers.Dense(num_departments,activation=&quot;sofmax&quot;) def call(self,inputs): title = inputs[&quot;title&quot;] text_body = inputs[&quot;text_body&quot;] tags = inputs[&quot;tags&quot;] features =self.concat_layer[title,text_body,tags] features =self.mixing_layer(features) priority =self.priority_scorer)(features) department = self.department_classifier(features) return priority,department model =CustomerTicketModel(num_departments=4) priority,department =model({&quot;title&quot;:title_data,&quot;text_body&quot;:text_body_data,&quot;tags&quot;:ttags_data}) ## Do compile(),fit(),evalueate and predict() as usual. . Writing own callbacks (loss,EarlyStopping,ModelCheckPoint) . There are a few methods available in keras.callbacks.Callback class . on_epoch_begin(epoch,logs) for calling at the start of every epoch | on_epoch_end(epoch,logs) for calling at the end of every epoch | on_batch_begin(batch,logs) for calling tight before processing each batch | on_batch_end(batch,logs) for calling right after processing each batch | on_train_begin(logs) for calling at the start of training | on_train_end(logs) for calling at the end of training | . from matplotlib import pyplot as plt ## Transform Training fit() ModelCheckPoint and EarlyStopping python from tensorflow.keras.datasets import mnist def get_mnist_model(): inputs = keras.Input(shape=(28*28,)) features = layers.Dense(512,activation=&quot;relu&quot;)(inputs) features =layers.Dropout(0.5)(features) outputs =layers.Dense(10,activation=&quot;softmax&quot;)(features) model =keras.Model(inputs,outputs) return model (images,labels),(test_images,test_labels) =mnist.load_data() images = images.reshape((60000,28*28)).astype(&quot;float32&quot;)/255 test_images =test_images.reshape((10000,28*28)).astype(&quot;float32&quot;)/255 train_images,val_images = images[10000:],images[:10000] train_labels,val_labels = labels[10000:],labels[:10000] model = get_mnist_model() model.compile(optimizer=&quot;rmsprop&quot;,loss=&quot;sparse_categorical_crossentropy&quot;, metrics =[&quot;accuracy&quot;]) model.fit(train_images,train_labels,epoch=3,validation_data=[test_images,test_labels]) test_metrics =model.evaluate(test_images,val_label) predictions =model.predict(test_images) callbacks_list =[ keras.callbacks.EarlyStopping( monitor=&quot;val_accuracy&quot;, patience=2,), keras.callbacks.ModelCheckPoint( filepath=&quot;checkpoint_path.keras&quot;, monitor =&quot;val_loss&quot;, save_best_only=True ) ] model = get_mnist_model() model.compile(optimizer=&quot;rmsprop&quot;, loss =&quot;sparse_categorical_crossentropy&quot;, metrics =[&quot;accuracy&quot;]) model.fit(train_images,train_labels,epochs=10,calbacks=callbacks_list, validation_data=[val_images,val_labels]) #Load the model model = keras.models.load_model(&quot;checkpoint_path.keras&quot;) class lossHistory(keras.callbacks.Callback): def on_train_begin(self_logs): self.per_batch_losses=[] def on_batch_end(self,batch,logs): self.per_batch_losses.append(logs.get(&quot;loss&quot;)) def on_epoch_end(self,epoch,logs): plt.clf() plt.plot(range(len(self.per_batch_losses)),self.per_batch_losses,label=&quot;Training loss for each batch&quot;) plt.xlabel(f&quot;Batch (epoch{epoch})&quot;) self.per_batch_losses =[] model = model_get_mnist_model() model.compile(optimizer=&quot;rmsprop&quot;, loss =&quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) model.fit(train_images,train_labels,epochs=10,callbacks=[LossHistory()], validation_data=[val_images,val_labels]) . A complete training and evaluation loop from scratch(fit() and evaluate() method) . # Training Process model = get_mnist_model() loss_fn =keras.losses.SparseCategoricalCrossentropy() optimizer =keras.optimizers.RMSprop() metrics = [keras.metrics.SparseCategoricalAccuracy()] loss_tracking_metrics =keras.metrics.Mean() def train_step(inputs,targets): with tf.GradientTape() as tape: # training=True during training predictions =model(inputs,training=True) loss =loss_fn(targets,predictions) gradients = tape.gradients(loss,,model.trainable_weights) optimizer.apply_gradients(zip(gradients,model.trainable_weights)) logs ={} for metric in metrics: metric.update_state(targets,predictions) logs[metric.name] =metric.result() loss_tracking_metric.update_state(loss) logs[&quot;loss&quot;] = loss_tracking_metric.result() return logs def reset_metrics(): for metric in metrics: metric.reset_state() loss_tracking_metric.reset_state() training_dataset =tf.data.Dataset.from_tensor_slices((train_images,train_labels)) training_dataset = training_dataset.batch(32) epochs =3 for epoch in range(epochs): reset_metrics() for input_batch,targets_batch in training_dataset: logs =train_step(input_batchs,targets_batch) print(f&quot;Results at the end of epoch {epoch}&quot;) for key,value in logs.items(): print(f&quot;....{key}: {value:.4f}&quot;) # Evaluation loop def test_step(inputs,targets): #training=False for evaluation predictions= model(inputs,training=False) loss =loss_fn(targets,predictions) logs ={} for metric in metrics: metric.update_state(targets,predictions) logs[&quot;val&quot; +metric.name] = metric.result() loss_tracking_metric.update_state(loss) logs[&quot;val_loss&quot;] = loss_tracking_metrics.result() return logs val_dataset = tf.data.Dataset.from_tesor_slices((val_images,val_labels)) val_dataset =val_dataset.batch(32) reset_metrics() for inputs_batch,targets_batch in val_dataset: logs = test_step(inputs_batch,targets_batch) print(&quot;Evaluation results.&quot;) for key,value in logs.items(): print(f&quot;...{key} : {value:.4f}&quot;) .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/tensorflow/keras/2022/05/31/tensofrflow-keras.html",
            "relUrl": "/tensorflow/keras/2022/05/31/tensofrflow-keras.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "UW-Madison GI Tract Image Segmentation",
            "content": "This is a comprehensive End-to End Deep Learning Models Implementation using Keras to do image segmentation based on one of Kaggle Competition provided by Wisconsin Carbone Cancer Center. I am interested in this competition to improve my skills related to the implemetation of AI in Health Sector. I will explain my approach about the dataset provided by the organizer. I also explain the some metrics which are available for evaluating the success of Image Segmentation process in MRI scan dataset. I used KERAS and some pretrained segmentation_models using Efficientbnet7 for running the training process and do the evaluation based on Dice coefficient, IoU coefficient and the loss for this problem statement. . Introduction . In 2019, There are about 5 million people were diagnosed with cancer of the gastro-intestinal tract worldwide. Half of these patients are eligible for radiation therapy by delivering 10-15 minutes a day for 1-6 weeks. The radiation system works by detecting parts of the tumor from the patient&#39;s body by giving high doses of radiation using X-ray beams to part of the body that consists of tumors while avoiding the stomach and intestines in order to avoid the spread of the tumor. Recent technology called MR-Linacs is only able to visualize the daily position of tumors and intestines which can vary day by day. However, The radiation oncologists must manually outline the position of the stomach and intestines so that they can manage the direction of the x-ray beams to the tumor and avoid the stomach and intestines which is labor-intensive and takes time that can prolong treatments from 15 minutes a day to an hour a day. The problem can be difficult for patients to tolerate to wait for the result about the recent condition of their health. Deep Learning, as a subset of Machine Learning through Computer Vision, can detect the segmentation process to distinguish parts of the tumor, stomach, and intestines. This is going to be a game-changer for patients and radiation oncologists to accelerate the treatment of patients in order to detect the condition of the patients to have more effective treatment. I will try solving this problem by using keras as deep learning framework library and do an inference based on the model built using U-Net Architecture for image segmentation by using dataset provided by the UW-Madison Carbone Cancer Center. . from IPython.display import YouTubeVideo # Full Link: https://www.youtube.com/watch?v=knUTrvJLeEg YouTubeVideo(&#39;knUTrvJLeEg&#39;, width=700, height=400) . The Dataset . Stomach, Large Bowel, Small Bowel . The class inside the train.csv file has 3 distinct values: large bowel, small bowel, stomach. These are all part of the digestive system. The bowels (small and large intestine) are responsible for breaking down food and absorbing the nutrients. . Import Libraries . import warnings warnings.filterwarnings(&quot;ignore&quot;) import pandas as pd import numpy as np import os import cv2 import gc from tqdm import tqdm from datetime import datetime from typing import Optional from glob import glob from PIL import Image import matplotlib.pyplot as plt from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold from tensorflow import keras import tensorflow as tf import keras from keras.models import load_model, save_model from keras.layers.convolutional import Conv2D, Conv2DTranspose from keras.layers.pooling import MaxPooling2D from keras.layers.merge import concatenate from keras.losses import binary_crossentropy from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping from keras import backend as K #from tensorflow.python.keras import backend as K from keras.models import Model from keras.layers import Input import matplotlib.gridspec as gridspec import matplotlib.patches as mpatches import matplotlib as mpl . BATCH_SIZE = 16 EPOCH = 10 n_splits = 5 fold_selected = 2 # 1,...,5 TRAIN_ROOT_DIR = &quot;../input/uw-madison-gi-tract-image-segmentation/&quot; TEST_ROOT_DIR = &quot;../input/uw-madison-gi-tract-image-segmentation/test&quot; . train_df_original = pd.read_csv(TRAIN_ROOT_DIR + &#39;train.csv&#39;) print(train_df_original.shape) train_df_original.sample(30) . print(f&quot;There are about { round(train_df_original[&#39;segmentation&#39;].isnull().sum()/train_df_original.shape[0],2) }% of pictures without segmentation which is quite a lot&quot;) . We can see that the segmentation images taken from MRI scanners shows an imbalanced data among pictures that shows indices segmentation and not having indices segmentation(NaN). This is not good for training the data without paying attention to imbalanced Dataset. We use StratifieldKFold validation to compensate the balance of this data. . plt.figure(figsize=(28, 12)) train_df_original[&#39;class&#39;].value_counts(normalize=True).plot.pie() . test_df = pd.read_csv(TRAIN_ROOT_DIR + &#39;sample_submission.csv&#39;) test_df.head() . if len(test_df) == 0: DEBUG=True # test_df=train_df_original.iloc[:300, :] test_df=pd.read_csv(TRAIN_ROOT_DIR + &#39;train.csv&#39;).iloc[:300, :] test_df[&#39;segmentation&#39;] = &#39;&#39; test_df = test_df.rename(columns={&#39;segmentation&#39; : &#39;prediction&#39;}) else: DEBUG=False submission = test_df.copy() test_df.head() . train_df_original.head() train_df_original.shape . def df_preparation(df, subset=&quot;train&quot;, DEBUG=False): df[&quot;case&quot;] = df[&quot;id&quot;].apply(lambda x: int(x.split(&quot;_&quot;)[0].replace(&quot;case&quot;, &quot;&quot;))) df[&quot;day&quot;] = df[&quot;id&quot;].apply(lambda x: int(x.split(&quot;_&quot;)[1].replace(&quot;day&quot;, &quot;&quot;))) df[&quot;slice&quot;] = df[&quot;id&quot;].apply(lambda x: x.split(&quot;_&quot;)[3]) if (subset == &quot;train&quot;) or (DEBUG): DIR = TRAIN_ROOT_DIR + &quot;train&quot; else: DIR = TEST_ROOT_DIR &quot;&quot;&quot;Also another cool feature of `glob.glob`, if you want to avoid `/*/*/*` type of pattern( as not all datasets follows certain pattern hence we might not be able find how many `/` to use) you can use, glob.glob(`/kaggle/input/uw-madison-gi-tract-image-segmentation/train/**/*png&#39;, recursive=True) &quot;&quot;&quot; all_images = glob(os.path.join(DIR, &quot;**&quot;, &quot;*.png&quot;), recursive=True) print(&quot;all_images length &quot;, len(all_images)) # 38496 x = all_images[0].rsplit(&quot;/&quot;, 4)[0] # print(&#39;x &#39;, x) # ../../input/uw-madison-gi-tract-image-segmentation/train # Now I need a column named &#39;path&#39; holding the full path of all the images in this dataframe # But I can not simply create them with the below kind of line # df[&#39;path&#39;] = all_images # Because each image is repeated. And so if I do the above line directly I will get below error # ValueError: Length of values (38496) does not match length of index (115488) # So the solution is to create a temporary dataframe &gt; then merge this temp dataframe with the original df &gt; then delete the temp df # To make a column which will have th full pathname of all the iamges # Hence I have to build the full path name. Below is an example. # &#39;../../input/uw-madison-gi-tract-image-segmentation/train/case44/case44_day0/scans/slice_0085_266_266_1.50_1.50.png&#39;, path_partial_list = [] for i in range(0, df.shape[0]): path_partial_list.append( os.path.join( x, &quot;case&quot; + str(df[&quot;case&quot;].values[i]), &quot;case&quot; + str(df[&quot;case&quot;].values[i]) + &quot;_&quot; + &quot;day&quot; + str(df[&quot;day&quot;].values[i]), &quot;scans&quot;, &quot;slice_&quot; + str(df[&quot;slice&quot;].values[i]), ) ) df[&quot;path_partial&quot;] = path_partial_list path_partial_list = [] for i in range(0, len(all_images)): path_partial_list.append(str(all_images[i].rsplit(&quot;_&quot;, 4)[0])) tmp_df = pd.DataFrame() tmp_df[&quot;path_partial&quot;] = path_partial_list tmp_df[&quot;path&quot;] = all_images df = df.merge(tmp_df, on=&quot;path_partial&quot;).drop(columns=[&quot;path_partial&quot;]) df[&quot;width&quot;] = df[&quot;path&quot;].apply(lambda x: int(x[:-4].rsplit(&quot;_&quot;, 4)[1])) df[&quot;height&quot;] = df[&quot;path&quot;].apply(lambda x: int(x[:-4].rsplit(&quot;_&quot;, 4)[2])) del x, path_partial_list, tmp_df return df . train_df = df_preparation(train_df_original, subset=&quot;train&quot;) train_df.head(10) . print(train_df[&#39;path&#39;][0]) print(train_df[&#39;path&#39;][1]) . test_df=df_preparation(test_df, subset=&quot;test&quot;, DEBUG=True) test_df.head() . def df_rearrange_for_3_segmentation_classes(df, subset=&quot;train&quot;): df_restructured = pd.DataFrame({&quot;id&quot;: df[&quot;id&quot;][::3]}) if subset == &quot;train&quot;: df_restructured[&quot;large_bowel&quot;] = df[&quot;segmentation&quot;][::3].values df_restructured[&quot;small_bowel&quot;] = df[&quot;segmentation&quot;][1::3].values df_restructured[&quot;stomach&quot;] = df[&quot;segmentation&quot;][2::3].values df_restructured[&quot;path&quot;] = df[&quot;path&quot;][::3].values df_restructured[&quot;case&quot;] = df[&quot;case&quot;][::3].values df_restructured[&quot;day&quot;] = df[&quot;day&quot;][::3].values df_restructured[&quot;slice&quot;] = df[&quot;slice&quot;][::3].values df_restructured[&quot;width&quot;] = df[&quot;width&quot;][::3].values df_restructured[&quot;height&quot;] = df[&quot;height&quot;][::3].values df_restructured = df_restructured.reset_index(drop=True) df_restructured = df_restructured.fillna(&quot;&quot;) if subset == &quot;train&quot;: df_restructured[&quot;count&quot;] = np.sum( df_restructured.iloc[:, 1:4] != &quot;&quot;, axis=1 ).values return df_restructured . train_df_rearranged=df_rearrange_for_3_segmentation_classes(train_df, subset=&quot;train&quot;) train_df_rearranged.head(100) . train_df_rearranged = train_df_rearranged[(train_df[&#39;case&#39;]!=7)|(train_df[&#39;day&#39;]!=0)].reset_index(drop=True) train_df_rearranged = train_df_rearranged[(train_df[&#39;case&#39;]!=81)|(train_df[&#39;day&#39;]!=30)].reset_index(drop=True) train_df_rearranged = train_df_rearranged[(train_df[&#39;case&#39;]!=138)|(train_df[&#39;day&#39;]!=00)].reset_index(drop=True) . gc.collect() . def plot_bar(df): plt.figure(figsize=(12, 6)) bar = plt.bar([1, 2, 3], 100 * np.mean(df.iloc[:, 1:4] != &quot;&quot;, axis=0)) plt.title(&quot;Percent Training Images with Mask&quot;, fontsize=16) plt.ylabel(&quot;Percent of Train images with mask&quot;) plt.xlabel(&quot;Class Types&quot;) # labels = [&quot;large bowel&quot;, &quot;small bowel&quot;, &quot;stomach&quot;] labels = [&quot;large_bowel&quot;, &quot;small_bowel&quot;, &quot;stomach&quot;] for rect, lbl in zip(bar, labels): height = rect.get_height() plt.text( rect.get_x() + rect.get_width() / 3, height, lbl, ha=&quot;center&quot;, va=&quot;bottom&quot;, fontsize=12, ) plt.ylim((0, 50)) plt.show() plot_bar(train_df_rearranged) . def rle_decode(mask_rle, shape, color=1): s = mask_rle.split() starts, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])] starts -= 1 ends = starts + length img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32) for lo, hi in zip(starts, ends): img[lo:hi] = color return img.reshape(shape) class DataGenerator(tf.keras.utils.Sequence): def __init__(self, df, batch_size=BATCH_SIZE, subset=&quot;train&quot;, shuffle=False): super().__init__() self.df = df self.shuffle = shuffle self.subset = subset self.batch_size = batch_size self.indexes = np.arange(len(df)) self.on_epoch_end() def __len__(self): return int(np.floor(len(self.df) / self.batch_size)) def on_epoch_end(self): if self.shuffle == True: np.random.shuffle(self.indexes) &quot;&quot;&quot; __getitem__ returns a batch of images and masks &quot;&quot;&quot; def __getitem__(self, index): X = np.empty((self.batch_size, 224, 224, 3)) # Makes a 4-D Tensor y = np.empty((self.batch_size, 224, 224, 3)) # Makes a 4-D Tensor indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size] for i, img_path in enumerate(self.df[&quot;path&quot;].iloc[indexes]): # print(&quot;df[&#39;path&#39;].iloc[indexes].shape &quot;, self.df[&#39;path&#39;].iloc[indexes].shape) # (16,) # in above &#39;i&#39; is just the counter. i.e. starts from 0 and goes upto the max length of all the rows w = self.df[&quot;width&quot;].iloc[ indexes[i] ] # selects the row number of indexes[i] h = self.df[&quot;height&quot;].iloc[indexes[i]] img = self._load_grayscaled_img(img_path) # shape: (128,128,1) # print(&#39;img shape after _load_grayscaled_img &#39;, img.shape) #(128, 128, 1) # Now update X[i,] to be this image. X[ i, ] = img # broadcast to shape: (128,128,3) # As we know, that arr[1,] is equivalent to arr[1, :] # As NumPy will automatically insert trailing slices for you # print(&#39;X after &#39;, X.shape) # (16, 128, 128, 3) # The slice notation in the above line means - # Set me the (i+1)th Row of X to be this image if self.subset == &quot;train&quot;: for k, j in enumerate([&quot;large_bowel&quot;, &quot;small_bowel&quot;, &quot;stomach&quot;]): # Now &#39;j&#39; will take each value from the above list # e.g. self.df[&#39;large_bowel&#39;] # and in my train_df_rearranged each of the [&quot;large_bowel&quot;,&quot;small_bowel&quot;,&quot;stomach&quot;] # column names contain RLE formatted segmentation data. rles = self.df[j].iloc[indexes[i]] # so the above line will actually be something like =&gt; self.df[&#39;stomach&#39;].iloc[indexes[20]] # giving me the RLE data for that row and column # mask = rle_decode(rles, shape=(h, w, 1)) # if all my utils method is in separate file then uncomment below mask = rle_decode(rles, shape=(h, w, 1)) mask = cv2.resize(mask, (224, 224)) y[i, :, :, k] = mask if self.subset == &quot;train&quot;: return X, y else: return X def _load_grayscaled_img(self, img_path): img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH) img_size = (224, 224) img = cv2.resize(img, img_size) img = img.astype(np.float32) / 255.0 img = np.expand_dims(img, axis=-1) return img &quot;&quot;&quot;cv2.IMREAD_ANYDEPTH =&gt; If set, return 16-bit/32-bit image when the input has the corresponding depth, otherwise convert it to 8-bit. &quot;&quot;&quot; . def plot_mask_with_color_patches(df, colors, labels): list_indices_of_mask_random = list( df[df[&quot;large_bowel&quot;] != &quot;&quot;].sample(BATCH_SIZE).index ) list_indices_of_mask_random += list( df[df[&quot;small_bowel&quot;] != &quot;&quot;].sample(BATCH_SIZE * 2).index ) list_indices_of_mask_random += list( df[df[&quot;stomach&quot;] != &quot;&quot;].sample(BATCH_SIZE * 3).index ) # print(&#39;list_indices_of_mask_random &#39;, list_indices_of_mask_random) # It will be a list of indexes like [15176, 13709, 30423, ..., 12730] batches_from_datagen = DataGenerator( df[df.index.isin(list_indices_of_mask_random)], shuffle=True ) num_rows = 6 fig = plt.figure(figsize=(10, 25)) gs = gridspec.GridSpec(nrows=num_rows, ncols=2) patches = [ mpatches.Patch(color=colors[i], label=f&quot;{labels[i]}&quot;) for i in range(len(labels)) ] cmap1 = mpl.colors.ListedColormap(colors[0]) cmap2 = mpl.colors.ListedColormap(colors[1]) cmap3 = mpl.colors.ListedColormap(colors[2]) &quot;&quot;&quot; The `matplotlib.colors.ListedColormap` class is used to create colarmap objects from a list of colors. The class belongs to the `matplotlib.colors` module. This module is used for converting color or numbers arguments to RGBA or RGB and for mapping numbers to colors or color specification conversion in a 1-D array of colors also known as colormap. This can be useful for directly indexing into colormap and it can also be used to create special colormaps for normal mapping. &quot;&quot;&quot; for i in range(num_rows): images, mask = batches_from_datagen[i] # print(&#39;images.shape &#39;, images.shape) # (16, 128, 128, 3) # print(&#39;mask.shape &#39;, mask.shape) # (16, 128, 128, 3) &quot;&quot;&quot; For each ID, we are going to create an image of shape [img height, img width, 3], where 3 (number of channels) are the 3 layers for each class: * the first layer: large bowel * the second layer: small bowel * the third layer: stomach &quot;&quot;&quot; sample_img = images[0, :, :, 0] # After this the shapes will be (128, 128) mask1 = mask[0, :, :, 0] # After this the shapes will be (128, 128) mask2 = mask[0, :, :, 1] # After this the shapes will be (128, 128) mask3 = mask[0, :, :, 2] # After this the shapes will be (128, 128) ax0 = fig.add_subplot(gs[i, 0]) # i here is the row-counter which is 6 im = ax0.imshow(sample_img, cmap=&quot;bone&quot;) ax1 = fig.add_subplot(gs[i, 1]) if i == 0: ax0.set_title(&quot;Image&quot;, fontsize=15, weight=&quot;bold&quot;, y=1.02) ax1.set_title(&quot;Mask&quot;, fontsize=15, weight=&quot;bold&quot;, y=1.02) plt.legend( handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4, fontsize=14, title=&quot;Mask Labels&quot;, title_fontsize=14, edgecolor=&quot;black&quot;, facecolor=&quot;#c5c6c7&quot;, ) # print(&#39;mask1 &#39;, mask1.shape) # (128, 128) # print(&#39;mask2 &#39;, mask2.shape) # (128, 128) # print(&#39;mask3 &#39;, mask3.shape) # (128, 128) # print(&#39;np.ma.masked_where(mask1== False, mask1) &#39;, np.ma.masked_where(mask1== True, mask1)) l0 = ax1.imshow(sample_img, cmap=&quot;bone&quot;) l1 = ax1.imshow(np.ma.masked_where(mask1 == False, mask1), cmap=cmap1, alpha=1) l2 = ax1.imshow(np.ma.masked_where(mask2 == False, mask2), cmap=cmap2, alpha=1) l3 = ax1.imshow(np.ma.masked_where(mask3 == False, mask3), cmap=cmap3, alpha=1) # l1 = ax1.imshow(np.ma.masked_where(mask1== 0, mask1),cmap=cmap1, alpha=1) # l2 = ax1.imshow(np.ma.masked_where(mask2== 0, mask2),cmap=cmap2, alpha=1) # l3 = ax1.imshow(np.ma.masked_where(mask3== 0, mask3),cmap=cmap3, alpha=1) _ = [ax.set_axis_off() for ax in [ax0, ax1]] colors = [im.cmap(im.norm(1)) for im in [l1, l2, l3]] . colors = [&#39;blue&#39;,&#39;green&#39;,&#39;red&#39;] labels = [&quot;large_bowel&quot;, &quot;small_bowel&quot;, &quot;stomach&quot;] plot_mask_with_color_patches(train_df_rearranged, colors, labels) . We can now see the the segmentation among three labels( large bowel,small bowel, and stomach) to envision how we will predict these 3 classes with unseen test data in the private leaderboard or in real health industry where given the scan of image from patient taken from MRI scanner, the predictive system can predict these 3 classes so that radiation oncologist can fasten their treatment process. . train_df_rearranged.head(100) . skf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42) for fold, (_, val_idx) in enumerate( skf.split( X=train_df_rearranged, y=train_df_rearranged[&quot;count&quot;], groups=train_df_rearranged[&quot;case&quot;], ), 1, ): train_df_rearranged.loc[val_idx, &quot;fold&quot;] = fold train_df_rearranged[&quot;fold&quot;] = train_df_rearranged[&quot;fold&quot;].astype(np.uint8) train_ids = train_df_rearranged[train_df_rearranged[&quot;fold&quot;] != fold_selected].index valid_ids = train_df_rearranged[train_df_rearranged[&quot;fold&quot;] == fold_selected].index X_train = train_df_rearranged[train_df_rearranged.index.isin(train_ids)] X_valid = train_df_rearranged[train_df_rearranged.index.isin(valid_ids)] train_df_rearranged.groupby(&quot;fold&quot;).size() . train_df_rearranged.head() . train_df_rearranged.groupby([&#39;fold&#39;,&#39;count&#39;])[&#39;id&#39;].count() . experiment = False if experiment: X_train = X_train[X_train.case.isin(X_train.case.unique()[:5])] X_valid = X_valid[X_valid.case.isin(X_valid.case.unique()[:2])] print(X_train.shape) print(X_valid.shape) . train_generator = DataGenerator(X_train, shuffle = True) val_generator = DataGenerator(X_valid) . ! pip install segmentation-models . ! pip install git+https://github.com/qubvel/segmentation_models . Training Process . In the training process, We divide the data into training and validation dataset so that we can measure how effective our predictive model to predict the 3 classes in the image segmentation. We use a few metrics which are widely used for evaluating the success of image segmentation problems. They are Dice Coefficient, IOU Coefficient and the loss for this multiclassification problem. You can check this article written by Ekin Tiu how he explained these concepts and why we use these metrics for image segmentation problems and the implemetation of these metrics using KERAS in a very easy way to understand. . def dice_coef(y_true, y_pred, smooth=1): y_true_f = K.flatten(y_true) y_pred_f = K.flatten(y_pred) intersection = K.sum(y_true_f * y_pred_f) return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) def iou_coef(y_true, y_pred, smooth=1): intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3]) union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection iou = K.mean((intersection + smooth) / (union + smooth), axis=0) return iou def dice_loss(y_true, y_pred): smooth = 1. y_true_f = K.flatten(y_true) y_pred_f = K.flatten(y_pred) intersection = y_true_f * y_pred_f score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth) return 1. - score def bce_dice_loss(y_true, y_pred): return binary_crossentropy(tf.cast(y_true, tf.float32), y_pred) + 0.5 * dice_loss(tf.cast(y_true, tf.float32), y_pred) . import segmentation_models as sm sm.set_framework(&#39;tf.keras&#39;) sm.framework() . from segmentation_models import Unet from segmentation_models.utils import set_trainable model = Unet(&#39;efficientnetb7&#39;, input_shape=(224, 224, 3), classes=3, activation=&#39;sigmoid&#39;, encoder_weights = &#39;imagenet&#39; ) model.compile(optimizer = &#39;adam&#39;, loss=bce_dice_loss, metrics=[dice_coef, iou_coef]) . checkpoint = ModelCheckpoint( &#39;UNET_model&#39;, monitor = &#39;val_loss&#39;, verbose=1, save_best_only=True, mode = &#39;auto&#39; ) early_stopping = EarlyStopping( patience = 5, min_delta = 0.0001, restore_best_weights= True ) . history = model.fit( train_generator, validation_data=val_generator, callbacks=[checkpoint, early_stopping], use_multiprocessing=False, workers=4, epochs=EPOCH ) . history_df = pd.DataFrame(history.history) history_df.to_csv(&quot;history_df.csv&quot;) . plt.figure(figsize=(20, 20)) plt.subplot(1, 3, 1) plt.plot(range(history.epoch[-1] + 1), history.history[&#39;loss&#39;], label=&#39;Train Loss&#39; ) plt.plot(range(history.epoch[-1] + 1), history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39; ) plt.title(&#39;Loss&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Losses&#39;) plt.legend() plt.subplot(1, 3, 2) plt.plot(range(history.epoch[-1] + 1), history.history[&#39;dice_coef&#39;], label=&#39;Train Dice Coeff&#39; ) plt.plot(range(history.epoch[-1] + 1), history.history[&#39;val_dice_coef&#39;], label=&#39;Validation Dice Coef&#39; ) plt.title(&#39;Dice Loss&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Dice Coef&#39;) plt.legend() plt.subplot(1, 3, 3) plt.plot(range(history.epoch[-1] + 1), history.history[&#39;iou_coef&#39;], label=&#39;Train IoU Coeff&#39; ) plt.plot(range(history.epoch[-1] + 1), history.history[&#39;val_iou_coef&#39;], label=&#39;Validation IoU Coef&#39; ) plt.title(&#39;IoU Loss&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;IoU Coef&#39;) plt.legend() plt.show() . We can see from these graphs, show a good result where we can decrease the loss and imcrease the value of dice coefficient and IoU coefficient eventhough there are some rooms where we can improve this model. By implementating data augmentation. Due to lack of compute machine of using GPU in my local machine. so i give the ide how to improve the model for getting better result and decrease the loss as well. . Evaluation Model . pred_batches = DataGenerator(X_valid.iloc[200:208, :], batch_size =1, subset = &#39;train&#39;, shuffle = True) preds = model.predict(pred_batches, verbose = 1) . Threshold = 0.5 # Visualizing fig = plt.figure(figsize=(10, 25)) gs = gridspec.GridSpec(nrows=8, ncols=3) colors = [&#39;yellow&#39;,&#39;green&#39;,&#39;red&#39;] labels = [&quot;Large Bowel&quot;, &quot;Small Bowel&quot;, &quot;Stomach&quot;] patches = [ mpatches.Patch(color=colors[i], label=f&quot;{labels[i]}&quot;) for i in range(len(labels))] cmap1 = mpl.colors.ListedColormap(colors[0]) cmap2 = mpl.colors.ListedColormap(colors[1]) cmap3= mpl.colors.ListedColormap(colors[2]) for i in range(8): images, mask = pred_batches[i] sample_img=images[0,:,:,0] mask1=mask[0,:,:,0] mask2=mask[0,:,:,1] mask3=mask[0,:,:,2] pre=preds[i] predict1=pre[:,:,0] predict2=pre[:,:,1] predict3=pre[:,:,2] predict1= (predict1 &gt; Threshold).astype(np.float32) predict2= (predict2 &gt; Threshold).astype(np.float32) predict3= (predict3 &gt; Threshold).astype(np.float32) ax0 = fig.add_subplot(gs[i, 0]) im = ax0.imshow(sample_img, cmap=&#39;bone&#39;) ax0.set_title(&quot;Image&quot;, fontsize=12, y=1.01) #-- ax1 = fig.add_subplot(gs[i, 1]) ax1.set_title(&quot;Mask&quot;, fontsize=12, y=1.01) l0 = ax1.imshow(sample_img, cmap=&#39;bone&#39;) l1 = ax1.imshow(np.ma.masked_where(mask1== False, mask1),cmap=cmap1, alpha=1) l2 = ax1.imshow(np.ma.masked_where(mask2== False, mask2),cmap=cmap2, alpha=1) l3 = ax1.imshow(np.ma.masked_where(mask3== False, mask3),cmap=cmap3, alpha=1) #-- ax2 = fig.add_subplot(gs[i, 2]) ax2.set_title(&quot;Predict&quot;, fontsize=12, y=1.01) l0 = ax2.imshow(sample_img, cmap=&#39;bone&#39;) l1 = ax2.imshow(np.ma.masked_where(predict1== False, predict1),cmap=cmap1, alpha=1) l2 = ax2.imshow(np.ma.masked_where(predict2== False, predict2),cmap=cmap2, alpha=1) l3 = ax2.imshow(np.ma.masked_where(predict3== False, predict3),cmap=cmap3, alpha=1) _ = [ax.set_axis_off() for ax in [ax0,ax1,ax2]] colors = [im.cmap(im.norm(1)) for im in [l1,l2, l3]] plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4,fontsize = 12,title=&#39;Mask Labels&#39;, title_fontsize=12, edgecolor=&quot;black&quot;, facecolor=&#39;#c5c6c7&#39;) . You can see there are a few errors based on segmentation and it is quite good because we are going to do generalization where the model do not overfit. . custom_objects = custom_objects={ &#39;dice_coef&#39;: dice_coef, &#39;iou_coef&#39;: iou_coef, &#39;bce_dice_loss&#39;: bce_dice_loss } model = load_model(&#39;./UNET_model&#39;, custom_objects=custom_objects) gc.collect() . def rle_encode(img): &#39;&#39;&#39; img: numpy array, 1 - mask, 0 - background Returns run length as string formated &#39;&#39;&#39; pixels = img.flatten() pixels = np.concatenate([[0], pixels, [0]]) runs = np.where(pixels[1:] != pixels[:-1])[0] + 1 runs[1::2] -= runs[::2] return &#39; &#39;.join(str(x) for x in runs) pred_batches = DataGenerator(test_df, batch_size = BATCH_SIZE, subset=&quot;test&quot;, shuffle=False) num_batches = int(len(test_df)/BATCH_SIZE) for i in range(num_batches): # Predict preds = model.predict(pred_batches[i],verbose=0) # shape: (16,128,128,3) # Rle encode for j in range(BATCH_SIZE): for k in range(3): pred_img = cv2.resize(preds[j,:,:,k], (test_df.loc[i*BATCH_SIZE+j,&quot;width&quot;], test_df.loc[i*BATCH_SIZE+j,&quot;height&quot;]), interpolation=cv2.INTER_NEAREST) # resize probabilities to original shape pred_img = (pred_img&gt;0.5).astype(dtype=&#39;uint8&#39;) # classify submission.loc[3*(i*BATCH_SIZE+j)+k,&#39;prediction&#39;] = rle_encode(pred_img) . submission.to_csv(&#39;submission.csv&#39;, index=False) submission.head(5) .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/kaggle/imagesegmentation/keras/dicecoefficient/ioucoefficient/2022/05/10/training-inferences.html",
            "relUrl": "/kaggle/imagesegmentation/keras/dicecoefficient/ioucoefficient/2022/05/10/training-inferences.html",
            "date": " • May 10, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "NLP Approach using Word Embedding",
            "content": "This is a minimal yet very powerful approach of NLP problems that you can use on Kaggle competition. I got my attention to the book written by one of Kaggle Grandmaster in his book called Approaching (Almost) Any Machine Learning Problem. I am so interested in how the author approaches the problems for each case in Kaggle Competition started by Supervised and unsupervised problems. So, NLP is one of the problems in the competition in Kaggle. We know that many approaches that we can do to preprocess text data from Bag of Words, TFIDF to Word Embedding. I am interested to dive deeper into Word Embedding because this approach yield a better result compared to BOW or Tfidf approach based on this dataset. You can find the comparison by reading this book. I found this approach quite useful for my NLP competition. Before reading this NLP approach, I hope the readers have the fundamentals concepts of pytorch, data visualization libraries and NLP concept to know the code better. . Import Data . import pandas as pd movies = pd.read_csv(&quot;imdb.csv&quot;) movies.head() . review sentiment . 0 One of the other reviewers has mentioned that ... | positive | . 1 A wonderful little production. &lt;br /&gt;&lt;br /&gt;The... | positive | . 2 I thought this was a wonderful way to spend ti... | positive | . 3 Basically there&#39;s a family where a little boy ... | negative | . 4 Petter Mattei&#39;s &quot;Love in the Time of Money&quot; is... | positive | . Check Proportion of target . movies.sentiment.value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . Create Cross Validation . import pandas as pd from sklearn import model_selection if __name__==&quot;__main__&quot;: df = pd.read_csv(&quot;imdb.csv&quot;) df.sentiment = df.sentiment.apply(lambda x: 1 if x == &quot;positive&quot; else 0) df[&quot;kfold&quot;] =-1 df = df.sample(frac=1).reset_index(drop=True) y = df.sentiment.values kf = model_selection.StratifiedKFold(n_splits=5) for f,(t_,v_) in enumerate(kf.split(X=df,y=y)): df.loc[v_,&quot;kfold&quot;] =f df.to_csv(&quot;imdb_folds.csv&quot;,index=False) . movies_folds = pd.read_csv(&quot;imdb_folds.csv&quot;) movies_folds.head() . review sentiment kfold . 0 I enjoyed Erkan &amp; Stefan  a cool and fast sto... | 1 | 0 | . 1 The only reason I rated this film as 2 is beca... | 0 | 0 | . 2 One of those movies where you take bets on who... | 0 | 0 | . 3 This series was just like what you would expec... | 1 | 0 | . 4 While many people found this film simply too s... | 1 | 0 | . There is one additional features called kfold. . Word Embedding . import numpy as np def sentence_to_vec(s,embedding_dict,stop_words,tokenizer): words =str(s).lower() words =tokenizer(words) words = [ w for w in words if w not in stop_words] words = [w for w in words if w.alpha()] M =[] for w in words: if w in embedding_dict: M.append(embedding_dict[w]) if len(M)==0: return np.zeros(300) M = np.array(M) v = M.sum() return v/np.sqrt((v**2).sum()) . Create Dataset in pytorch based on model in our dataset . import torch class IMDBDataset: def __init__(self,reviews,targets): self.reviews =reviews self.targets = targets def __len__(self): return len(self.reviews) def __getitem__(self,item): review =self.reviews[item,:] target =self.target[item] return { &quot;review&quot;: torch.tensor(review,dtype=torch.long), &quot;target&quot;: torch.tensor(target,dtype=torch.float) } . Create Model . import torch.nn as nn class LSTM(nn.Module): def __init__(self,embedding_matrix): super(LSTM,self).__init__() num_words =embedding_matrix.shape[0] embed_dim= embedding_matrix.shape[1] self.embedding = nn.Embedding( num_embeddings = num_words, embedding_dim=embed_dim ) self.embedding.weight = nn.Parameter( torch.tensor( embedding_matrix, dtype=torch.float32 ) ) self.embedding.weight.requires_grad=False self.lstm = nn.LSTM( embed_dim, 128, bidirectional=True, batch_first=True ) self.out = nn.Linear(512,1) def forward(self,x): x = self.embedding(x) x,_ = self.lstm(x) avg_pool =torch.mean(x,1) max_pool, _ = torch.max(x,1) out = torch.cat((avg_pool,maxpool),1) out = self.out(out) return out . Create Training Function for Modelling . def train(data_loader,model,optimizer,device): model.train() for data in data_loader: reviews = data[&quot;review&quot;] targets = data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.float) optimizer.zero_grad() predictions = model(reviews) loss =nn.BCEWithLogitsLoss()( predictions, targets.view(-1,1) ) loss.bakward() optimizer.step() . Create Evaluation for Modelling . def evaluate(data_loader,model,device): final_predictions =[] final_targets = [] model.eval() with torch.no_grad(): for data in data_loader: reviews =data[&quot;review&quot;] targets =data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.long) predictions = model(reviews) predictions = predictions.cpu().numpy().tolist() targets = data[&quot;target&quot;].cpu().numpy.tolist() final_predictions.extend(predictions) final_targets.extend(targets) return final_predictions,final_targets . Word Embedding Creation . import io #from tensorflow.keras import import tensorflow as tf def load_vectors(fname): fin = io.open( fname, &quot;r&quot;, encoding=&quot;utf-8&quot;, newline=&quot; n&quot;, errors=&quot;ignore&quot; ) n,d = map(int,fin.readline().split()) data ={} for line in fin: tokens = line.rstrip().split(&#39; &#39;) data[tokens[0]] = list(map(float,tokens[1:])) return data def create_embedding_matrix(world_index,embedding_dict): embedding_matrix = np.zeros((len(word_index)+1,300)) for word , i in word_index.items(): if word in embedding_dict: embedding_dict[i] = embedding_dict[word] return embedding_matrix def run(df,fold): train_df = df[df.kfold != fold].reset_index(drop=True) valid_df = df[df.kfold ==fold].reset_index(drop=True) print(&quot;Fitting tokenizer&quot;) tokenizer = tf.keras.preprocessing.text.Tokenizer() tokenizer.fit_on_texts(df.review.values.tolist()) xtrain = tokenizer.texts_to_sequences(train_df.review.values) xtest = tokenizer.texts_to_sequences(valid_df.review.values) xtrain = tf.keras.preprocessing.sequence.pad_sequences( xtrain,maxlen=128 ) xtest = tf.keras.preprocessing.sequence.pad_sequences( xtest,maxlen=128 ) train_dataset = IMDBDataset( reviews =xtrain, targets = train_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( train_dataset, batch_size =16, num_workers=2 ) valid_dataset =IMDBDataset( reviews =xtest, targets = valid_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size =8, num_workers=1 ) print(&quot;Loading Embeddings&quot;) # you can suit based on where you put your vec fasttext embedding_dict = load_vectors(&quot;crawl-300d-2M.vec/crawl-300d-2M.vec&quot;) embedding_matrix = create_embedding_matrix( tokenizer.word_index,embedding_dict ) device =torch.device(&quot;cuda&quot;) model =LSTM(embedding_matrix) model.to(device) optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) print(&quot;Training Model&quot;) best_accuracy =0 early_stopping_counter =0 for epoch in range(10): train(train_data_loader,model,optimizer,device) outputs,targets = evaluate(valid_data_loader,model,device) outputs = np.array(outputs) &gt;=0.5 accuracy = metrics.accuracy_score(targets,outputs) print(f&quot;{fold}, Epoch {epoch}, Accuracy Score ={accuracy}&quot;) if accuracy &gt; best_accuracy: best_accuracy = accuracy else: early_stopping_counter +=1 if early_stopping_counter &gt; 2: break if __name__ == &quot;__main__&quot;: df = pd.read_csv(&quot;imdb_folds.csv&quot;) run(df,0) run(df,1) run(df,2) run(df,3) run(df,4) . Fitting tokenizer Loading Embeddings . The choice of Machine learning algorithms will determine the quality of our prediction score. However, The simple model will also determine how fast the training process compared to state of art of ML algorithms. If we have laptop/computer with better GPU can help the training process. But in real life, Simple models with better preprocessing will have prediction score that is not too dfferent with newest ML algorithms one. So, It is better to discuss with the stakeholders for improving the models based on business metrics. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/nlp/wordembedding/jupyter/2022/04/06/approachingnlpusingwordembedding.html",
            "relUrl": "/nlp/wordembedding/jupyter/2022/04/06/approachingnlpusingwordembedding.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Ensembling Implementation",
            "content": "Version 1.0.1 . import numpy as np import pandas as pd import sklearn import scipy.sparse import lightgbm for p in [np, pd, scipy, sklearn, lightgbm]: print (p.__name__, p.__version__) . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 lightgbm 2.0.6 . Important! There is a huge chance that the assignment will be impossible to pass if the versions of lighgbm and scikit-learn are wrong. The versions being tested: . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 ligthgbm 2.0.6 . To install an older version of lighgbm you may use the following command: . pip uninstall lightgbm pip install lightgbm==2.0.6 . Ensembling . In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking. . We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what&#39;s happening. . import pandas as pd import numpy as np import gc import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 600) pd.set_option(&#39;display.max_columns&#39;, 50) import lightgbm as lgb from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from tqdm import tqdm_notebook from itertools import product def downcast_dtypes(df): &#39;&#39;&#39; Changes column types in the dataframe: `float64` type to `float32` `int64` type to `int32` &#39;&#39;&#39; # Select columns to downcast float_cols = [c for c in df if df[c].dtype == &quot;float64&quot;] int_cols = [c for c in df if df[c].dtype == &quot;int64&quot;] # Downcast df[float_cols] = df[float_cols].astype(np.float32) df[int_cols] = df[int_cols].astype(np.int32) return df . Load data subset . Let&#39;s load the data from the hard drive first. . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) shops = pd.read_csv(&#39;../readonly/final_project_data/shops.csv&#39;) items = pd.read_csv(&#39;../readonly/final_project_data/items.csv&#39;) item_cats = pd.read_csv(&#39;../readonly/final_project_data/item_categories.csv&#39;) . And use only 3 shops for simplicity. . sales = sales[sales[&#39;shop_id&#39;].isin([26, 27, 28])] . Get a feature matrix . We now need to prepare the features. This part is all implemented for you. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;shop_id&#39;].unique() cur_items = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) # Turn the grid into a dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) # Groupby data to get shop-item-month aggregates gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) # Fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] # Join it to the grid all_data = pd.merge(grid, gb, how=&#39;left&#39;, on=index_cols).fillna(0) # Same as above but with shop-month aggregates gb = sales.groupby([&#39;shop_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_shop&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Same as above but with item-month aggregates gb = sales.groupby([&#39;item_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_item&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1] == &#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;item_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Downcast dtypes from 64 to 32 bit to save memory all_data = downcast_dtypes(all_data) del grid, gb gc.collect(); . /opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs) . After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago. . cols_to_rename = list(all_data.columns.difference(index_cols)) shift_range = [1, 2, 3, 4, 5, 12] for month_shift in tqdm_notebook(shift_range): train_shift = all_data[index_cols + cols_to_rename].copy() train_shift[&#39;date_block_num&#39;] = train_shift[&#39;date_block_num&#39;] + month_shift foo = lambda x: &#39;{}_lag_{}&#39;.format(x, month_shift) if x in cols_to_rename else x train_shift = train_shift.rename(columns=foo) all_data = pd.merge(all_data, train_shift, on=index_cols, how=&#39;left&#39;).fillna(0) del train_shift # Don&#39;t use old data from year 2013 all_data = all_data[all_data[&#39;date_block_num&#39;] &gt;= 12] # List of all lagged features fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] # We will drop these at fitting stage to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + [&#39;date_block_num&#39;] # Category for each item item_category_mapping = items[[&#39;item_id&#39;,&#39;item_category_id&#39;]].drop_duplicates() all_data = pd.merge(all_data, item_category_mapping, how=&#39;left&#39;, on=&#39;item_id&#39;) all_data = downcast_dtypes(all_data) gc.collect(); . . To this end, we&#39;ve created a feature matrix. It is stored in all_data variable. Take a look: . all_data.head() . shop_id item_id date_block_num target target_shop target_item target_lag_1 target_item_lag_1 target_shop_lag_1 target_lag_2 target_item_lag_2 target_shop_lag_2 target_lag_3 target_item_lag_3 target_shop_lag_3 target_lag_4 target_item_lag_4 target_shop_lag_4 target_lag_5 target_item_lag_5 target_shop_lag_5 target_lag_12 target_item_lag_12 target_shop_lag_12 item_category_id . 0 28 | 10994 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 1.0 | 6454.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37 | . 1 28 | 10992 | 12 | 3.0 | 6949.0 | 4.0 | 3.0 | 7.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 37 | . 2 28 | 10991 | 12 | 1.0 | 6949.0 | 5.0 | 1.0 | 3.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 2.0 | 4.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 40 | . 3 28 | 10988 | 12 | 1.0 | 6949.0 | 2.0 | 2.0 | 5.0 | 8499.0 | 4.0 | 5.0 | 6454.0 | 5.0 | 6.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . 4 28 | 11002 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . Train/test split . For a sake of the programming assignment, let&#39;s artificially split the data into train and test. We will treat last month data as the test set. . dates = all_data[&#39;date_block_num&#39;] last_block = dates.max() print(&#39;Test `date_block_num` is %d&#39; % last_block) . Test `date_block_num` is 33 . dates_train = dates[dates &lt; last_block] dates_test = dates[dates == last_block] X_train = all_data.loc[dates &lt; last_block].drop(to_drop_cols, axis=1) X_test = all_data.loc[dates == last_block].drop(to_drop_cols, axis=1) y_train = all_data.loc[dates &lt; last_block, &#39;target&#39;].values y_test = all_data.loc[dates == last_block, &#39;target&#39;].values . First level models . You need to implement a basic stacking scheme. We have a time component here, so we will use scheme f) from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let&#39;s see how we get test meta-features first. . Test meta-features . Firts, we will run linear regression on numeric columns and get predictions for the last month. . lr = LinearRegression() lr.fit(X_train.values, y_train) pred_lr = lr.predict(X_test.values) print(&#39;Test R-squared for linreg is %f&#39; % r2_score(y_test, pred_lr)) . Test R-squared for linreg is 0.743180 . And the we run LightGBM. . lgb_params = { &#39;feature_fraction&#39;: 0.75, &#39;metric&#39;: &#39;rmse&#39;, &#39;nthread&#39;:1, &#39;min_data_in_leaf&#39;: 2**7, &#39;bagging_fraction&#39;: 0.75, &#39;learning_rate&#39;: 0.03, &#39;objective&#39;: &#39;mse&#39;, &#39;bagging_seed&#39;: 2**7, &#39;num_leaves&#39;: 2**7, &#39;bagging_freq&#39;:1, &#39;verbose&#39;:0 } model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100) pred_lgb = model.predict(X_test) print(&#39;Test R-squared for LightGBM is %f&#39; % r2_score(y_test, pred_lgb)) . Test R-squared for LightGBM is 0.738391 . Finally, concatenate test predictions to get test meta-features. . X_test_level2 = np.c_[pred_lr, pred_lgb] . Train meta-features . Now it is your turn to write the code. You need to implement scheme f) from the reading material. Here, we will use duration T equal to month and M=15. . That is, you need to get predictions (meta-features) from linear regression and LightGBM for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models. . dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])] # That is how we get target for the 2nd level dataset y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])] . X_train_level2 = np.zeros([y_train_level2.shape[0], 2]) # Now fill `X_train_level2` with metafeatures for cur_block_num in [27, 28, 29, 30, 31, 32]: print(cur_block_num) &#39;&#39;&#39; 1. Split `X_train` into parts Remember, that corresponding dates are stored in `dates_train` 2. Fit linear regression 3. Fit LightGBM and put predictions 4. Store predictions from 2. and 3. in the right place of `X_train_level2`. You can use `dates_train_level2` for it Make sure the order of the meta-features is the same as in `X_test_level2` &#39;&#39;&#39; # YOUR CODE GOES HERE X_train_meta = all_data.loc[dates &lt; cur_block_num].drop(to_drop_cols, axis=1) X_test_meta = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1) y_train_meta = all_data.loc[dates &lt; cur_block_num, &#39;target&#39;].values y_test_meta = all_data.loc[dates == cur_block_num, &#39;target&#39;].values lr.fit(X_train_meta.values, y_train_meta) X_train_level2[dates_train_level2 == cur_block_num, 0] = lr.predict(X_test_meta.values) model = lgb.train(lgb_params, lgb.Dataset(X_train_meta, label=y_train_meta), 100) X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_meta) # Sanity check assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988, 1.38811989])) . 27 28 29 30 31 32 . Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig scatter plot between the two metafeatures. Plot the scatter plot below. . plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1]) . &lt;matplotlib.collections.PathCollection at 0x7fa38c41ca58&gt; . Ensembling . Now, when the meta-features are created, we can ensemble our first level models. . Simple convex mix . Let&#39;s start with simple linear convex mix: . $$ mix= alpha cdot text{linreg_prediction}+(1- alpha) cdot text{lgb_prediction} $$We need to find an optimal $ alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $ alpha$ out of alphas_to_try array. Remember, that you need to use train meta-features (not test) when searching for $ alpha$. . alphas_to_try = np.linspace(0, 1, 1001) # YOUR CODE GOES HERE r2_scores = np.array([r2_score(y_train_level2, np.dot(X_train_level2, [alpha, 1 - alpha])) for alpha in alphas_to_try]) best_alpha = alphas_to_try[r2_scores.argmax()] # YOUR CODE GOES HERE r2_train_simple_mix = r2_scores.max() # YOUR CODE GOES HERE print(&#39;Best alpha: %f; Corresponding r2 score on train: %f&#39; % (best_alpha, r2_train_simple_mix)) . Best alpha: 0.765000; Corresponding r2 score on train: 0.627255 . Now use the $ alpha$ you&#39;ve found to compute predictions for the test set . test_preds = best_alpha * pred_lr + (1 - best_alpha) * pred_lgb # YOUR CODE GOES HERE r2_test_simple_mix = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Test R-squared for simple mix is %f&#39; % r2_test_simple_mix) . Test R-squared for simple mix is 0.781144 . Stacking . Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above. . lr.fit(X_train_level2, y_train_level2) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) . Compute R-squared on the train and test sets. . train_preds = lr.predict(X_train_level2) # YOUR CODE GOES HERE r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE test_preds = lr.predict(np.vstack((pred_lr, pred_lgb)).T) # YOUR CODE GOES HERE r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Train R-squared for stacking is %f&#39; % r2_train_stacking) print(&#39;Test R-squared for stacking is %f&#39; % r2_test_stacking) . Train R-squared for stacking is 0.632176 Test R-squared for stacking is 0.771297 . Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. Examine and compare train and test scores for the two methods. . And of course this particular case does not mean simple mix is always better than stacking. . We all done! Submit everything we need to the grader now. . from grader import Grader grader = Grader() grader.submit_tag(&#39;best_alpha&#39;, best_alpha) grader.submit_tag(&#39;r2_train_simple_mix&#39;, r2_train_simple_mix) grader.submit_tag(&#39;r2_test_simple_mix&#39;, r2_test_simple_mix) grader.submit_tag(&#39;r2_train_stacking&#39;, r2_train_stacking) grader.submit_tag(&#39;r2_test_stacking&#39;, r2_test_stacking) . Current answer for task best_alpha is: 0.765 Current answer for task r2_train_simple_mix is: 0.627255043446 Current answer for task r2_test_simple_mix is: 0.781144169579 Current answer for task r2_train_stacking is: 0.632175561459 Current answer for task r2_test_stacking is: 0.771297132342 . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot; TOKEN HERE&quot;# TOKEN HERE grader.status() . You want to submit these numbers: Task best_alpha: 0.765 Task r2_train_simple_mix: 0.627255043446 Task r2_test_simple_mix: 0.781144169579 Task r2_train_stacking: 0.632175561459 Task r2_test_stacking: 0.771297132342 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/01/Ensembling_Implementation.html",
            "relUrl": "/2022/04/01/Ensembling_Implementation.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Z-unlock Challenge: Data Visualization",
            "content": "We will Analyze the correlation of temperatures changes on energy use, land cover,waste use and deforestoration by questioning these questions. . What are the areas with biggest/smallest change in temperature? | Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) | How does the seasonal temperature change look like? | How does this vary by continent? Particularly South America? | . # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) . df_temperature = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv&quot;) df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . temp_max = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].max().sort_values(ascending=False).reset_index() temp_min = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].min().sort_values().reset_index() d2 = temp_max[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Max temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . d2 = temp_min[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Min temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . Biggest/smallest change in temperature: . Svalbard and Jan Mayeb Island is the most change in temperature based on the chart above | . Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) . Look at all the possibilities from another dataset/tables | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . land_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv&quot;) land_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2001 | 2001 | 1000 ha | 88.1603 | FC | Calculated data | . 1 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2002 | 2002 | 1000 ha | 88.1818 | FC | Calculated data | . 2 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2003 | 2003 | 1000 ha | 88.2247 | FC | Calculated data | . 3 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2004 | 2004 | 1000 ha | 88.2462 | FC | Calculated data | . 4 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2005 | 2005 | 1000 ha | 88.3106 | FC | Calculated data | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . waste_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv&quot;) waste_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1990 | 1990 | kilotonnes | 0.0 | Fc | Calculated data | . 1 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1991 | 1991 | kilotonnes | 0.0 | Fc | Calculated data | . 2 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1992 | 1992 | kilotonnes | 0.0 | Fc | Calculated data | . 3 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1993 | 1993 | kilotonnes | 0.0 | Fc | Calculated data | . 4 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1994 | 1994 | kilotonnes | 0.0 | Fc | Calculated data | . fires_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv&quot;) fires_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Source Code Source Unit Value Flag Flag Description Note . 0 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1990 | 1990 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 1 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1991 | 1991 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 2 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1992 | 1992 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 3 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1993 | 1993 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 4 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1994 | 1994 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . temp_change= df_temperature.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, hue=&#39;Months&#39;, legend=&#39;full&#39;, data=temp_change, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temp_change.Months.unique()))) max_value_per_year = temp_change.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.title(&quot;The trend for temperature change annually over Months&quot;) plt.axvspan(2015, 2020,alpha=0.15) plt.show() . land_cover= land_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=land_cover, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(land_cover.Year.unique()))) max_value_per_year = land_cover.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Land Cover&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2004, 2006,alpha=0.15) plt.show() . energy_use= energy_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=energy_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(energy_use.Year.unique()))) max_value_per_year = energy_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Energy Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1985, 1989,alpha=0.15) plt.show() . waste_use= waste_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=waste_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(waste_use.Year.unique()))) max_value_per_year = waste_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Waste Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1990, 1993,alpha=0.15) plt.show() . fires_use= fires_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=fires_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(fires_use.Year.unique()))) max_value_per_year = fires_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Fires Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1999, 2003,alpha=0.15) plt.show() . Correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions and Fires.) . Insight Based on Aggregating the mean per year shows correlation among temperature, energy use, land cover, waste use, and fires. All country-Value indicator(Value feature based on each tables) combinations show an increase, but there are subtle differences: . In Land cover use, in 2004-2005, there was a signifant increase followed by a slighly increase in from 2011-2017. | In Energy use, in 1985-1989, there was a signifant increase followed by a slighly increase in from 2019-2020. | In Waste use, in 1999-1993, there was a signifant drop followed by a significant increase from 1994-2020. | In Fires use, in 1990-2003, there was a signifant increase followed by a slighly decrease from 2003-2020. . | Almost everywhere, the end-of-year show an correlation that the the temperature that increase yearly affect the use of waste, energy,deforestoration, and land cover yearly. . | . How does the seasonal temperature change look like? . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . df_temperature.groupby(&quot;Months&quot;)[&quot;Value&quot;].agg([&quot;sum&quot;,&quot;mean&quot;,&quot;max&quot;]) . sum mean max . Months . Dec–Jan–Feb 6113.952 | 0.467428 | 8.206 | . Jun–Jul–Aug 6951.271 | 0.531890 | 4.764 | . Mar–Apr–May 6872.110 | 0.525511 | 5.533 | . Meteorological year 6413.093 | 0.491651 | 5.328 | . Sep–Oct–Nov 5761.315 | 0.441108 | 6.084 | . plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_temperature.groupby([&#39;Months&#39;])): ax = plt.subplot(6, 3, i+1, ymargin=0.5) ax.plot(df.Value) ax.set_title(combi) #if i == 6: break plt.tight_layout(h_pad=3.0) plt.suptitle(&#39;Seasonal Temperature Change&#39;, y=1.03) plt.show() . Seasonal Temperature Change . We can see that on each month has different maximum temperrature. DEC-Jan-Feb has the hottest temperature with 8.206 followed by Sept-Oct-Nov. | . How does this vary by continent? Particularly South America? . south_america_countries =[&#39;Brazil&#39;,&#39;Argentina&#39;,&#39;Chile&#39;,&#39;Colombia&#39;, &#39;Ecuador&#39;,&#39;Venezuela (Bolivarian Republic of)&#39;, &#39;Bolivia (Plurinational State of)&#39;,&#39;Guyana&#39;, &#39;Uruguay&#39;,&#39;Suriname&#39;, &#39;Paraguay&#39;,&#39;Aruba&#39;,&#39;Trinidad and Tobago&#39;] temperature_sa =df_temperature[df_temperature[&quot;Area&quot;].isin(south_america_countries)] temperature_sa.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 2700 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | 0.035 | Fc | Calculated data | . 2701 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | -0.144 | Fc | Calculated data | . 2702 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 0.552 | Fc | Calculated data | . 2703 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | 0.052 | Fc | Calculated data | . 2704 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.034 | Fc | Calculated data | . temperature_sa.groupby([&quot;Area&quot;])[&quot;Value&quot;].agg([&quot;max&quot;,&quot;min&quot;]).plot(kind=&quot;bar&quot;,figsize=(12,8)) plt.ylabel(&quot;Temperature&quot;) . Text(0, 0.5, &#39;Temperature&#39;) . How about Meterological season temperature changes in South America? . temperature_sa= temperature_sa.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, hue=&#39;Months&#39;, data=temperature_sa, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temperature_sa.Months.unique()))) max_value_per_year = temperature_sa.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature Change&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2013, 2016,alpha=0.15) plt.show() . Ultimately, there is an uptrend for temperature change in South America annually in which the peak is around 2013-2016. | . For joining this competition, see Z-Unlocked_Challenge1. There is a chance to visit Barcelona for Kaggle Competition. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/03/30/data-visualization-challenge.html",
            "relUrl": "/2022/03/30/data-visualization-challenge.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Play for a chance to win 1 of 10 HP ZBook Studios & a trip to the Kaggle Days x Z by HP World Championship in Barcelona",
            "content": "Challenge 4 - (Image Classification) . The Task . The challenge is to build a machine learning model to classify images of &quot;La Eterna&quot;. This can be done in a variety of ways. For this challenge i implemented CNN using pytorch to classify the images. The data is split into a training and a submission set. The images includes two labeled folders in the Test folder. The folder labeled &quot;la_eterna&quot; includes the pictures of la eterna that Eva captured. The other folder labeled &quot;other_flowers&quot; includes pictures of other flowers that are not la eterna. We will use this data to build our classifier. Each of the images has been formatted to the dimensions (224,224, 3) for the analysis. You can check the episode for each chalenge in this video . from IPython.display import YouTubeVideo YouTubeVideo(&#39;iycrQpWIMnQ&#39;, width=800, height=500) . Import libraries . %matplotlib inline %config InlineBackend.figure_format = &#39;retina&#39; import matplotlib.pyplot as plt import numpy as np import torch import torchvision from torch import nn from torch import optim import torch.nn.functional as F from torchvision import datasets, transforms, models . data_dir = &quot;data_cleaned/Train&quot; def load_split_train_test(datadir, valid_size = .2): train_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) test_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) train_data = datasets.ImageFolder(datadir, transform=train_transforms) test_data = datasets.ImageFolder(datadir, transform=test_transforms) num_train = len(train_data) indices = list(range(num_train)) split = int(np.floor(valid_size * num_train)) np.random.shuffle(indices) from torch.utils.data.sampler import SubsetRandomSampler train_idx, test_idx = indices[split:], indices[:split] train_sampler = SubsetRandomSampler(train_idx) test_sampler = SubsetRandomSampler(test_idx) trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=64) testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=64) return trainloader, testloader trainloader, testloader = load_split_train_test(data_dir, .2) print(trainloader.dataset.classes) . [&#39;la_eterna&#39;, &#39;other_flowers&#39;] . def img_display(img): img = img # unnormalize npimg = img.numpy() npimg = np.transpose(npimg, (1, 2, 0)) return npimg . Check trainloader Images . dataiter = iter(trainloader) images, labels = dataiter.next() arthopod_types = {0: &#39;la_eterna&#39;, 1: &#39;other_flowers&#39;} # Viewing data examples used for training fig, axis = plt.subplots(3, 5, figsize=(15, 10)) for i, ax in enumerate(axis.flat): with torch.no_grad(): image, label = images[i], labels[i] ax.imshow(img_display(image)) # add image ax.set(title = f&quot;{arthopod_types[label.item()]}&quot;) # add label . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Check the images shape in each steps of Neural Network Process to make us easier to form the CNN model . import matplotlib.pyplot as plt import torchvision dataiter = iter(trainloader) images,labels = dataiter.next() img_display(torchvision.utils.make_grid(images)) conv1 = nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) pool = nn.MaxPool2d(2, 2) conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) print(images.shape) x = conv1(images) print(x.shape) x =pool(x) print(x.shape) x =conv2(x) print(x.shape) x =conv3(x) print(x.shape) . torch.Size([64, 3, 224, 224]) torch.Size([64, 12, 224, 224]) torch.Size([64, 12, 112, 112]) torch.Size([64, 20, 112, 112]) torch.Size([64, 32, 112, 112]) . Create Convolutional Neural Network Architecture . class ConvNet(nn.Module): def __init__(self,num_classes=2): super(ConvNet,self).__init__() #Output size after convolution filter #((w-f+2P)/s) +1 #Input shape= (64,3,224,224) self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1) #Shape= (64,12,224,224) self.bn1=nn.BatchNorm2d(num_features=12) #Shape= (64,12,224,224) self.relu1=nn.ReLU() #Shape= (64,12,224,224) self.pool=nn.MaxPool2d(kernel_size=2) #Shape= (64,12,224,224) self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1) #Shape= (64,20,112,112) self.relu2=nn.ReLU() #Shape= (64,20,112,112) self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1) #Shape= (64,32,112,112) self.bn3=nn.BatchNorm2d(num_features=32) #Shape= (64,32,112,112) self.relu3=nn.ReLU() #Shape= (64,32,112,112) self.fc=nn.Linear(in_features=112 * 112* 32,out_features=num_classes) #Feed forwad function def forward(self,input): output=self.conv1(input) output=self.bn1(output) output=self.relu1(output) output=self.pool(output) output=self.conv2(output) output=self.relu2(output) output=self.conv3(output) output=self.bn3(output) output=self.relu3(output) #Above output will be in matrix form, with shape (256,32,112,112) output=output.view(-1,32*112*112) output=self.fc(output) return output . model = ConvNet() # On CPU print(model) . ConvNet( (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (relu2): ReLU() (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu3): ReLU() (fc): Linear(in_features=401408, out_features=2, bias=True) ) . Initialize Loss function . criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.0001) . def accuracy(out, labels): _,pred = torch.max(out, dim=1) return torch.sum(pred==labels).item() . Training Process through Convolutional Neural Network . n_epochs = 12 print_every = 10 valid_loss_min = np.Inf val_loss = [] val_acc = [] train_loss = [] train_acc = [] total_step = len(trainloader) for epoch in range(1, n_epochs+1): running_loss = 0.0 # scheduler.step(epoch) correct = 0 total=0 print(f&#39;Epoch {epoch} n&#39;) for batch_idx, (data_, target_) in enumerate(trainloader): #data_, target_ = data_.to(device), target_.to(device)# on GPU # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = model(data_) loss = criterion(outputs, target_) loss.backward() optimizer.step() # print statistics running_loss += loss.item() _,pred = torch.max(outputs, dim=1) correct += torch.sum(pred==target_).item() total += target_.size(0) if (batch_idx) % 20 == 0: print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; .format(epoch, n_epochs, batch_idx, total_step, loss.item())) train_acc.append(100 * correct / total) train_loss.append(running_loss/total_step) print(f&#39; ntrain loss: {np.mean(train_loss):.4f}, train acc: {(100 * correct / total):.4f}&#39;) batch_loss = 0 total_t=0 correct_t=0 with torch.no_grad(): model.eval() for data_t, target_t in (testloader): #data_t, target_t = data_t.to(device), target_t.to(device)# on GPU outputs_t = model(data_t) loss_t = criterion(outputs_t, target_t) batch_loss += loss_t.item() _,pred_t = torch.max(outputs_t, dim=1) correct_t += torch.sum(pred_t==target_t).item() total_t += target_t.size(0) val_acc.append(100 * correct_t / total_t) val_loss.append(batch_loss/len(testloader)) network_learned = batch_loss &lt; valid_loss_min print(f&#39;validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t / total_t):.4f} n&#39;) # Saving the best weight if network_learned: valid_loss_min = batch_loss torch.save(model.state_dict(), &#39;model_classification_tutorial.pt&#39;) print(&#39;Detected network improvement, saving current model&#39;) model.train() . Epoch 1 Epoch [1/12], Step [0/7], Loss: 0.7828 train loss: 4.3233, train acc: 55.4779 validation loss: 0.5667, validation acc: 69.1589 Detected network improvement, saving current model Epoch 2 Epoch [2/12], Step [0/7], Loss: 2.0096 train loss: 3.6552, train acc: 75.0583 validation loss: 0.7703, validation acc: 42.0561 Epoch 3 Epoch [3/12], Step [0/7], Loss: 1.2640 train loss: 2.7823, train acc: 80.1865 validation loss: 0.7495, validation acc: 76.6355 Epoch 4 Epoch [4/12], Step [0/7], Loss: 0.8584 train loss: 2.2371, train acc: 89.7436 validation loss: 0.8120, validation acc: 60.7477 Epoch 5 Epoch [5/12], Step [0/7], Loss: 1.0303 train loss: 1.8770, train acc: 90.9091 validation loss: 0.7693, validation acc: 86.9159 Epoch 6 Epoch [6/12], Step [0/7], Loss: 0.2469 train loss: 1.5874, train acc: 97.2028 validation loss: 0.7170, validation acc: 89.7196 Detected network improvement, saving current model Epoch 7 Epoch [7/12], Step [0/7], Loss: 0.0705 train loss: 1.3695, train acc: 97.6690 validation loss: 0.6692, validation acc: 91.5888 Detected network improvement, saving current model Epoch 8 Epoch [8/12], Step [0/7], Loss: 0.0019 train loss: 1.1994, train acc: 99.5338 validation loss: 0.6778, validation acc: 87.8505 Epoch 9 Epoch [9/12], Step [0/7], Loss: 0.0163 train loss: 1.0672, train acc: 99.7669 validation loss: 0.6578, validation acc: 89.7196 Epoch 10 Epoch [10/12], Step [0/7], Loss: 0.0003 train loss: 0.9605, train acc: 100.0000 validation loss: 0.6351, validation acc: 91.5888 Epoch 11 Epoch [11/12], Step [0/7], Loss: 0.0018 train loss: 0.8734, train acc: 100.0000 validation loss: 0.6198, validation acc: 91.5888 Epoch 12 Epoch [12/12], Step [0/7], Loss: 0.0006 train loss: 0.8007, train acc: 100.0000 validation loss: 0.6055, validation acc: 91.5888 . Insights &#128161; . You can see that at epoch 5 we get the best result with accuracy 97.6 on training images and 91.6 on the validation images and loss is 1.3 on training images and 0.6 on validation images. . Train - Validation Loss . fig = plt.figure(figsize=(20,10)) plt.title(&quot;Train - Validation Loss&quot;) plt.plot( train_loss, label=&#39;train&#39;) plt.plot( val_loss, label=&#39;validation&#39;) plt.xlabel(&#39;num_epochs&#39;, fontsize=12) plt.ylabel(&#39;loss&#39;, fontsize=12) plt.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x15ad05fe1c0&gt; . Train - Validation Accuracy . fig = plt.figure(figsize=(20,10)) plt.title(&quot;Train - Validation Accuracy&quot;) plt.plot(train_acc, label=&#39;train&#39;) plt.plot(val_acc, label=&#39;validation&#39;) plt.xlabel(&#39;num_epochs&#39;, fontsize=12) plt.ylabel(&#39;accuracy&#39;, fontsize=12) plt.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x15ad0406130&gt; . model.load_state_dict(torch.load(&#39;model_classification_tutorial.pt&#39;)) . &lt;All keys matched successfully&gt; . Evaluation using submission images . submission_path =&quot;data_cleaned/scraped_images&quot; submission_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) submission_data = datasets.ImageFolder(submission_path,transform=submission_transforms) submissionloader = torch.utils.data.DataLoader(submission_data,shuffle=True, batch_size=64) . dataiter = iter(submissionloader) images, labels = dataiter.next() flower_types = {0:&#39;la_eterna&#39;, 1:&#39;other_flowers&#39;} # Viewing data examples used for training fig, axis = plt.subplots(3, 5, figsize=(15, 10)) submission =pd.DataFrame(columns = [&quot;labels&quot;,&quot;preditions&quot;]) with torch.no_grad(): model.eval() for ax, image, label in zip(axis.flat,images, labels): ax.imshow(img_display(image)) # add image image_tensor = image.unsqueeze_(0) output_ = model(image_tensor) output_ = output_.argmax() k = output_.item()==label.item() ax.set_title(str(flower_types[label.item()])+&quot;:&quot; +str(k)) # add label . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Insights &#128161; . You can see that based on the images given, the model can clasify whether it is la eterna or other flower. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/pytorch/neural%20network/images%20classification/2022/03/25/zunlock-Image-classification.html",
            "relUrl": "/pytorch/neural%20network/images%20classification/2022/03/25/zunlock-Image-classification.html",
            "date": " • Mar 25, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Model Design using Pytorch",
            "content": "Source: 3Blue1Brown . Neural Network has been evolving recently. Many applications have been developed by using neural network. The development of GPU provided by big company like NVIDIA has fasten how the training process in Architecture of Neural network that needs many parameters in order to get better result for any kind of problems. You can see on the GIf above how neural network works throguh many layers that involve many parameters that can create good output that can identify the real value. The practical way is image identification where Neural network through combining many layers and parameters, activation function, and loss that can be improved to identify the image based on the GIF above. We will learn the implementation through Pytorch in this tutorial. . Study Case 1 . You work as an assistant of the mayor of Somerville and the HR department has asked you to build a model capable of predicting whether a person is happy with the current administration based on their satisfaction with the city&#39;s services . import pandas as pd import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim import warnings warnings.filterwarnings(&quot;ignore&quot;) . df = pd.read_csv(&quot;SomervilleHappinessSurvey2015.csv&quot;) df.head() . D X1 X2 X3 X4 X5 X6 . 0 0 | 3 | 3 | 3 | 4 | 2 | 4 | . 1 0 | 3 | 2 | 3 | 5 | 4 | 3 | . 2 1 | 5 | 3 | 3 | 3 | 3 | 5 | . 3 0 | 5 | 4 | 3 | 3 | 3 | 5 | . 4 0 | 5 | 4 | 3 | 3 | 3 | 5 | . Columns Information: . D = decision attribute (D) with values 0 (unhappy) and 1 (happy) | X1 = the availability of information about the city services | X2 = the cost of housing | X3 = the overall quality of public schools | X4 = your trust in the local police | X5 = the maintenance of streets and sidewalks | X6 = the availability of social community events . | Attributes X1 to X6 have values 1 to 5. . | . X = torch.tensor(df.drop(&quot;D&quot;,axis=1).astype(np.float32).values) y = torch.tensor(df[&quot;D&quot;].astype(np.float32).values) . X[:10] . tensor([[3., 3., 3., 4., 2., 4.], [3., 2., 3., 5., 4., 3.], [5., 3., 3., 3., 3., 5.], [5., 4., 3., 3., 3., 5.], [5., 4., 3., 3., 3., 5.], [5., 5., 3., 5., 5., 5.], [3., 1., 2., 2., 1., 3.], [5., 4., 4., 4., 4., 5.], [4., 1., 4., 4., 4., 4.], [4., 4., 4., 2., 5., 5.]]) . # 6 from how many features we have # output whether happy or unhappy model = nn.Sequential(nn.Linear(6,1), nn.Sigmoid()) print(model) . Sequential( (0): Linear(in_features=6, out_features=1, bias=True) (1): Sigmoid() ) . loss_func = nn.MSELoss() optimizer = optim.Adam(model.parameters(),lr=1e-2) . losses =[] for i in range(20): y_pred = model(X) loss = loss_func(y_pred,y) # item() is used for getting value from tensor losses.append(loss.item()) optimizer.zero_grad() #do back propagation loss.backward() #update weights during backward propagation optimizer.step() if i%5 ==0: print(i,loss.item()) . 0 0.5094006061553955 5 0.46509113907814026 10 0.3730385899543762 15 0.26985085010528564 . Plot the loss for each epochs . plt.plot(range(0,20),losses) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Loss&quot;) . Text(0, 0.5, &#39;Loss&#39;) . This is just a simple sequential neural network by implementing pytorch. We will deep dive into a process of building model in the study case 2 where we implement the process of Data Science lifecycle from cleaning the data,splitting the data, making the prediction and evaluating the prediction. . Study Case 2 . Deep Learning in Bank . Deep Learning has been implementing in many sectors including Bank.The problem thas has been happening for this sector is to predict whether bank should grant loan for the customers who will be making credit card. This is essential for Bank because it can measure how they can validate how much money that they can provide and estimate the profit from customers who will use the credit card based on a period of time. We will detect the customers who will be potential to grant loan that can affect to the income of the bank through this dataset. . We will follow a few steps before modelling our data into ANN using Pytorch including : . Understand the data including dealing with quality of data | rescale the features (giving different scales for each features may result that a given features is more important thatn others as it has higher numerical values) | split the data | . df_credit = pd.read_excel(&quot;default of credit card clients.xls&quot;,skiprows=1) df_credit.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 1 | 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 2 | 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 3 | 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 4 | 50000 | 2 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 5 | 50000 | 1 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 25 columns . print(f&quot;Rows : {df_credit.shape[0]}, Columns:{df_credit.shape[1]}&quot;) . Rows : 30000, Columns:25 . data_clean =df_credit.drop([&quot;ID&quot;,&quot;SEX&quot;],axis=1) data_clean.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 50000 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 50000 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 23 columns . (data_clean.isnull().sum()/data_clean.shape[0]).plot() . &lt;AxesSubplot:&gt; . data_clean.describe() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 3.000000e+04 | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 167484.322667 | 1.853133 | 1.551867 | 35.485500 | -0.016700 | -0.133767 | -0.166200 | -0.220667 | -0.266200 | -0.291100 | ... | 43262.948967 | 40311.400967 | 38871.760400 | 5663.580500 | 5.921163e+03 | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | . std 129747.661567 | 0.790349 | 0.521970 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | 1.133187 | 1.149988 | ... | 64332.856134 | 60797.155770 | 59554.107537 | 16563.280354 | 2.304087e+04 | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | . min 10000.000000 | 0.000000 | 0.000000 | 21.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | ... | -170000.000000 | -81334.000000 | -339603.000000 | 0.000000 | 0.000000e+00 | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 50000.000000 | 1.000000 | 1.000000 | 28.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 2326.750000 | 1763.000000 | 1256.000000 | 1000.000000 | 8.330000e+02 | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | . 50% 140000.000000 | 2.000000 | 2.000000 | 34.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 19052.000000 | 18104.500000 | 17071.000000 | 2100.000000 | 2.009000e+03 | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | . 75% 240000.000000 | 2.000000 | 2.000000 | 41.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 54506.000000 | 50190.500000 | 49198.250000 | 5006.000000 | 5.000000e+03 | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | . max 1000000.000000 | 6.000000 | 3.000000 | 79.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | ... | 891586.000000 | 927171.000000 | 961664.000000 | 873552.000000 | 1.684259e+06 | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | . 8 rows × 23 columns . outliers ={} for i in range(data_clean.shape[1]): min_t = data_clean[data_clean.columns[i]].mean()-(3*data_clean[data_clean.columns[i]].std()) max_t = data_clean[data_clean.columns[i]].mean()+(3*data_clean[data_clean.columns[i]].std()) count =0 for j in data_clean[data_clean.columns[i]]: if j &lt; min_t or j &gt; max_t: count +=1 percentage = count/data_clean.shape[0] outliers[data_clean.columns[i]] = round(percentage,3) . from pprint import pprint pprint(outliers) . {&#39;AGE&#39;: 0.005, &#39;BILL_AMT1&#39;: 0.023, &#39;BILL_AMT2&#39;: 0.022, &#39;BILL_AMT3&#39;: 0.022, &#39;BILL_AMT4&#39;: 0.023, &#39;BILL_AMT5&#39;: 0.022, &#39;BILL_AMT6&#39;: 0.022, &#39;EDUCATION&#39;: 0.011, &#39;LIMIT_BAL&#39;: 0.004, &#39;MARRIAGE&#39;: 0.0, &#39;PAY_0&#39;: 0.005, &#39;PAY_2&#39;: 0.005, &#39;PAY_3&#39;: 0.005, &#39;PAY_4&#39;: 0.006, &#39;PAY_5&#39;: 0.005, &#39;PAY_6&#39;: 0.004, &#39;PAY_AMT1&#39;: 0.013, &#39;PAY_AMT2&#39;: 0.01, &#39;PAY_AMT3&#39;: 0.012, &#39;PAY_AMT4&#39;: 0.013, &#39;PAY_AMT5&#39;: 0.014, &#39;PAY_AMT6&#39;: 0.015, &#39;default payment next month&#39;: 0.0} . data_clean[&quot;default payment next month&quot;].value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . target = data_clean[&quot;default payment next month&quot;] yes = target[target == 1].count() no = target[target == 0].count() . data_yes = data_clean[data_clean[&quot;default payment next month&quot;] == 1] data_no = data_clean[data_clean[&quot;default payment next month&quot;] == 0] over_sampling = data_yes.sample(no, replace=True, random_state = 0) data_resampled = pd.concat([data_no, over_sampling], axis=0) . data_resampled[&quot;default payment next month&quot;].value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . data_resampled = data_resampled.reset_index(drop=True) X = data_resampled.drop(&quot;default payment next month&quot;,axis=1) y =data_resampled[&quot;default payment next month&quot;] . X = (X-X.min())/(X.max()-X.min()) X.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 . 0 0.080808 | 0.333333 | 0.666667 | 0.224138 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.093789 | 0.173637 | 0.095470 | 0.272928 | 0.001738 | 0.000891 | 0.001116 | 0.001610 | 0.002345 | 0.009458 | . 1 0.040404 | 0.333333 | 0.333333 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.113407 | 0.186809 | 0.109363 | 0.283685 | 0.002290 | 0.001199 | 0.001339 | 0.001771 | 0.002506 | 0.001892 | . 2 0.040404 | 0.333333 | 0.333333 | 0.620690 | 0.1 | 0.2 | 0.1 | 0.2 | 0.2 | 0.2 | ... | 0.106020 | 0.179863 | 0.099633 | 0.275681 | 0.002290 | 0.021779 | 0.011160 | 0.014493 | 0.001615 | 0.001284 | . 3 0.040404 | 0.166667 | 0.666667 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.117974 | 0.178407 | 0.100102 | 0.276367 | 0.002862 | 0.001078 | 0.000733 | 0.001610 | 0.002345 | 0.001513 | . 4 0.494949 | 0.166667 | 0.666667 | 0.137931 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.330672 | 0.671310 | 0.559578 | 0.625196 | 0.062961 | 0.023749 | 0.042409 | 0.032591 | 0.032237 | 0.026047 | . 5 rows × 22 columns . final_data =pd.concat([X,y],axis=1) final_data.to_csv(&quot;data_prepared.csv&quot;,index=False) . Build Model . import torch.nn.functional as F from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.metrics import accuracy_score . data = pd.read_csv(&quot;data_prepared.csv&quot;) data.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 0.080808 | 0.333333 | 0.666667 | 0.224138 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.173637 | 0.095470 | 0.272928 | 0.001738 | 0.000891 | 0.001116 | 0.001610 | 0.002345 | 0.009458 | 0 | . 1 0.040404 | 0.333333 | 0.333333 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.186809 | 0.109363 | 0.283685 | 0.002290 | 0.001199 | 0.001339 | 0.001771 | 0.002506 | 0.001892 | 0 | . 2 0.040404 | 0.333333 | 0.333333 | 0.620690 | 0.1 | 0.2 | 0.1 | 0.2 | 0.2 | 0.2 | ... | 0.179863 | 0.099633 | 0.275681 | 0.002290 | 0.021779 | 0.011160 | 0.014493 | 0.001615 | 0.001284 | 0 | . 3 0.040404 | 0.166667 | 0.666667 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.178407 | 0.100102 | 0.276367 | 0.002862 | 0.001078 | 0.000733 | 0.001610 | 0.002345 | 0.001513 | 0 | . 4 0.494949 | 0.166667 | 0.666667 | 0.137931 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.671310 | 0.559578 | 0.625196 | 0.062961 | 0.023749 | 0.042409 | 0.032591 | 0.032237 | 0.026047 | 0 | . 5 rows × 23 columns . X = data.drop(&quot;default payment next month&quot;,axis=1) y =data[&quot;default payment next month&quot;] . X_new , X_test,y_new,y_test =train_test_split(X,y,test_size=0.2,random_state=3) dev_per = X_test.shape[0]/X_new.shape[0] X_train,X_dev,y_train,y_dev = train_test_split(X_new,y_new,test_size=dev_per,random_state=3) . print(&quot;Training sets:&quot;,X_train.shape, y_train.shape) print(&quot;Validation sets:&quot;,X_dev.shape, y_dev.shape) print(&quot;Testing sets:&quot;,X_test.shape, y_test.shape) . Training sets: (28036, 22) (28036,) Validation sets: (9346, 22) (9346,) Testing sets: (9346, 22) (9346,) . X_dev_torch = torch.tensor(X_dev.values).float() y_dev_torch = torch.tensor(y_dev.values) X_test_torch = torch.tensor(X_test.values).float() y_test_torch = torch.tensor(y_test.values) . class Classifier(nn.Module): def __init__(self, input_size): super().__init__() self.hidden_1 = nn.Linear(input_size, 10) self.hidden_2 = nn.Linear(10, 10) self.hidden_3 = nn.Linear(10, 10) self.output = nn.Linear(10, 2) def forward(self, x): z = F.relu(self.hidden_1(x)) z = F.relu(self.hidden_2(z)) z = F.relu(self.hidden_3(z)) out = F.log_softmax(self.output(z), dim=1) return out . model = Classifier(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) epochs = 50 batch_size = 128 # for faster training procces/mini batch gradient descent . train_losses,dev_losses,train_acc,dev_acc =[],[],[],[] for epoch in range(epochs): X_,y_ =shuffle(X_train,y_train) running_loss=0 running_acc=0 iterations =0 for i in range(0,len(X_),batch_size): iterations +=1 b = i +batch_size X_batch = torch.tensor(X_.iloc[i:b,:].values).float() y_batch = torch.tensor(y_.iloc[i:b].values) pred = model(X_batch) loss = criterion(pred,y_batch) optimizer.zero_grad() loss.backward() optimizer.step() running_loss +=loss.item() ps = torch.exp(pred) top_p,top_class = ps.topk(1,dim=1) running_acc +=accuracy_score(y_batch,top_class) dev_loss =0 acc =0 with torch.no_grad(): pred_dev = model(X_dev_torch) dev_loss =criterion(pred_dev,y_dev_torch) ps_dev = torch.exp(pred_dev) top_p,top_class_dev = ps_dev.topk(1,dim=1) acc +=accuracy_score(y_dev_torch,top_class_dev) train_losses.append(running_loss/iterations) dev_losses.append(dev_loss) train_acc.append(running_acc/iterations) dev_acc.append(acc) print(&quot;Epoch: {}/{}.. &quot;.format(epoch+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/iterations), &quot;Validation Loss: {:.3f}.. &quot;.format(dev_loss), &quot;Training Accuracy: {:.3f}.. &quot;.format(running_acc/iterations), &quot;Validation Accuracy: {:.3f}&quot;.format(acc)) . Epoch: 1/50.. Training Loss: 0.675.. Validation Loss: 0.617.. Training Accuracy: 0.602.. Validation Accuracy: 0.663 Epoch: 2/50.. Training Loss: 0.607.. Validation Loss: 0.600.. Training Accuracy: 0.676.. Validation Accuracy: 0.686 Epoch: 3/50.. Training Loss: 0.598.. Validation Loss: 0.599.. Training Accuracy: 0.686.. Validation Accuracy: 0.688 Epoch: 4/50.. Training Loss: 0.595.. Validation Loss: 0.592.. Training Accuracy: 0.691.. Validation Accuracy: 0.695 Epoch: 5/50.. Training Loss: 0.592.. Validation Loss: 0.590.. Training Accuracy: 0.692.. Validation Accuracy: 0.693 Epoch: 6/50.. Training Loss: 0.589.. Validation Loss: 0.586.. Training Accuracy: 0.692.. Validation Accuracy: 0.693 Epoch: 7/50.. Training Loss: 0.585.. Validation Loss: 0.584.. Training Accuracy: 0.695.. Validation Accuracy: 0.696 Epoch: 8/50.. Training Loss: 0.583.. Validation Loss: 0.579.. Training Accuracy: 0.696.. Validation Accuracy: 0.700 Epoch: 9/50.. Training Loss: 0.576.. Validation Loss: 0.575.. Training Accuracy: 0.701.. Validation Accuracy: 0.703 Epoch: 10/50.. Training Loss: 0.572.. Validation Loss: 0.572.. Training Accuracy: 0.704.. Validation Accuracy: 0.707 Epoch: 11/50.. Training Loss: 0.572.. Validation Loss: 0.571.. Training Accuracy: 0.705.. Validation Accuracy: 0.709 Epoch: 12/50.. Training Loss: 0.570.. Validation Loss: 0.569.. Training Accuracy: 0.704.. Validation Accuracy: 0.709 Epoch: 13/50.. Training Loss: 0.569.. Validation Loss: 0.568.. Training Accuracy: 0.708.. Validation Accuracy: 0.709 Epoch: 14/50.. Training Loss: 0.567.. Validation Loss: 0.568.. Training Accuracy: 0.708.. Validation Accuracy: 0.709 Epoch: 15/50.. Training Loss: 0.566.. Validation Loss: 0.566.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 16/50.. Training Loss: 0.566.. Validation Loss: 0.567.. Training Accuracy: 0.708.. Validation Accuracy: 0.710 Epoch: 17/50.. Training Loss: 0.564.. Validation Loss: 0.566.. Training Accuracy: 0.711.. Validation Accuracy: 0.708 Epoch: 18/50.. Training Loss: 0.566.. Validation Loss: 0.564.. Training Accuracy: 0.707.. Validation Accuracy: 0.710 Epoch: 19/50.. Training Loss: 0.564.. Validation Loss: 0.564.. Training Accuracy: 0.712.. Validation Accuracy: 0.708 Epoch: 20/50.. Training Loss: 0.565.. Validation Loss: 0.564.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 21/50.. Training Loss: 0.562.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.707 Epoch: 22/50.. Training Loss: 0.563.. Validation Loss: 0.562.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 23/50.. Training Loss: 0.561.. Validation Loss: 0.564.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 24/50.. Training Loss: 0.562.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.709 Epoch: 25/50.. Training Loss: 0.561.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 26/50.. Training Loss: 0.562.. Validation Loss: 0.562.. Training Accuracy: 0.708.. Validation Accuracy: 0.707 Epoch: 27/50.. Training Loss: 0.562.. Validation Loss: 0.563.. Training Accuracy: 0.710.. Validation Accuracy: 0.709 Epoch: 28/50.. Training Loss: 0.560.. Validation Loss: 0.565.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 29/50.. Training Loss: 0.561.. Validation Loss: 0.562.. Training Accuracy: 0.711.. Validation Accuracy: 0.709 Epoch: 30/50.. Training Loss: 0.560.. Validation Loss: 0.564.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 31/50.. Training Loss: 0.561.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.709 Epoch: 32/50.. Training Loss: 0.560.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.706 Epoch: 33/50.. Training Loss: 0.559.. Validation Loss: 0.561.. Training Accuracy: 0.711.. Validation Accuracy: 0.709 Epoch: 34/50.. Training Loss: 0.559.. Validation Loss: 0.561.. Training Accuracy: 0.714.. Validation Accuracy: 0.710 Epoch: 35/50.. Training Loss: 0.560.. Validation Loss: 0.561.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 36/50.. Training Loss: 0.562.. Validation Loss: 0.559.. Training Accuracy: 0.710.. Validation Accuracy: 0.710 Epoch: 37/50.. Training Loss: 0.560.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.708 Epoch: 38/50.. Training Loss: 0.557.. Validation Loss: 0.563.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 39/50.. Training Loss: 0.561.. Validation Loss: 0.559.. Training Accuracy: 0.712.. Validation Accuracy: 0.711 Epoch: 40/50.. Training Loss: 0.560.. Validation Loss: 0.560.. Training Accuracy: 0.710.. Validation Accuracy: 0.708 Epoch: 41/50.. Training Loss: 0.559.. Validation Loss: 0.559.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 42/50.. Training Loss: 0.557.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 43/50.. Training Loss: 0.556.. Validation Loss: 0.559.. Training Accuracy: 0.714.. Validation Accuracy: 0.709 Epoch: 44/50.. Training Loss: 0.557.. Validation Loss: 0.563.. Training Accuracy: 0.714.. Validation Accuracy: 0.701 Epoch: 45/50.. Training Loss: 0.557.. Validation Loss: 0.558.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 46/50.. Training Loss: 0.560.. Validation Loss: 0.559.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 47/50.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 48/50.. Training Loss: 0.556.. Validation Loss: 0.558.. Training Accuracy: 0.714.. Validation Accuracy: 0.714 Epoch: 49/50.. Training Loss: 0.556.. Validation Loss: 0.564.. Training Accuracy: 0.714.. Validation Accuracy: 0.699 Epoch: 50/50.. Training Loss: 0.559.. Validation Loss: 0.560.. Training Accuracy: 0.712.. Validation Accuracy: 0.711 . fig = plt.subplots(figsize=(15,5)) plt.plot(train_losses,label=&quot;Training loss&quot;) plt.plot(dev_losses,label=&quot;Validation loss&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27ee31061c0&gt; . fig = plt.subplots(figsize=(15,5)) plt.plot(train_acc,label=&quot;Training accuracy&quot;) plt.plot(dev_acc,label=&quot;Validation accuracy&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efa576d30&gt; . We can see there is a change in the loss and accuracy accuracy during each epoch. We can do tune the learning rate for getting better result. You can do the experimentation by comparing the LR as well. This is a few steps in modelling using pytorch. You can see that we can do modelling by using Sequential or custom models using torch.nn. . # by adding hidden layer or epochs for training process class Classifier_Layer(nn.Module): def __init__(self, input_size): super().__init__() self.hidden_1 = nn.Linear(input_size, 10) self.hidden_2 = nn.Linear(10, 10) self.hidden_3 = nn.Linear(10, 10) self.hidden_4 = nn.Linear(10, 10) self.output = nn.Linear(10, 2) def forward(self, x): z = F.relu(self.hidden_1(x)) z = F.relu(self.hidden_2(z)) z = F.relu(self.hidden_3(z)) z = F.relu(self.hidden_4(z)) out = F.log_softmax(self.output(z), dim=1) return out . model = Classifier_Layer(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.01) epochs = 100 batch_size = 150 # for faster training procces/mini batch gradient descent . train_losses,dev_losses,train_acc,dev_acc =[],[],[],[] for epoch in range(epochs): X_,y_ =shuffle(X_train,y_train) running_loss=0 running_acc=0 iterations =0 for i in range(0,len(X_),batch_size): iterations +=1 b = i +batch_size X_batch = torch.tensor(X_.iloc[i:b,:].values).float() y_batch = torch.tensor(y_.iloc[i:b].values) pred = model(X_batch) loss = criterion(pred,y_batch) optimizer.zero_grad() loss.backward() optimizer.step() running_loss +=loss.item() ps = torch.exp(pred) top_p,top_class = ps.topk(1,dim=1) running_acc +=accuracy_score(y_batch,top_class) dev_loss =0 acc =0 with torch.no_grad(): pred_dev = model(X_dev_torch) dev_loss =criterion(pred_dev,y_dev_torch) ps_dev = torch.exp(pred_dev) top_p,top_class_dev = ps_dev.topk(1,dim=1) acc +=accuracy_score(y_dev_torch,top_class_dev) train_losses.append(running_loss/iterations) dev_losses.append(dev_loss) train_acc.append(running_acc/iterations) dev_acc.append(acc) print(&quot;Epoch: {}/{}.. &quot;.format(epoch+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/iterations), &quot;Validation Loss: {:.3f}.. &quot;.format(dev_loss), &quot;Training Accuracy: {:.3f}.. &quot;.format(running_acc/iterations), &quot;Validation Accuracy: {:.3f}&quot;.format(acc)) . Epoch: 1/100.. Training Loss: 0.618.. Validation Loss: 0.595.. Training Accuracy: 0.662.. Validation Accuracy: 0.688 Epoch: 2/100.. Training Loss: 0.594.. Validation Loss: 0.587.. Training Accuracy: 0.686.. Validation Accuracy: 0.690 Epoch: 3/100.. Training Loss: 0.588.. Validation Loss: 0.580.. Training Accuracy: 0.687.. Validation Accuracy: 0.693 Epoch: 4/100.. Training Loss: 0.584.. Validation Loss: 0.583.. Training Accuracy: 0.692.. Validation Accuracy: 0.692 Epoch: 5/100.. Training Loss: 0.583.. Validation Loss: 0.577.. Training Accuracy: 0.695.. Validation Accuracy: 0.694 Epoch: 6/100.. Training Loss: 0.580.. Validation Loss: 0.573.. Training Accuracy: 0.696.. Validation Accuracy: 0.703 Epoch: 7/100.. Training Loss: 0.575.. Validation Loss: 0.569.. Training Accuracy: 0.703.. Validation Accuracy: 0.704 Epoch: 8/100.. Training Loss: 0.574.. Validation Loss: 0.574.. Training Accuracy: 0.704.. Validation Accuracy: 0.709 Epoch: 9/100.. Training Loss: 0.571.. Validation Loss: 0.582.. Training Accuracy: 0.705.. Validation Accuracy: 0.708 Epoch: 10/100.. Training Loss: 0.571.. Validation Loss: 0.564.. Training Accuracy: 0.706.. Validation Accuracy: 0.712 Epoch: 11/100.. Training Loss: 0.569.. Validation Loss: 0.565.. Training Accuracy: 0.707.. Validation Accuracy: 0.712 Epoch: 12/100.. Training Loss: 0.569.. Validation Loss: 0.568.. Training Accuracy: 0.707.. Validation Accuracy: 0.705 Epoch: 13/100.. Training Loss: 0.566.. Validation Loss: 0.569.. Training Accuracy: 0.710.. Validation Accuracy: 0.706 Epoch: 14/100.. Training Loss: 0.566.. Validation Loss: 0.563.. Training Accuracy: 0.709.. Validation Accuracy: 0.713 Epoch: 15/100.. Training Loss: 0.566.. Validation Loss: 0.561.. Training Accuracy: 0.709.. Validation Accuracy: 0.711 Epoch: 16/100.. Training Loss: 0.564.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.715 Epoch: 17/100.. Training Loss: 0.563.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.713 Epoch: 18/100.. Training Loss: 0.566.. Validation Loss: 0.572.. Training Accuracy: 0.708.. Validation Accuracy: 0.701 Epoch: 19/100.. Training Loss: 0.564.. Validation Loss: 0.565.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 20/100.. Training Loss: 0.564.. Validation Loss: 0.561.. Training Accuracy: 0.712.. Validation Accuracy: 0.712 Epoch: 21/100.. Training Loss: 0.562.. Validation Loss: 0.566.. Training Accuracy: 0.710.. Validation Accuracy: 0.706 Epoch: 22/100.. Training Loss: 0.563.. Validation Loss: 0.571.. Training Accuracy: 0.711.. Validation Accuracy: 0.705 Epoch: 23/100.. Training Loss: 0.563.. Validation Loss: 0.563.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 24/100.. Training Loss: 0.559.. Validation Loss: 0.562.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 25/100.. Training Loss: 0.561.. Validation Loss: 0.564.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 26/100.. Training Loss: 0.561.. Validation Loss: 0.565.. Training Accuracy: 0.710.. Validation Accuracy: 0.704 Epoch: 27/100.. Training Loss: 0.560.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.714 Epoch: 28/100.. Training Loss: 0.560.. Validation Loss: 0.566.. Training Accuracy: 0.715.. Validation Accuracy: 0.701 Epoch: 29/100.. Training Loss: 0.559.. Validation Loss: 0.568.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 30/100.. Training Loss: 0.561.. Validation Loss: 0.566.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 31/100.. Training Loss: 0.559.. Validation Loss: 0.560.. Training Accuracy: 0.712.. Validation Accuracy: 0.716 Epoch: 32/100.. Training Loss: 0.558.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.715 Epoch: 33/100.. Training Loss: 0.560.. Validation Loss: 0.559.. Training Accuracy: 0.711.. Validation Accuracy: 0.713 Epoch: 34/100.. Training Loss: 0.558.. Validation Loss: 0.565.. Training Accuracy: 0.714.. Validation Accuracy: 0.705 Epoch: 35/100.. Training Loss: 0.560.. Validation Loss: 0.560.. Training Accuracy: 0.711.. Validation Accuracy: 0.713 Epoch: 36/100.. Training Loss: 0.557.. Validation Loss: 0.556.. Training Accuracy: 0.714.. Validation Accuracy: 0.715 Epoch: 37/100.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 38/100.. Training Loss: 0.557.. Validation Loss: 0.565.. Training Accuracy: 0.712.. Validation Accuracy: 0.714 Epoch: 39/100.. Training Loss: 0.559.. Validation Loss: 0.559.. Training Accuracy: 0.711.. Validation Accuracy: 0.714 Epoch: 40/100.. Training Loss: 0.559.. Validation Loss: 0.566.. Training Accuracy: 0.714.. Validation Accuracy: 0.713 Epoch: 41/100.. Training Loss: 0.556.. Validation Loss: 0.559.. Training Accuracy: 0.714.. Validation Accuracy: 0.713 Epoch: 42/100.. Training Loss: 0.557.. Validation Loss: 0.575.. Training Accuracy: 0.714.. Validation Accuracy: 0.716 Epoch: 43/100.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.715 Epoch: 44/100.. Training Loss: 0.555.. Validation Loss: 0.556.. Training Accuracy: 0.716.. Validation Accuracy: 0.715 Epoch: 45/100.. Training Loss: 0.557.. Validation Loss: 0.560.. Training Accuracy: 0.714.. Validation Accuracy: 0.715 Epoch: 46/100.. Training Loss: 0.556.. Validation Loss: 0.560.. Training Accuracy: 0.715.. Validation Accuracy: 0.714 Epoch: 47/100.. Training Loss: 0.556.. Validation Loss: 0.561.. Training Accuracy: 0.715.. Validation Accuracy: 0.709 Epoch: 48/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.714 Epoch: 49/100.. Training Loss: 0.555.. Validation Loss: 0.561.. Training Accuracy: 0.712.. Validation Accuracy: 0.713 Epoch: 50/100.. Training Loss: 0.555.. Validation Loss: 0.564.. Training Accuracy: 0.715.. Validation Accuracy: 0.703 Epoch: 51/100.. Training Loss: 0.556.. Validation Loss: 0.566.. Training Accuracy: 0.713.. Validation Accuracy: 0.699 Epoch: 52/100.. Training Loss: 0.555.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 53/100.. Training Loss: 0.553.. Validation Loss: 0.554.. Training Accuracy: 0.713.. Validation Accuracy: 0.716 Epoch: 54/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 55/100.. Training Loss: 0.555.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 56/100.. Training Loss: 0.552.. Validation Loss: 0.558.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 57/100.. Training Loss: 0.555.. Validation Loss: 0.557.. Training Accuracy: 0.713.. Validation Accuracy: 0.712 Epoch: 58/100.. Training Loss: 0.554.. Validation Loss: 0.565.. Training Accuracy: 0.716.. Validation Accuracy: 0.699 Epoch: 59/100.. Training Loss: 0.554.. Validation Loss: 0.560.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 60/100.. Training Loss: 0.554.. Validation Loss: 0.557.. Training Accuracy: 0.713.. Validation Accuracy: 0.711 Epoch: 61/100.. Training Loss: 0.552.. Validation Loss: 0.554.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 62/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 63/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.716 Epoch: 64/100.. Training Loss: 0.554.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.717 Epoch: 65/100.. Training Loss: 0.555.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 66/100.. Training Loss: 0.552.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.716 Epoch: 67/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.717.. Validation Accuracy: 0.713 Epoch: 68/100.. Training Loss: 0.552.. Validation Loss: 0.563.. Training Accuracy: 0.717.. Validation Accuracy: 0.705 Epoch: 69/100.. Training Loss: 0.552.. Validation Loss: 0.561.. Training Accuracy: 0.715.. Validation Accuracy: 0.705 Epoch: 70/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.714.. Validation Accuracy: 0.716 Epoch: 71/100.. Training Loss: 0.554.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 72/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 73/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 74/100.. Training Loss: 0.552.. Validation Loss: 0.562.. Training Accuracy: 0.716.. Validation Accuracy: 0.696 Epoch: 75/100.. Training Loss: 0.553.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.715 Epoch: 76/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.717.. Validation Accuracy: 0.715 Epoch: 77/100.. Training Loss: 0.550.. Validation Loss: 0.556.. Training Accuracy: 0.718.. Validation Accuracy: 0.714 Epoch: 78/100.. Training Loss: 0.551.. Validation Loss: 0.565.. Training Accuracy: 0.717.. Validation Accuracy: 0.700 Epoch: 79/100.. Training Loss: 0.552.. Validation Loss: 0.558.. Training Accuracy: 0.718.. Validation Accuracy: 0.714 Epoch: 80/100.. Training Loss: 0.552.. Validation Loss: 0.555.. Training Accuracy: 0.717.. Validation Accuracy: 0.712 Epoch: 81/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 82/100.. Training Loss: 0.551.. Validation Loss: 0.557.. Training Accuracy: 0.715.. Validation Accuracy: 0.710 Epoch: 83/100.. Training Loss: 0.550.. Validation Loss: 0.553.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 84/100.. Training Loss: 0.550.. Validation Loss: 0.557.. Training Accuracy: 0.718.. Validation Accuracy: 0.711 Epoch: 85/100.. Training Loss: 0.551.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.715 Epoch: 86/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.716.. Validation Accuracy: 0.704 Epoch: 87/100.. Training Loss: 0.553.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.717 Epoch: 88/100.. Training Loss: 0.551.. Validation Loss: 0.554.. Training Accuracy: 0.716.. Validation Accuracy: 0.714 Epoch: 89/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 90/100.. Training Loss: 0.549.. Validation Loss: 0.554.. Training Accuracy: 0.717.. Validation Accuracy: 0.718 Epoch: 91/100.. Training Loss: 0.551.. Validation Loss: 0.556.. Training Accuracy: 0.716.. Validation Accuracy: 0.717 Epoch: 92/100.. Training Loss: 0.550.. Validation Loss: 0.554.. Training Accuracy: 0.717.. Validation Accuracy: 0.715 Epoch: 93/100.. Training Loss: 0.549.. Validation Loss: 0.559.. Training Accuracy: 0.720.. Validation Accuracy: 0.705 Epoch: 94/100.. Training Loss: 0.549.. Validation Loss: 0.558.. Training Accuracy: 0.717.. Validation Accuracy: 0.706 Epoch: 95/100.. Training Loss: 0.549.. Validation Loss: 0.559.. Training Accuracy: 0.717.. Validation Accuracy: 0.714 Epoch: 96/100.. Training Loss: 0.549.. Validation Loss: 0.553.. Training Accuracy: 0.719.. Validation Accuracy: 0.718 Epoch: 97/100.. Training Loss: 0.551.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.718 Epoch: 98/100.. Training Loss: 0.551.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.711 Epoch: 99/100.. Training Loss: 0.549.. Validation Loss: 0.557.. Training Accuracy: 0.719.. Validation Accuracy: 0.716 Epoch: 100/100.. Training Loss: 0.547.. Validation Loss: 0.563.. Training Accuracy: 0.719.. Validation Accuracy: 0.714 . fig = plt.subplots(figsize=(15,5)) plt.plot(train_losses,label=&quot;Training loss&quot;) plt.plot(dev_losses,label=&quot;Validation loss&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efa57d460&gt; . fig = plt.subplots(figsize=(15,5)) plt.plot(train_acc,label=&quot;Training accuracy&quot;) plt.plot(dev_acc,label=&quot;Validation accuracy&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efb09d940&gt; . You can experiment by changing the architecture of the model .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/pytorch/neural%20network/activation%20function/loss/2022/03/23/NN-for-new-user-of-Pytorch.html",
            "relUrl": "/pytorch/neural%20network/activation%20function/loss/2022/03/23/NN-for-new-user-of-Pytorch.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "just random datacamp competition using advanced data visualization to extract electricity price in Australia",
            "content": "Understanding the local electricity market . 📖 Background You work for an energy company in Australia. Your company builds solar panel arrays and then sells the energy they produce to industrial customers. The company wants to expand to the city of Melbourne in the state of Victoria. . Prices and demand for electricity change every day. Customers pay for the energy received using a formula based on the local energy market&#39;s daily price. . Your company&#39;s pricing committee wants your team to estimate energy prices for the next 12-18 months to use those prices as the basis for contract negotiations. . In addition, the VP of strategy is researching investing in storage capacity (i.e., batteries) as a new source of revenue. The plan is to store some of the energy produced by the solar panels when pricing conditions are unfavorable and sell it by the next day on the open market if the prices are higher. . &#128190; The data . You have access to over five years of energy price and demand data (source): . &quot;date&quot; - from January 1, 2015, to October 6, 2020. | &quot;demand&quot; - daily electricity demand in MWh. | &quot;price&quot; - recommended retail price in AUD/MWh. | &quot;demand_pos_price&quot; - total daily demand at a positive price in MWh. | &quot;price_positive&quot; - average positive price, weighted by the corresponding intraday demand in AUD/MWh. | &quot;demand_neg_price&quot; - total daily demand at a negative price in MWh. | &quot;price_negative&quot; - average negative price, weighted by the corresponding intraday demand in AUD/MWh. | &quot;frac_neg_price&quot; - the fraction of the day when the demand traded at a negative price. | &quot;min_temperature&quot; - minimum temperature during the day in Celsius. | &quot;max_temperature&quot; - maximum temperature during the day in Celsius. | &quot;solar_exposure&quot; - total daily sunlight energy in MJ/m^2. | &quot;rainfall&quot; - daily rainfall in mm. | &quot;school_day&quot; - &quot;Y&quot; if that day was a school day, &quot;N&quot; otherwise. | &quot;holiday&quot; - &quot;Y&quot; if the day was a state or national holiday, &quot;N&quot; otherwise. | . Note: The price was negative during some intraday intervals, so energy producers were paying buyers rather than vice-versa. . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import xgboost as xgb from xgboost import plot_importance, plot_tree from sklearn.metrics import mean_squared_error, mean_absolute_error from sklearn.model_selection import RandomizedSearchCV # Set defaults of the notebook sns.set(font=&quot;&#39;Source Code Pro&#39;, monospace&quot;) plt.rcParams[&quot;font.family&quot;] = &quot;&#39;Source Code Pro&#39;, monospace&quot; import warnings warnings.filterwarnings(&quot;ignore&quot;) # Color Palettes treasure_colors = [&quot;#703728&quot;, &quot;#c86b25&quot;, &quot;#dc9555&quot;, &quot;#fed56f&quot;, &quot;#c89a37&quot;] pirate_colors = [&quot;#010307&quot;, &quot;#395461&quot;, &quot;#449FAF&quot;, &quot;#B1F4FC&quot;, &quot;#F4D499&quot;, &quot;#835211&quot;] . df_electricity = pd.read_csv(&#39;./data/energy_demand.csv&#39;, parse_dates=[&#39;date&#39;],index_col =[&quot;date&quot;]) df_electricity.head() . demand price demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day holiday . date . 2015-01-01 99635.030 | 25.633696 | 97319.240 | 26.415953 | 2315.790 | -7.240000 | 0.020833 | 13.3 | 26.9 | 23.6 | 0.0 | N | Y | . 2015-01-02 129606.010 | 33.138988 | 121082.015 | 38.837661 | 8523.995 | -47.809777 | 0.062500 | 15.4 | 38.8 | 26.8 | 0.0 | N | N | . 2015-01-03 142300.540 | 34.564855 | 142300.540 | 34.564855 | 0.000 | 0.000000 | 0.000000 | 20.0 | 38.2 | 26.5 | 0.0 | N | N | . 2015-01-04 104330.715 | 25.005560 | 104330.715 | 25.005560 | 0.000 | 0.000000 | 0.000000 | 16.3 | 21.4 | 25.2 | 4.2 | N | N | . 2015-01-05 118132.200 | 26.724176 | 118132.200 | 26.724176 | 0.000 | 0.000000 | 0.000000 | 15.0 | 22.0 | 30.7 | 0.0 | N | N | . &#128170; Competition challenge . Create a report that covers the following: . How do energy prices change throughout the year? Are there any patterns by season or month of the year? | Build a forecast of daily energy prices the company can use as the basis of its financial planning. | Provide guidance on how much revenue the energy storage venture could generate per year using retail prices and a 70MWh storage system. | &#8987;&#65039; Exploratory Data Analysis! . color_pal = [&quot;#F8766D&quot;, &quot;#D39200&quot;, &quot;#93AA00&quot;, &quot;#00BA38&quot;, &quot;#00C19F&quot;, &quot;#00B9E3&quot;, &quot;#619CFF&quot;, &quot;#DB72FB&quot;] _ = df_electricity[&quot;price&quot;].plot(style=&#39;.&#39;, figsize=(15,5), color=color_pal[0], title=&#39;Daily electricity price in Victoria from January 2015 to October 2020&#39;) plt.ylim(0,300) plt.ylabel(&quot;Price&quot;) . Text(0, 0.5, &#39;Price&#39;) . Insights &#128205; : The distribution of daily electricity price . Based on the plot above show: . The distribution of daily electricity price in Victoria from January 2015 to October 2020 . | There is a significant electricity price change especially in 2016 to 2020. | (df_electricity.describe() .style .highlight_max(axis=0,color=&quot;#c07fef&quot;) .highlight_min(axis=0,color=&quot;#00FF00&quot;) .set_caption(&quot;Statistics of Electricity in Australia for 2015-2020&quot;)) . Statistics of Electricity in Australia for 2015-2020 demand price demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall . count 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2106.000000 | 2105.000000 | 2103.000000 | . mean 120035.476503 | 76.079554 | 119252.305055 | 76.553847 | 783.171448 | -2.686052 | 0.008547 | 11.582289 | 20.413200 | 14.743373 | 1.505944 | . std 13747.993761 | 130.246805 | 14818.631319 | 130.114184 | 3578.920686 | 19.485432 | 0.039963 | 4.313711 | 6.288693 | 7.945527 | 4.307897 | . min 85094.375000 | -6.076028 | 41988.240000 | 13.568986 | 0.000000 | -342.220000 | 0.000000 | 0.600000 | 9.000000 | 0.700000 | 0.000000 | . 25% 109963.650000 | 38.707040 | 109246.250000 | 39.117361 | 0.000000 | 0.000000 | 0.000000 | 8.500000 | 15.525000 | 8.200000 | 0.000000 | . 50% 119585.912500 | 66.596738 | 119148.082500 | 66.869058 | 0.000000 | 0.000000 | 0.000000 | 11.300000 | 19.100000 | 12.700000 | 0.000000 | . 75% 130436.006250 | 95.075012 | 130119.477500 | 95.130181 | 0.000000 | 0.000000 | 0.000000 | 14.600000 | 23.900000 | 20.700000 | 0.800000 | . max 170653.840000 | 4549.645105 | 170653.840000 | 4549.645105 | 57597.595000 | 0.000000 | 0.625000 | 28.000000 | 43.500000 | 33.300000 | 54.600000 | . fig, ax = plt.subplots(4, 1, figsize = (15, 20)) ax[0].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) ax[1].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) ax[2].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) ax[3].tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=13) df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;demand_pos_price&#39;, lw = 1, ax = ax[0]) ax[0].set_title(&quot;Daily electricity demand in Victoria from January 2015 to October 2020&quot;) ax[0].set_ylabel(&quot;Demand Positive Price [MWh]&quot;) ax[0].get_legend().remove() df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;demand_neg_price&#39;, lw = 1, color=&#39;red&#39;, ax = ax[1]) ax[1].set_ylabel(&quot;Demand Negative Price [MWh]&quot;) ax[1].get_legend().remove() df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;price_positive&#39;, lw = 1, color=&#39;red&#39;, ax = ax[2]) ax[2].set_ylabel(&quot;Price Positive [AUD$/MWh]&quot;) ax[2].get_legend().remove() df_electricity.reset_index().plot(x=&#39;date&#39;, y=&#39;price_negative&#39;, lw = 1, color=&#39;red&#39;, ax = ax[3]) ax[3].set_ylabel(&quot;Price Negative [AUD$/MWh]&quot;) ax[3].get_legend().remove() . Insights &#128205; : Daily electricity demand in Victoria from January 2015 to October 2020 . Based on the plot above show: . Demand positive price shows some seasonality based on plot shown above&lt;/span&gt;. Maybe this feature may benefit our model. | Demand negative price We can see there is a significant difference electricity price through maximum price which is very far from the mean and standard deviation. | Positive price mostly with small values except for some values that is greater than 1000. | Price negative ocurred in each year with variation of values. | Make a correlation plot . def heatmap(x, y, size): fig, ax = plt.subplots() # Mapping from column names to integer coordinates x_labels = [v for v in sorted(x.unique())] y_labels = [v for v in sorted(y.unique())] x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} size_scale = 500 ax.scatter( x=x.map(x_to_num), # Use mapping for x y=y.map(y_to_num), # Use mapping for y s=size * size_scale, # Vector of square sizes, proportional to size parameter marker=&#39;s&#39; # Use square as scatterplot marker ) # Show column labels on the axes ax.set_xticks([x_to_num[v] for v in x_labels]) ax.set_xticklabels(x_labels, rotation=45, horizontalalignment=&#39;right&#39;) ax.set_yticks([y_to_num[v] for v in y_labels]) ax.set_yticklabels(y_labels) data = df_electricity columns = df_electricity.columns corr = data[columns].corr() corr = pd.melt(corr.reset_index(), id_vars=&#39;index&#39;) # Unpivot the dataframe, so we can get pair of arrays for x and y corr.columns = [&#39;x&#39;, &#39;y&#39;, &#39;value&#39;] heatmap( x=corr[&#39;x&#39;], y=corr[&#39;y&#39;], size=corr[&#39;value&#39;].abs() ) . Insights &#128205; : Correlation Matrix . Based on the plot above show: . Price target has higher correlation with price_positive, demand, demand_pos_price and frac_negative_price. | plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_electricity.groupby([df_electricity.index.year])): ax = plt.subplot(2, 3, i+1, ymargin=0.5) #print(df.num_sold.values.shape, df.num_sold.values) ax.plot(df.price) ax.set_title(combi) plt.xticks(rotation=45) plt.tight_layout(h_pad=3.0) plt.suptitle(&#39;Energy prices change for 2015-2020&#39;, y=1.03) plt.show() . Insights &#128205; : Electricity price change . Based on the plot above show: . There is a significant increase of electricity price in December 2015 after a stable price during January-September.&gt; 2. The electricity price tends to be stable in range 50-150 AUD except in February where there is an increase of energy price.&gt; 2. For 2016, significant increase of price occured in January and March followed by stable price during April-December&gt; 3. For 2018-2019, There is a significant increase of electricity price around January-February. | plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_electricity.groupby([df_electricity.index.year,&quot;holiday&quot;])): ax = plt.subplot(3,4, i+1, ymargin=0.5) #print(df.num_sold.values.shape, df.num_sold.values) ax.plot(df.price) ax.set_title(combi) plt.xticks(rotation=45) plt.suptitle(&#39;Energy prices change for 2015-2020 on holiday Season&#39;, y=1.03) . Text(0.5, 1.03, &#39;Energy prices change for 2015-2020 on holiday Season&#39;) . Insights &#128205; : Electricity prices change for 2015-2020 on holiday Season . Based on the plot above show: . There is an unstable price pattern of energy price in holiday season.&gt; 2. For annual Non-Holiday Season, There is an increase of energy price in the beginning of the year | plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_electricity.groupby([df_electricity.index.year,&quot;school_day&quot;])): ax = plt.subplot(3,4, i+1, ymargin=0.5) #print(df.num_sold.values.shape, df.num_sold.values) ax.plot(df.price) ax.set_title(combi) plt.xticks(rotation=40) plt.suptitle(&#39;Energy prices change for 2015-2020 on School Season&#39;, y=1.03) . Text(0.5, 1.03, &#39;Energy prices change for 2015-2020 on School Season&#39;) . Insights &#128205; : Electricity price change for 2015-2020 on School Season . Based on the plot above show: . There is a significant increase of electricity price in the end of 2015 on school season&gt; 2. There is a similar pattern of the increase of electricity price on school season in February-March in 2018, 2019, 2020&gt; 2. For annual non-holiday Season, There is an increase of electricity price in the beginning of the year in 2016, 2018, 2019 | Splitting the data based on time . split_date = &#39;01-Jan-2019&#39; df_train = df_electricity.loc[df_electricity.index &lt;= split_date].copy() df_test = df_electricity.loc[df_electricity.index &gt; split_date].copy() . def create_features(df, label=None): &quot;&quot;&quot; Creates time series features from datetime index &quot;&quot;&quot; df[&#39;date&#39;] = df.index df[&#39;hour&#39;] = df[&#39;date&#39;].dt.hour df[&#39;dayofweek&#39;] = df[&#39;date&#39;].dt.dayofweek df[&#39;quarter&#39;] = df[&#39;date&#39;].dt.quarter df[&#39;month&#39;] = df[&#39;date&#39;].dt.month df[&#39;year&#39;] = df[&#39;date&#39;].dt.year df[&#39;dayofyear&#39;] = df[&#39;date&#39;].dt.dayofyear df[&#39;dayofmonth&#39;] = df[&#39;date&#39;].dt.day df[&#39;weekofyear&#39;] = df[&#39;date&#39;].dt.isocalendar().week.astype(np.int64) df[&quot;school_day&quot;] = pd.get_dummies(df[&quot;school_day&quot;]) df[&quot;school_day&quot;] = pd.get_dummies(df[&quot;school_day&quot;]) df[&#39;price_7_days_lag&#39;] = df[&#39;price&#39;].shift(7) df[&#39;price_15_days_lag&#39;] = df[&#39;price&#39;].shift(15) df[&#39;price_30_days_lag&#39;] = df[&#39;price&#39;].shift(30) df[&#39;price_7_days_mean&#39;] = df[&#39;price&#39;].rolling(window = 7).mean() df[&#39;price_15_days_mean&#39;] = df[&#39;price&#39;].rolling(window = 15).mean() df[&#39;price_30_days_mean&#39;] = df[&#39;price&#39;].rolling(window = 30).mean() df[&#39;price_7_days_std&#39;] = df[&#39;price&#39;].rolling(window = 7).std() df[&#39;price_15_days_std&#39;] = df[&#39;price&#39;].rolling(window = 15).std() df[&#39;price_30_days_std&#39;] = df[&#39;price&#39;].rolling(window = 30).std() df[&#39;price_7_days_max&#39;] = df[&#39;price&#39;].rolling(window = 7).max() df[&#39;price_15_days_max&#39;] = df[&#39;price&#39;].rolling(window = 15).max() df[&#39;price_30_days_max&#39;] = df[&#39;price&#39;].rolling(window = 30).max() df[&#39;price_7_days_min&#39;] = df[&#39;price&#39;].rolling(window = 7).min() df[&#39;price_15_days_min&#39;] = df[&#39;price&#39;].rolling(window = 15).min() df[&#39;price_30_days_min&#39;] = df[&#39;price&#39;].rolling(window = 30).min() cols = [&#39;hour&#39;,&#39;dayofweek&#39;,&#39;dayofyear&#39;,&#39;quarter&#39;,&#39;month&#39;,&#39;year&#39;,&#39;dayofmonth&#39;,&#39;weekofyear&#39;,&#39;price_positive&#39;,&#39;demand_pos_price&#39;,&#39;demand&#39;,&#39;demand_neg_price&#39;,&#39;price_negative&#39;,&#39;frac_neg_price&#39;] #cols = [&#39;hour&#39;,&#39;dayofweek&#39;,&#39;quarter&#39;,&#39;month&#39;,&#39;year&#39;,&#39;dayofyear&#39;,&#39;dayofmonth&#39;,&#39;weekofyear&#39;,&#39;demand_pos_price&#39;,] for d in (&#39;7&#39;, &#39;15&#39;, &#39;30&#39;): for c in (&#39;lag&#39;, &#39;mean&#39;, &#39;std&#39;, &#39;max&#39;, &#39;min&#39;): cols.append(f&#39;price_{d}_days_{c}&#39;) X = df[cols] if label: y = df[label] return X, y return X . X_train, y_train = create_features(df_train, label=&#39;price&#39;) X_test, y_test = create_features(df_test, label=&#39;price&#39;) . reg = xgb.XGBRegressor(n_estimators=100,eta=0.1) fitted_xgb_model = reg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], early_stopping_rounds=50, verbose=True) . [0] validation_0-rmse:78.10894 validation_1-rmse:232.43314 [1] validation_0-rmse:71.00056 validation_1-rmse:226.20828 [2] validation_0-rmse:64.59680 validation_1-rmse:220.67780 [3] validation_0-rmse:58.83662 validation_1-rmse:215.75482 [4] validation_0-rmse:53.66943 validation_1-rmse:211.30708 [5] validation_0-rmse:49.02540 validation_1-rmse:207.36438 [6] validation_0-rmse:44.84807 validation_1-rmse:203.70424 [7] validation_0-rmse:41.09291 validation_1-rmse:200.41937 [8] validation_0-rmse:37.70780 validation_1-rmse:197.01787 [9] validation_0-rmse:34.63074 validation_1-rmse:193.86409 [10] validation_0-rmse:31.85113 validation_1-rmse:190.97063 [11] validation_0-rmse:29.33829 validation_1-rmse:188.28787 [12] validation_0-rmse:27.05840 validation_1-rmse:185.79985 [13] validation_0-rmse:24.98889 validation_1-rmse:183.49159 [14] validation_0-rmse:23.10552 validation_1-rmse:181.34113 [15] validation_0-rmse:21.39121 validation_1-rmse:179.33682 [16] validation_0-rmse:19.83474 validation_1-rmse:178.55098 [17] validation_0-rmse:18.41598 validation_1-rmse:177.83699 [18] validation_0-rmse:17.11959 validation_1-rmse:177.18620 [19] validation_0-rmse:15.93410 validation_1-rmse:176.59129 [20] validation_0-rmse:14.84714 validation_1-rmse:176.04675 [21] validation_0-rmse:13.85011 validation_1-rmse:175.54684 [22] validation_0-rmse:12.93431 validation_1-rmse:175.08743 [23] validation_0-rmse:12.09328 validation_1-rmse:174.66530 [24] validation_0-rmse:11.32025 validation_1-rmse:173.82794 [25] validation_0-rmse:10.60783 validation_1-rmse:173.03954 [26] validation_0-rmse:9.95008 validation_1-rmse:172.29640 [27] validation_0-rmse:9.34216 validation_1-rmse:171.59558 [28] validation_0-rmse:8.77918 validation_1-rmse:170.93398 [29] validation_0-rmse:8.25816 validation_1-rmse:170.30927 [30] validation_0-rmse:7.77358 validation_1-rmse:169.71977 [31] validation_0-rmse:7.32364 validation_1-rmse:169.16220 [32] validation_0-rmse:6.90481 validation_1-rmse:168.63498 [33] validation_0-rmse:6.51409 validation_1-rmse:168.13646 [34] validation_0-rmse:6.14908 validation_1-rmse:167.66449 [35] validation_0-rmse:5.80786 validation_1-rmse:167.21773 [36] validation_0-rmse:5.48851 validation_1-rmse:166.79488 [37] validation_0-rmse:5.18929 validation_1-rmse:166.39388 [38] validation_0-rmse:4.90825 validation_1-rmse:166.01436 [39] validation_0-rmse:4.64466 validation_1-rmse:165.65491 [40] validation_0-rmse:4.39683 validation_1-rmse:165.31377 [41] validation_0-rmse:4.16372 validation_1-rmse:164.99037 [42] validation_0-rmse:3.94428 validation_1-rmse:164.68390 [43] validation_0-rmse:3.73747 validation_1-rmse:164.39325 [44] validation_0-rmse:3.54249 validation_1-rmse:164.11774 [45] validation_0-rmse:3.35867 validation_1-rmse:163.85646 [46] validation_0-rmse:3.18519 validation_1-rmse:163.60869 [47] validation_0-rmse:3.02139 validation_1-rmse:163.37366 [48] validation_0-rmse:2.86655 validation_1-rmse:163.15067 [49] validation_0-rmse:2.72040 validation_1-rmse:162.93970 [50] validation_0-rmse:2.58236 validation_1-rmse:162.73958 [51] validation_0-rmse:2.45181 validation_1-rmse:162.54944 [52] validation_0-rmse:2.32823 validation_1-rmse:162.36912 [53] validation_0-rmse:2.21122 validation_1-rmse:162.19797 [54] validation_0-rmse:2.10046 validation_1-rmse:162.03554 [55] validation_0-rmse:1.99561 validation_1-rmse:161.88142 [56] validation_0-rmse:1.89622 validation_1-rmse:161.73517 [57] validation_0-rmse:1.80210 validation_1-rmse:161.59613 [58] validation_0-rmse:1.71306 validation_1-rmse:161.46413 [59] validation_0-rmse:1.62860 validation_1-rmse:161.33887 [60] validation_0-rmse:1.54868 validation_1-rmse:161.21994 [61] validation_0-rmse:1.47296 validation_1-rmse:161.10706 [62] validation_0-rmse:1.40104 validation_1-rmse:160.99974 [63] validation_0-rmse:1.33303 validation_1-rmse:160.89801 [64] validation_0-rmse:1.26847 validation_1-rmse:160.80133 [65] validation_0-rmse:1.20725 validation_1-rmse:160.70961 [66] validation_0-rmse:1.14925 validation_1-rmse:160.62248 [67] validation_0-rmse:1.09428 validation_1-rmse:160.53976 [68] validation_0-rmse:1.04232 validation_1-rmse:160.46124 [69] validation_0-rmse:0.99303 validation_1-rmse:160.38670 [70] validation_0-rmse:0.94642 validation_1-rmse:160.31592 [71] validation_0-rmse:0.90220 validation_1-rmse:160.24872 [72] validation_0-rmse:0.86030 validation_1-rmse:160.18489 [73] validation_0-rmse:0.82075 validation_1-rmse:160.12427 [74] validation_0-rmse:0.78316 validation_1-rmse:160.06668 [75] validation_0-rmse:0.74781 validation_1-rmse:160.01205 [76] validation_0-rmse:0.71458 validation_1-rmse:159.96014 [77] validation_0-rmse:0.68281 validation_1-rmse:159.91087 [78] validation_0-rmse:0.65270 validation_1-rmse:159.86403 [79] validation_0-rmse:0.62447 validation_1-rmse:159.81957 [80] validation_0-rmse:0.59772 validation_1-rmse:159.77733 [81] validation_0-rmse:0.57273 validation_1-rmse:159.73723 [82] validation_0-rmse:0.54923 validation_1-rmse:159.69914 [83] validation_0-rmse:0.52638 validation_1-rmse:159.66295 [84] validation_0-rmse:0.50531 validation_1-rmse:159.62856 [85] validation_0-rmse:0.48540 validation_1-rmse:159.59593 [86] validation_0-rmse:0.46696 validation_1-rmse:159.56493 [87] validation_0-rmse:0.44928 validation_1-rmse:159.53546 [88] validation_0-rmse:0.43296 validation_1-rmse:159.50746 [89] validation_0-rmse:0.41758 validation_1-rmse:159.48086 [90] validation_0-rmse:0.40334 validation_1-rmse:159.45570 [91] validation_0-rmse:0.38988 validation_1-rmse:159.43173 [92] validation_0-rmse:0.37735 validation_1-rmse:159.40901 [93] validation_0-rmse:0.36581 validation_1-rmse:159.38736 [94] validation_0-rmse:0.35467 validation_1-rmse:159.36688 [95] validation_0-rmse:0.34369 validation_1-rmse:159.34740 [96] validation_0-rmse:0.33422 validation_1-rmse:159.32890 [97] validation_0-rmse:0.32500 validation_1-rmse:159.31133 [98] validation_0-rmse:0.31654 validation_1-rmse:159.29463 [99] validation_0-rmse:0.30846 validation_1-rmse:159.27872 . plt.plot(fitted_xgb_model.evals_result()[&#39;validation_0&#39;] [&#39;rmse&#39;]) plt.plot(fitted_xgb_model.evals_result()[&#39;validation_1&#39;] [&#39;rmse&#39;]) plt.ylabel(&#39;RMSE&#39;, fontsize=14) plt.xlabel(&#39;Price&#39;, fontsize=14) plt.legend([&#39;Train&#39;, &#39;Val&#39;], loc=&#39;upper right&#39;) . &lt;matplotlib.legend.Legend at 0x7f0267b412e0&gt; . We can see the comparison between training rmse and validation rmse score where the is an underfitting problem in our case where maybe due to outlier electricity price in 2019-2020 . feature_important = reg.get_booster().get_score(importance_type=&#39;weight&#39;) keys = list(feature_important.keys()) values = list(feature_important.values()) data = pd.DataFrame(data=values, index=keys, columns=[&quot;score&quot;]).sort_values(by = &quot;score&quot;, ascending=False) data.nlargest(10, columns=&quot;score&quot;).plot(kind=&#39;barh&#39;, figsize = (20,10)) ## plot top 10 features . &lt;AxesSubplot:&gt; . Based on the feature importances extracted from the model, price_positive feature, demand_neg_price, dayofweek,dayofyear and dayofyear show the most 5th of feature importances. . df_test[&#39;price_prediction&#39;] = reg.predict(X_test) price_all = pd.concat([df_test, df_train], sort=False) . _ = price_all[[&#39;price&#39;,&#39;price_prediction&#39;]].plot(figsize=(15, 5)) plt.ylim(0,1000) . (0.0, 1000.0) . f, ax = plt.subplots(1) f.set_figheight(5) f.set_figwidth(15) _ = price_all[[&#39;price&#39;,&#39;price_prediction&#39;]].plot(ax=ax,style=[&#39;-&#39;,&#39;.&#39;]) ax.set_xbound(lower=&#39;01-05-2019&#39;, upper=&#39;02-10-2020&#39;) ax.set_ylim(0, 2000) plot = plt.suptitle(&#39;May 2019 Forecast vs Actuals&#39;) . Insights &#128205; : Prediction error price between real price and prediction price . Our model is generalizable for the next 12-18 months based on the plot above. In Machine Learninig, It is a mandatory to make our model to be generalizable as possible so that it can predict the real/actual value in real situation. There is a few errors around february and in the beginining of January 2020. If it can predict the same value for actual demand, it is a overfitting problem. We have to take into account this case to make better prediction that can mimic the actual value with fewer errors. . mean_squared_error(y_true=df_test[&#39;price&#39;],y_pred=df_test[&#39;price_prediction&#39;]) . 25369.714397883905 . mean_absolute_error(y_true=df_test[&#39;price&#39;],y_pred=df_test[&#39;price_prediction&#39;]) . 10.73348890658493 . df_test[&#39;error&#39;] = df_test[&#39;price&#39;] - df_test[&#39;price_prediction&#39;] df_test[&#39;abs_error&#39;] = df_test[&#39;error&#39;].apply(np.abs) error_by_day = df_test.groupby([&#39;year&#39;,&#39;month&#39;,&#39;dayofmonth&#39;]).mean()[[&#39;price&#39;,&#39;price_prediction&#39;,&#39;error&#39;,&#39;abs_error&#39;]] . (error_by_day.sort_values(&#39;error&#39;, ascending=True).head(10) .style .bar(subset=&quot;error&quot;,color=&quot;#c07fef&quot;,vmax=error_by_day.error.quantile(0.95)) .highlight_max(axis=0,color=&quot;#fef70c&quot;) ) . price price_prediction error abs_error . year month dayofmonth . 2019 1 22 278.777743 | 350.801331 | -72.023587 | 72.023587 | . 12 30 295.829202 | 342.720764 | -46.891562 | 46.891562 | . 2020 1 30 1044.447303 | 1078.613281 | -34.165979 | 34.165979 | . 10 2 -6.076028 | 21.710295 | -27.786323 | 27.786323 | . 3 -1.983471 | 21.918243 | -23.901714 | 23.901714 | . 1 23 -1.761423 | 19.056923 | -20.818346 | 20.818346 | . 2019 2 2 240.954524 | 257.686920 | -16.732396 | 16.732396 | . 8 31 79.347198 | 91.819000 | -12.471802 | 12.471802 | . 2020 8 30 9.421019 | 20.588486 | -11.167467 | 11.167467 | . 1 4 28.042231 | 38.049507 | -10.007276 | 10.007276 | . (error_by_day.sort_values(&#39;abs_error&#39;, ascending=False).head(10) .style .bar(subset=&quot;abs_error&quot;,color=&quot;#c07fef&quot;,vmax=error_by_day.abs_error.quantile(0.95)) .highlight_max(axis=0,color=&quot;#fef70c&quot;) ) . price price_prediction error abs_error . year month dayofmonth . 2019 1 24 4549.645105 | 1078.613281 | 3471.031824 | 3471.031824 | . 2020 1 31 2809.437516 | 812.882507 | 1996.555008 | 1996.555008 | . 2019 3 1 1284.799876 | 812.882507 | 471.917369 | 471.917369 | . 1 25 906.437232 | 645.617493 | 260.819740 | 260.819740 | . 22 278.777743 | 350.801331 | -72.023587 | 72.023587 | . 12 30 295.829202 | 342.720764 | -46.891562 | 46.891562 | . 2020 1 30 1044.447303 | 1078.613281 | -34.165979 | 34.165979 | . 10 2 -6.076028 | 21.710295 | -27.786323 | 27.786323 | . 3 -1.983471 | 21.918243 | -23.901714 | 23.901714 | . 2019 8 13 267.347650 | 246.398193 | 20.949457 | 20.949457 | . Insights &#128205; : Notice anything about the over forecasted days? . #1 worst day - February 2nd, 2020. | #6 worst day - March 25, 2019. | . Looks like our model influenced by outliers . . (error_by_day.sort_values(&#39;abs_error&#39;, ascending=True).head(10) .style .bar(subset=&quot;abs_error&quot;,color=&quot;#c07fef&quot;,vmax=error_by_day.abs_error.quantile(0.95)) .highlight_max(axis=0,color=&quot;#fef70c&quot;) ) . price price_prediction error abs_error . year month dayofmonth . 2019 7 29 95.873753 | 95.874405 | -0.000652 | 0.000652 | . 8 20 61.926547 | 61.927246 | -0.000699 | 0.000699 | . 2020 8 6 69.433709 | 69.434418 | -0.000708 | 0.000708 | . 9 9 37.015434 | 37.016582 | -0.001149 | 0.001149 | . 8 15 52.515043 | 52.512608 | 0.002436 | 0.002436 | . 2019 10 20 87.533212 | 87.530640 | 0.002573 | 0.002573 | . 2020 4 2 61.537020 | 61.540165 | -0.003145 | 0.003145 | . 14 40.838554 | 40.842064 | -0.003510 | 0.003510 | . 2019 6 9 69.561306 | 69.557457 | 0.003849 | 0.003849 | . 2020 3 23 44.639439 | 44.643665 | -0.004226 | 0.004226 | . Revenue generated per year using retail prices and a 70MWh storage system by the energy storage venture . predicton_vs_Actual = df_test.rename(columns={&quot;price&quot;:&quot;PRICE&quot;,&quot;price_prediction&quot;:&quot;PRICE_PREDICTION&quot;}) predicton_vs_Actual.head() . demand PRICE demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure ... price_30_days_std price_7_days_max price_15_days_max price_30_days_max price_7_days_min price_15_days_min price_30_days_min PRICE_PREDICTION error abs_error . date . 2019-01-02 106470.675 | 92.202011 | 106470.675 | 92.202011 | 0.0 | 0.0 | 0.0 | 18.4 | 22.2 | 26.3 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 92.063560 | 0.138451 | 0.138451 | . 2019-01-03 118789.605 | 127.380303 | 118789.605 | 127.380303 | 0.0 | 0.0 | 0.0 | 15.9 | 29.5 | 27.6 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 126.187721 | 1.192581 | 1.192581 | . 2019-01-04 133288.460 | 121.020997 | 133288.460 | 121.020997 | 0.0 | 0.0 | 0.0 | 18.0 | 42.6 | 27.4 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 121.093582 | -0.072585 | 0.072585 | . 2019-01-05 97262.790 | 83.493520 | 97262.790 | 83.493520 | 0.0 | 0.0 | 0.0 | 17.4 | 21.2 | 12.9 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 83.704323 | -0.210802 | 0.210802 | . 2019-01-06 93606.215 | 65.766407 | 93606.215 | 65.766407 | 0.0 | 0.0 | 0.0 | 14.6 | 22.1 | 30.9 | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | 65.620796 | 0.145611 | 0.145611 | . 5 rows × 40 columns . (predicton_vs_Actual.sample(5) .style .background_gradient(axis=0,subset=[&quot;PRICE&quot;,&quot;PRICE_PREDICTION&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real price and Prediction price&quot;) ) . Comparison of Real price and Prediction price demand PRICE demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day holiday date hour dayofweek quarter month year dayofyear dayofmonth weekofyear price_7_days_lag price_15_days_lag price_30_days_lag price_7_days_mean price_15_days_mean price_30_days_mean price_7_days_std price_15_days_std price_30_days_std price_7_days_max price_15_days_max price_30_days_max price_7_days_min price_15_days_min price_30_days_min PRICE_PREDICTION error abs_error . date . 2019-10-17 00:00:00 116690.040000 | 92.498044 | 116690.040000 | 92.498044 | 0.000000 | 0.000000 | 0.000000 | 9.500000 | 15.200000 | 9.600000 | 4.400000 | 1 | N | 2019-10-17 00:00:00 | 0 | 3 | 4 | 10 | 2019 | 290 | 17 | 42 | 167.981136 | 108.661677 | 149.599176 | 107.013610 | 111.958930 | 115.165759 | 14.385468 | 26.376231 | 28.347460 | 123.261328 | 167.981136 | 192.485035 | 90.164236 | 54.720115 | 54.720115 | 92.161629 | 0.336415 | 0.336415 | . 2020-02-28 00:00:00 110006.970000 | 50.306353 | 110006.970000 | 50.306353 | 0.000000 | 0.000000 | 0.000000 | 13.100000 | 20.100000 | 17.600000 | 0.000000 | 1 | N | 2020-02-28 00:00:00 | 0 | 4 | 1 | 2 | 2020 | 59 | 28 | 9 | 48.658203 | 79.018494 | 78.272342 | 50.750555 | 54.706821 | 174.695236 | 6.434453 | 9.322584 | 529.975511 | 60.554702 | 79.502644 | 2809.437516 | 42.236094 | 42.236094 | 14.235635 | 50.108658 | 0.197695 | 0.197695 | . 2019-05-26 00:00:00 106153.430000 | 63.903082 | 106153.430000 | 63.903082 | 0.000000 | 0.000000 | 0.000000 | 9.500000 | 15.100000 | 8.600000 | 6.600000 | 1 | N | 2019-05-26 00:00:00 | 0 | 6 | 2 | 5 | 2019 | 146 | 26 | 21 | 76.294168 | 87.036533 | 60.918383 | 90.270987 | 89.690794 | 95.045620 | 17.449081 | 13.195141 | 13.561886 | 115.894456 | 115.894456 | 121.029642 | 63.903082 | 63.903082 | 63.903082 | 63.706802 | 0.196280 | 0.196280 | . 2019-03-19 00:00:00 123290.835000 | 113.644774 | 123290.835000 | 113.644774 | 0.000000 | 0.000000 | 0.000000 | 16.900000 | 23.400000 | 12.700000 | 0.000000 | 1 | N | 2019-03-19 00:00:00 | 0 | 1 | 1 | 3 | 2019 | 78 | 19 | 12 | 76.971548 | 135.206447 | 110.489593 | 110.805143 | 100.527496 | 148.568061 | 11.182552 | 17.085113 | 215.716053 | 126.785957 | 126.785957 | 1284.799876 | 95.251188 | 74.374932 | 74.374932 | 113.493477 | 0.151297 | 0.151297 | . 2019-04-02 00:00:00 114927.205000 | 119.397551 | 114927.205000 | 119.397551 | 0.000000 | 0.000000 | 0.000000 | 7.400000 | 25.900000 | 16.600000 | 0.000000 | 1 | N | 2019-04-02 00:00:00 | 0 | 1 | 2 | 4 | 2019 | 92 | 2 | 14 | 95.567894 | 111.149795 | 153.784296 | 105.271113 | 105.146212 | 103.555577 | 17.997891 | 15.698139 | 17.232821 | 131.333531 | 131.333531 | 135.206447 | 83.989538 | 80.292927 | 74.374932 | 118.808243 | 0.589308 | 0.589308 | . (predicton_vs_Actual.describe() .style .highlight_max(axis=0,color=&quot;#c07fef&quot;) .highlight_min(axis=0,color=&quot;#00FF00&quot;) .set_caption(&quot;Price for the next 12-18 months dataset&quot;) ) . Price for the next 12-18 months dataset demand PRICE demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day hour dayofweek quarter month year dayofyear dayofmonth weekofyear price_7_days_lag price_15_days_lag price_30_days_lag price_7_days_mean price_15_days_mean price_30_days_mean price_7_days_std price_15_days_std price_30_days_std price_7_days_max price_15_days_max price_30_days_max price_7_days_min price_15_days_min price_30_days_min PRICE_PREDICTION error abs_error . count 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 644.000000 | 637.000000 | 629.000000 | 614.000000 | 638.000000 | 630.000000 | 615.000000 | 638.000000 | 630.000000 | 615.000000 | 638.000000 | 630.000000 | 615.000000 | 638.000000 | 630.000000 | 615.000000 | 644.000000 | 644.000000 | 644.000000 | . mean 117643.164884 | 92.987010 | 115960.088610 | 93.920500 | 1683.076273 | -2.628288 | 0.018763 | 11.258851 | 20.047516 | 14.121429 | 1.571118 | 0.579193 | 0.000000 | 3.000000 | 2.310559 | 5.920807 | 2019.434783 | 164.804348 | 15.633540 | 24.015528 | 93.691424 | 94.312974 | 95.825203 | 93.212968 | 93.538990 | 91.768173 | 54.150249 | 72.852308 | 87.392116 | 200.741524 | 322.505861 | 494.818502 | 52.455595 | 43.836577 | 36.683321 | 83.745338 | 9.241676 | 10.733489 | . std 13719.536422 | 221.045838 | 16058.351649 | 220.815258 | 5468.830434 | 14.501710 | 0.061677 | 4.178613 | 6.275934 | 7.527379 | 4.622587 | 0.494072 | 0.000000 | 2.001555 | 1.038818 | 3.204041 | 0.496114 | 97.726483 | 8.830309 | 13.916315 | 222.139909 | 223.477412 | 225.979776 | 105.016925 | 78.052549 | 55.971449 | 204.237108 | 204.005225 | 183.989880 | 555.624866 | 798.388712 | 1010.230907 | 26.400018 | 23.519784 | 21.738763 | 83.260063 | 159.133995 | 159.040192 | . min 86891.230000 | -6.076028 | 41988.240000 | 14.558266 | 0.000000 | -304.150000 | 0.000000 | 1.700000 | 9.600000 | 1.300000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 2019.000000 | 1.000000 | 1.000000 | 1.000000 | -1.761423 | -1.761423 | -1.761423 | 23.314900 | 26.399112 | 30.011994 | 2.643165 | 5.364451 | 8.586690 | 33.031411 | 37.881752 | 53.063441 | -6.076028 | -6.076028 | -6.076028 | 14.941895 | -72.023587 | 0.000652 | . 25% 107860.791250 | 47.951427 | 106688.311250 | 48.534078 | 0.000000 | 0.000000 | 0.000000 | 8.300000 | 15.300000 | 8.200000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 3.000000 | 2019.000000 | 81.750000 | 8.000000 | 12.000000 | 48.336165 | 48.542669 | 49.852245 | 48.849650 | 50.822983 | 52.430808 | 10.068998 | 12.243217 | 15.547400 | 64.270729 | 75.818450 | 103.916022 | 31.208768 | 21.755296 | 18.875436 | 47.338225 | -0.289771 | 0.100342 | . 50% 115552.020000 | 70.942903 | 115005.677500 | 72.009555 | 0.000000 | 0.000000 | 0.000000 | 11.000000 | 18.700000 | 11.700000 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | 2.000000 | 6.000000 | 2019.000000 | 162.000000 | 16.000000 | 24.000000 | 72.065599 | 72.815311 | 74.405598 | 75.313011 | 83.410650 | 87.232646 | 15.213009 | 18.972156 | 23.018682 | 100.676262 | 125.099031 | 134.747615 | 49.910197 | 44.176312 | 42.328652 | 71.584919 | -0.011184 | 0.268921 | . 75% 127321.842500 | 101.439833 | 127115.777500 | 101.665942 | 0.000000 | 0.000000 | 0.000000 | 13.800000 | 23.200000 | 19.300000 | 1.050000 | 1.000000 | 0.000000 | 5.000000 | 3.000000 | 8.000000 | 2020.000000 | 242.250000 | 23.000000 | 35.000000 | 101.944171 | 102.608515 | 103.262500 | 103.681588 | 105.118976 | 105.963637 | 24.736949 | 32.092289 | 38.685609 | 134.488422 | 162.491995 | 243.810160 | 75.512742 | 63.903082 | 52.525320 | 101.335741 | 0.233059 | 0.588213 | . max 170653.840000 | 4549.645105 | 170653.840000 | 4549.645105 | 57597.595000 | 0.000000 | 0.625000 | 25.100000 | 43.500000 | 32.000000 | 54.600000 | 1.000000 | 0.000000 | 6.000000 | 4.000000 | 12.000000 | 2020.000000 | 365.000000 | 31.000000 | 52.000000 | 4549.645105 | 4549.645105 | 4549.645105 | 902.337815 | 506.831433 | 318.015562 | 1663.267896 | 1140.727552 | 817.222368 | 4549.645105 | 4549.645105 | 4549.645105 | 118.261945 | 89.427805 | 80.292927 | 1078.613281 | 3471.031824 | 3471.031824 | . train_df_result =df_train.groupby(&quot;year&quot;)[&quot;price&quot;,&quot;demand_pos_price&quot;,&quot;demand_neg_price&quot;,&quot;frac_neg_price&quot;,&quot;price_negative&quot;,&quot;demand&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).stack() (train_df_result .style .background_gradient(axis=0,subset=[&quot;demand&quot;,&quot;price&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real Demand and Prediction Demand&quot;) ) . Comparison of Real Demand and Prediction Demand price demand_pos_price demand_neg_price frac_neg_price price_negative demand . year . 2015 mean 35.068136 | 124491.479493 | 170.690384 | 0.001598 | -2.305297 | 124662.169877 | . max 188.086125 | 158052.890000 | 19480.250000 | 0.229167 | 0.000000 | 158052.890000 | . min 13.279841 | 84331.030000 | 0.000000 | 0.000000 | -318.660000 | 95093.295000 | . 2016 mean 50.094252 | 121028.504249 | 961.034508 | 0.010303 | -4.438947 | 121989.538757 | . max 545.737820 | 160285.015000 | 29110.575000 | 0.333333 | 0.000000 | 160285.015000 | . min 6.869135 | 65215.145000 | 0.000000 | 0.000000 | -289.190000 | 90227.480000 | . 2017 mean 94.740161 | 118910.605411 | 42.443274 | 0.000400 | -2.460516 | 118953.048685 | . max 213.339432 | 154632.335000 | 4666.465000 | 0.041667 | 0.000000 | 155060.610000 | . min 19.865114 | 85094.375000 | 0.000000 | 0.000000 | -342.220000 | 85094.375000 | . 2018 mean 94.648823 | 118438.166507 | 372.398726 | 0.003881 | -1.643921 | 118810.565233 | . max 1210.137920 | 165070.595000 | 22839.285000 | 0.250000 | 0.000000 | 165070.595000 | . min 14.673588 | 73658.870000 | 0.000000 | 0.000000 | -157.920000 | 88903.065000 | . 2019 mean 78.560979 | 98933.060000 | 0.000000 | 0.000000 | 0.000000 | 98933.060000 | . max 78.560979 | 98933.060000 | 0.000000 | 0.000000 | 0.000000 | 98933.060000 | . min 78.560979 | 98933.060000 | 0.000000 | 0.000000 | 0.000000 | 98933.060000 | . test_df_result =df_test.groupby(&quot;year&quot;)[&quot;price&quot;,&quot;demand_pos_price&quot;,&quot;demand_neg_price&quot;,&quot;frac_neg_price&quot;,&quot;price_negative&quot;,&quot;demand&quot;,&quot;price_prediction&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).stack() (test_df_result .style .background_gradient(axis=0,subset=[&quot;price&quot;,&quot;price_prediction&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real Demand and Prediction Demand&quot;) ) . Comparison of Real Demand and Prediction Demand price demand_pos_price demand_neg_price frac_neg_price price_negative demand price_prediction . year . 2019 mean 117.387745 | 117288.371772 | 703.457431 | 0.007612 | -1.718283 | 117991.829203 | 106.173088 | . max 4549.645105 | 168894.845000 | 17410.610000 | 0.166667 | 0.000000 | 168894.845000 | 1078.613281 | . min 19.170951 | 80859.020000 | 0.000000 | 0.000000 | -89.465035 | 90145.615000 | 21.854008 | . 2020 mean 61.266055 | 114233.320500 | 2956.580768 | 0.033259 | -3.811295 | 117189.901268 | 54.589256 | . max 2809.437516 | 170653.840000 | 57597.595000 | 0.625000 | 0.000000 | 170653.840000 | 1078.613281 | . min -6.076028 | 41988.240000 | 0.000000 | 0.000000 | -304.150000 | 86891.230000 | 14.941895 | . test_df_result =df_test.groupby(&quot;year&quot;)[&quot;price&quot;,&quot;demand_pos_price&quot;,&quot;demand_neg_price&quot;,&quot;frac_neg_price&quot;,&quot;price_negative&quot;,&quot;demand&quot;,&quot;price_prediction&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).stack() (test_df_result .style .background_gradient(axis=0,subset=[&quot;price&quot;,&quot;price_prediction&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Comparison of Real Demand and Prediction Demand&quot;) ) . Comparison of Real Demand and Prediction Demand price demand_pos_price demand_neg_price frac_neg_price price_negative demand price_prediction . year . 2019 mean 117.387745 | 117288.371772 | 703.457431 | 0.007612 | -1.718283 | 117991.829203 | 106.173088 | . max 4549.645105 | 168894.845000 | 17410.610000 | 0.166667 | 0.000000 | 168894.845000 | 1078.613281 | . min 19.170951 | 80859.020000 | 0.000000 | 0.000000 | -89.465035 | 90145.615000 | 21.854008 | . 2020 mean 61.266055 | 114233.320500 | 2956.580768 | 0.033259 | -3.811295 | 117189.901268 | 54.589256 | . max 2809.437516 | 170653.840000 | 57597.595000 | 0.625000 | 0.000000 | 170653.840000 | 1078.613281 | . min -6.076028 | 41988.240000 | 0.000000 | 0.000000 | -304.150000 | 86891.230000 | 14.941895 | . df_electricity[&quot;total_price&quot;] = df_electricity[&quot;demand&quot;]*df_electricity[&quot;price&quot;] . df_electricity.groupby(df_electricity.index.year)[&quot;price&quot;].agg([&quot;mean&quot;,&quot;max&quot;,&quot;min&quot;]).unstack() . date mean 2015 35.068136 2016 50.094252 2017 94.740161 2018 94.648823 2019 117.281370 2020 61.266055 max 2015 188.086125 2016 545.737820 2017 213.339432 2018 1210.137920 2019 4549.645105 2020 2809.437516 min 2015 13.279841 2016 6.869135 2017 19.865114 2018 14.673588 2019 19.170951 2020 -6.076028 dtype: float64 . demand_negative_price = df_electricity[df_electricity[&quot;price_negative&quot;] &lt; 0] demand_negative_price . demand price demand_pos_price price_positive demand_neg_price price_negative frac_neg_price min_temperature max_temperature solar_exposure rainfall school_day holiday total_price . date . 2015-01-01 99635.030 | 25.633696 | 97319.240 | 26.415953 | 2315.790 | -7.240000 | 0.020833 | 13.3 | 26.9 | 23.6 | 0.0 | N | Y | 2.554014e+06 | . 2015-01-02 129606.010 | 33.138988 | 121082.015 | 38.837661 | 8523.995 | -47.809777 | 0.062500 | 15.4 | 38.8 | 26.8 | 0.0 | N | N | 4.295012e+06 | . 2015-01-07 153514.820 | 48.312309 | 149498.715 | 49.639712 | 4016.105 | -1.100000 | 0.020833 | 18.9 | 37.4 | 20.7 | 0.0 | N | N | 7.416655e+06 | . 2015-01-18 97728.750 | 17.008681 | 95473.965 | 20.911790 | 2254.785 | -148.260000 | 0.020833 | 15.3 | 19.5 | 23.4 | 0.0 | N | N | 1.662237e+06 | . 2015-02-13 136070.620 | 18.736971 | 133078.540 | 26.322857 | 2992.080 | -318.660000 | 0.020833 | 16.1 | 32.4 | 14.9 | 0.0 | Y | N | 2.549551e+06 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2020-10-01 106641.790 | 34.654671 | 95349.610 | 41.651658 | 11292.180 | -24.426925 | 0.125000 | 9.4 | 19.5 | 21.2 | 1.8 | N | N | 3.695636e+06 | . 2020-10-02 99585.835 | -6.076028 | 41988.240 | 26.980251 | 57597.595 | -30.173823 | 0.625000 | 12.8 | 26.0 | 22.0 | 0.0 | N | N | -6.050864e+05 | . 2020-10-03 92277.025 | -1.983471 | 44133.510 | 32.438156 | 48143.515 | -33.538025 | 0.583333 | 17.4 | 29.4 | 19.8 | 0.0 | N | N | -1.830288e+05 | . 2020-10-04 94081.565 | 25.008614 | 88580.995 | 26.571687 | 5500.570 | -0.163066 | 0.062500 | 13.5 | 29.5 | 8.4 | 0.0 | N | N | 2.352850e+06 | . 2020-10-05 113610.030 | 36.764701 | 106587.375 | 39.616015 | 7022.655 | -6.511550 | 0.083333 | 9.1 | 12.7 | 7.3 | 12.8 | N | N | 4.176839e+06 | . 181 rows × 14 columns . demand_positive_price = df_electricity[df_electricity[&quot;price_negative&quot;] &gt;= 0] demand_positive_price . demand_negative= (demand_negative_price.groupby(demand_negative_price.index.year)[&quot;total_price&quot;].agg([&quot;sum&quot;,&quot;max&quot;,&quot;min&quot;])).reset_index() demand_negative . This is the condition when the demand for negative price per year. You can see on 2020 where the minimum price show a negative value. . demand_positive = demand_positive_price.groupby(demand_positive_price.index.year)[&quot;total_price&quot;].agg([&quot;sum&quot;,&quot;max&quot;,&quot;min&quot;]).reset_index() demand_positive . This is the condition when the demand for positive price per year. You can see on per year where the minimum price show a an appreciation from the market that is shown on the positive electiricity price. . result = demand_negative.merge(demand_positive,on=&quot;date&quot;,suffixes =[&quot;_demand_pos&quot;,&quot;_demand_neg&quot;]) result . result[&quot;profit&quot;] =result[&quot;sum_demand_pos&quot;]-result[&quot;sum_demand_neg&quot;] result . We then substract the result when the demand is positive and demand is negative based on the total price feature that we have been created. Most of the years shows a good result. . (result.sort_values(&quot;profit&quot;,ascending=False) .style .background_gradient(axis=0,subset=[&quot;profit&quot;],cmap=&quot;Blues&quot;) .set_caption(&quot;Revenue Generated per year in the daily electiricity in Victoria Australia&quot;) ) . This is the result of revenue generated per year. We can see a good revenue generated per year. If you find this notebook usefull, Please show your appreciation by upvoting this notebook. Thank you. . References . Here is a few references for creating advanced visualization using matplotlib 1. . 1. You can see a lot of data visualization plots here Data Visualization with Python!↩ . 2. Towards Data Science Article by Shiu Tang Li !↩ . 3. TPSJAN22-01 EDA which makes sense !↩ . .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/datavisualization/matplotlib/2022/01/01/datavisualization-advanced.html",
            "relUrl": "/datavisualization/matplotlib/2022/01/01/datavisualization-advanced.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "My key takeaways from reading Trustworthy Online Controlled Experiments Book",
            "content": "Achievement unlocked: Our book is the #1 Best Seller in Data Mining: Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing (https://t.co/aseJyTKbDh).I&#39;m humbled.#abtesting #hippo #experimentguide pic.twitter.com/5CckQcXnPH . &mdash; Ronny Kohavi (@ronnyk) June 29, 2021 This article contains about A/B testing, make sure the reader understands fundamental concepts about control and treatment, p-values, confidence interval, statistical significance, practical significance, randomization, sample, and population in order to get the idea about A/B testing. You can get the fundamental statistics by looking at this video. . . Experimentation/Testing has been everywhere. It is widely adopted by startups to the corporate firm to detect how good the simple or bigger changes of the project or additional feature(be it a website, mobile app, etc.) of the project to give impact to the real-life/business. In Data Science, Experimentation is widely used to predict how good our experimentation is based on a few metrics by using statistical approaches. Online Trustworthy Controlled Experiment is the book I wish I had when I started learning A/B testing/Experimentation. This book covered all the fundamental concepts to advanced concepts about A/B testing through a step-by-step walkthrough such as designing the experimentation, running the experimentation and getting data, interpreting the results and results to decision-making. The author explained clearly the pitfalls and solutions to the problems that could exist during the experimentation. . A step-by-step walkthrough of A/B Testing . Designing The Experiment . The first step of doing online experimentation is to ascertain our hypothesis, a practical significance boundary, and a few metrics before running the experimentation. We should check the randomization of the sample that we will use for the control and treatment. We also should pay attention to how large the sample is to be used for running the experimentation. If we are concerned about detecting a small change or being more confident about the conclusion, we have to consider using more samples and a lower p-value threshold to get a more accurate result. However, If we no longer care about small changes, we could reduce the sample to detect the practical significance. . Getting the Data and Running The Experimentation . In this section, you are going to get some data pertaining to the experimentation that we will be analyzing such as analyzing how many samples should be used, the day of week effect due to everyone having different behavior on weekdays over the weekend, and also seasonality where users behave differently on holiday. We also consider looking at primacy and novelty effects where users have a tendency to use new features, . Interpreting the Results . One thing we should consider when interpreting results is how our experimentation will run properly and avoid some bugs that could invalidate the experiment result(guardrail metrics). For instance, we can check the latency which is essential to check that can affect the control and treatment, or expect the control and treatment sample to be equal to the configuration we set for A/B testing. These factors must be fulfilled to get better results that can affect the metrics we are going to achieve. . From Results to Decisions . Getting a result from the experiment is not the end of the experimentation. Getting a result that can make an impact on the business will be a good way of implementing experimentation. In A/B Testing, good results can be considered good if they are repeatable and trustworthy. However, there are a few factors that should consider regarding whether we need to make a tradeoff between metrics for instance user engagement and revenue. Should we launch if there is no correlation between this metric?. We also consider launching costs whether the revenue will cover the launch cost or get more expected revenue even need much cost to launch the product. Furthermore, We also consider statistical and practical significance thresholds of whether to launch or not launch the product. . Statistical and Practical Significance Thresholds . Source: Online Trustworthy Controlled Experiment . The figure shown above depicts the statistical and practical significance threshold where the two dashed lines are the practical significance boundary and the black box is the statistical significance threshold along with the confidence interval. We know from statistics theory that the statistical significance threshold is less than or equal to 5% to quantify that we should reject the null hypothesis and the practical significance is managed based on the condition of our objective that we wanted to achieve. Based on the practical and statistical significance, we can take a step either choosing to launch or not. However, we can even take a follow-up test to test our hypothesis in order to translate practical and statistical significance boundaries based on some consideration of our experiment to get statistical power(a condition where the p-value is less than(more extreme) or equal to 0.05 implying there is a difference between control and treatment mean assuming the null hypothesis is true). . Conclusion . This book is really essential for every data scientist who specialized in product analytics in order to cover our understanding of data better through A/B testing. You will get a lot of enlightenment after reading this book. Read it, buy it. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/abtesting/statistics/2021/06/20/Trustworthy-Online-Controlled-Experiments.html",
            "relUrl": "/abtesting/statistics/2021/06/20/Trustworthy-Online-Controlled-Experiments.html",
            "date": " • Jun 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Josua is a business development analyst who turns into a self-taught Machine Learning Engineer. His interests include statistical learning, predictive modeling, and causal inference. He loves running and it teaches him against giving up doing anything, even when implementing the Machine Learning Lifecycle(MLOps). . Apart from pursuing his passion for Machine Learning, he is keen on investing in the Indonesian Stock Exchange and Cryptocurrency. He has been running a full marathon in Jakarta Marathon in 2015 and Osaka Marathon in 2019. His next dreams are to run a marathon in Boston Marathon, TCS New York City Marathon and Virgin Money London Marathon. . My Certification . I am not a guy who love collecting certifications in Data Science. Perhaps, you will find somewhere that i have completed a few certifications in Data Science. I love learning and implement what i learn in data science project is paramount. . Papers I am currently interested in arXiv . “Why Should I Trust You?” Explaining the Predictions of Any Classifier | Evaluation Gaps in Machine Learning Practice | 50 Years of Data Science | Cyclical Learning Rates for Training Neural Networks | Climbing the Kaggle Leaderboard by Exploiting the Log-Loss Oracle | On Artificial Intelligence - A European approach to excellence and trust | EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks | The Unreasonable Effectiveness of Data | Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX) | Microsoft COCO: Common Objects in Context | . Contact Me . josuadotnaiborhu94atgmaildotcom . © Josua Antonius Naiborhu, 2020-2022.These posts are meant to be used for educational purposes. Excerpts and links from this site may be used, provided that full and clear credit is given to Josua Naiborhu with appropriate and specific direction to the original content. .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}