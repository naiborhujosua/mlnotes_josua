{
  
    
        "post0": {
            "title": "NLP Approach using Word Embedding",
            "content": "This is a minimal yet very powerful approach of NLP problems that you can use on Kaggle competition. I got my attention to the book written by one of Kaggle Grandmaster in his book called Approaching (Almost) Any Machine Learning Problem. I am so interested in how the author approaches the problems for each case in Kaggle Competition started by Supervised and unsupervised problems. So, NLP is one of the problems in the competition in Kaggle. We know that many approaches that we can do to preprocess text data from Bag of Words, TFIDF to Word Embedding. I am interested to dive deeper into Word Embedding because this approach yield a better result compared to BOW or Tfidf approach based on this dataset. You can find the comparison by reading this book. I found this approach quite useful for my NLP competition. Before reading this NLP approach, I hope the readers have the fundamentals concepts of pytorch, data visualization libraries and NLP concept to know the code better. . Import Data . import pandas as pd movies = pd.read_csv(&quot;imdb.csv&quot;) movies.head() . review sentiment . 0 One of the other reviewers has mentioned that ... | positive | . 1 A wonderful little production. &lt;br /&gt;&lt;br /&gt;The... | positive | . 2 I thought this was a wonderful way to spend ti... | positive | . 3 Basically there&#39;s a family where a little boy ... | negative | . 4 Petter Mattei&#39;s &quot;Love in the Time of Money&quot; is... | positive | . Check Proportion of target . movies.sentiment.value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . Create Cross Validation . import pandas as pd from sklearn import model_selection if __name__==&quot;__main__&quot;: df = pd.read_csv(&quot;imdb.csv&quot;) df.sentiment = df.sentiment.apply(lambda x: 1 if x == &quot;positive&quot; else 0) df[&quot;kfold&quot;] =-1 df = df.sample(frac=1).reset_index(drop=True) y = df.sentiment.values kf = model_selection.StratifiedKFold(n_splits=5) for f,(t_,v_) in enumerate(kf.split(X=df,y=y)): df.loc[v_,&quot;kfold&quot;] =f df.to_csv(&quot;imdb_folds.csv&quot;,index=False) . movies_folds = pd.read_csv(&quot;imdb_folds.csv&quot;) movies_folds.head() . review sentiment kfold . 0 I enjoyed Erkan &amp; Stefan  a cool and fast sto... | 1 | 0 | . 1 The only reason I rated this film as 2 is beca... | 0 | 0 | . 2 One of those movies where you take bets on who... | 0 | 0 | . 3 This series was just like what you would expec... | 1 | 0 | . 4 While many people found this film simply too s... | 1 | 0 | . There is one additional features called kfold. . Word Embedding . import numpy as np def sentence_to_vec(s,embedding_dict,stop_words,tokenizer): words =str(s).lower() words =tokenizer(words) words = [ w for w in words if w not in stop_words] words = [w for w in words if w.alpha()] M =[] for w in words: if w in embedding_dict: M.append(embedding_dict[w]) if len(M)==0: return np.zeros(300) M = np.array(M) v = M.sum() return v/np.sqrt((v**2).sum()) . Create Dataset in pytorch based on model in our dataset . import torch class IMDBDataset: def __init__(self,reviews,targets): self.reviews =reviews self.targets = targets def __len__(self): return len(self.reviews) def __getitem__(self,item): review =self.reviews[item,:] target =self.target[item] return { &quot;review&quot;: torch.tensor(review,dtype=torch.long), &quot;target&quot;: torch.tensor(target,dtype=torch.float) } . Create Model . import torch.nn as nn class LSTM(nn.Module): def __init__(self,embedding_matrix): super(LSTM,self).__init__() num_words =embedding_matrix.shape[0] embed_dim= embedding_matrix.shape[1] self.embedding = nn.Embedding( num_embeddings = num_words, embedding_dim=embed_dim ) self.embedding.weight = nn.Parameter( torch.tensor( embedding_matrix, dtype=torch.float32 ) ) self.embedding.weight.requires_grad=False self.lstm = nn.LSTM( embed_dim, 128, bidirectional=True, batch_first=True ) self.out = nn.Linear(512,1) def forward(self,x): x = self.embedding(x) x,_ = self.lstm(x) avg_pool =torch.mean(x,1) max_pool, _ = torch.max(x,1) out = torch.cat((avg_pool,maxpool),1) out = self.out(out) return out . Create Training Function for Modelling . def train(data_loader,model,optimizer,device): model.train() for data in data_loader: reviews = data[&quot;review&quot;] targets = data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.float) optimizer.zero_grad() predictions = model(reviews) loss =nn.BCEWithLogitsLoss()( predictions, targets.view(-1,1) ) loss.bakward() optimizer.step() . Create Evaluation for Modelling . def evaluate(data_loader,model,device): final_predictions =[] final_targets = [] model.eval() with torch.no_grad(): for data in data_loader: reviews =data[&quot;review&quot;] targets =data[&quot;target&quot;] reviews = reviews.to(device,dtype=torch.long) targets = targets.to(device,dtype=torch.long) predictions = model(reviews) predictions = predictions.cpu().numpy().tolist() targets = data[&quot;target&quot;].cpu().numpy.tolist() final_predictions.extend(predictions) final_targets.extend(targets) return final_predictions,final_targets . Word Embedding Creation . import io #from tensorflow.keras import import tensorflow as tf def load_vectors(fname): fin = io.open( fname, &quot;r&quot;, encoding=&quot;utf-8&quot;, newline=&quot; n&quot;, errors=&quot;ignore&quot; ) n,d = map(int,fin.readline().split()) data ={} for line in fin: tokens = line.rstrip().split(&#39; &#39;) data[tokens[0]] = list(map(float,tokens[1:])) return data def create_embedding_matrix(world_index,embedding_dict): embedding_matrix = np.zeros((len(word_index)+1,300)) for word , i in word_index.items(): if word in embedding_dict: embedding_dict[i] = embedding_dict[word] return embedding_matrix def run(df,fold): train_df = df[df.kfold != fold].reset_index(drop=True) valid_df = df[df.kfold ==fold].reset_index(drop=True) print(&quot;Fitting tokenizer&quot;) tokenizer = tf.keras.preprocessing.text.Tokenizer() tokenizer.fit_on_texts(df.review.values.tolist()) xtrain = tokenizer.texts_to_sequences(train_df.review.values) xtest = tokenizer.texts_to_sequences(valid_df.review.values) xtrain = tf.keras.preprocessing.sequence.pad_sequences( xtrain,maxlen=128 ) xtest = tf.keras.preprocessing.sequence.pad_sequences( xtest,maxlen=128 ) train_dataset = IMDBDataset( reviews =xtrain, targets = train_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( train_dataset, batch_size =16, num_workers=2 ) valid_dataset =IMDBDataset( reviews =xtest, targets = valid_df.sentiment.values ) train_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size =8, num_workers=1 ) print(&quot;Loading Embeddings&quot;) # you can suit based on where you put your vec fasttext embedding_dict = load_vectors(&quot;crawl-300d-2M.vec/crawl-300d-2M.vec&quot;) embedding_matrix = create_embedding_matrix( tokenizer.word_index,embedding_dict ) device =torch.device(&quot;cuda&quot;) model =LSTM(embedding_matrix) model.to(device) optimizer = torch.optim.Adam(model.parameters(),lr=1e-3) print(&quot;Training Model&quot;) best_accuracy =0 early_stopping_counter =0 for epoch in range(10): train(train_data_loader,model,optimizer,device) outputs,targets = evaluate(valid_data_loader,model,device) outputs = np.array(outputs) &gt;=0.5 accuracy = metrics.accuracy_score(targets,outputs) print(f&quot;{fold}, Epoch {epoch}, Accuracy Score ={accuracy}&quot;) if accuracy &gt; best_accuracy: best_accuracy = accuracy else: early_stopping_counter +=1 if early_stopping_counter &gt; 2: break if __name__ == &quot;__main__&quot;: df = pd.read_csv(&quot;imdb_folds.csv&quot;) run(df,0) run(df,1) run(df,2) run(df,3) run(df,4) . Fitting tokenizer Loading Embeddings . The choice of Machine learning algorithms will determine the quality of our prediction score. However, The simple model will also determine how fast the training process compared to state of art of ML algorithms. If we have laptop/computer with better GPU can help the training process. But in real life, Simple models with better preprocessing will have prediction score that is not too dfferent with newest ML algorithms one. So, It is better to discuss with the stakeholders for improving the models based on business metrics. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/nlp/wordembedding/jupyter/2022/04/06/approachingnlpusingwordembedding.html",
            "relUrl": "/nlp/wordembedding/jupyter/2022/04/06/approachingnlpusingwordembedding.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Mean encodings",
            "content": "Version 1.1.0 . In this programming assignment you will be working with 1C dataset from the final competition. You are asked to encode item_id in 4 different ways: . 1) Via KFold scheme; 2) Via Leave-one-out scheme; 3) Via smoothing scheme; 4) Via expanding mean scheme. . You will need to submit the correlation coefficient between resulting encoding and target variable up to 4 decimal places. . General tips . Fill NANs in the encoding with 0.3343. | Some encoding schemes depend on sorting order, so in order to avoid confusion, please use the following code snippet to construct the data frame. This snippet also implements mean encoding without regularization. | . import pandas as pd import numpy as np from itertools import product import warnings warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) from grader import Grader . Read data . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) . sales.head() . date date_block_num shop_id item_id item_price item_cnt_day . 0 02.01.2013 | 0 | 59 | 22154 | 999.00 | 1.0 | . 1 03.01.2013 | 0 | 25 | 2552 | 899.00 | 1.0 | . 2 05.01.2013 | 0 | 25 | 2552 | 899.00 | -1.0 | . 3 06.01.2013 | 0 | 25 | 2554 | 1709.05 | 1.0 | . 4 15.01.2013 | 0 | 25 | 2555 | 1099.00 | 1.0 | . sales.shape . (2935849, 6) . Aggregate data . Since the competition task is to make a monthly prediction, we need to aggregate the data to montly level before doing any encodings. The following code-cell serves just that purpose. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales[sales[&#39;date_block_num&#39;]==block_num][&#39;shop_id&#39;].unique() cur_items = sales[sales[&#39;date_block_num&#39;]==block_num][&#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) #turn the grid into pandas dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) #get aggregated values for (shop_id, item_id, month) gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) #fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] #join aggregated data to the grid all_data = pd.merge(grid,gb,how=&#39;left&#39;,on=index_cols).fillna(0) #sort the data all_data.sort_values([&#39;date_block_num&#39;,&#39;shop_id&#39;,&#39;item_id&#39;],inplace=True) . Mean encodings without regularization . After we did the techinical work, we are ready to actually mean encode the desired item_id variable. . Here are two ways to implement mean encoding features without any regularization. You can use this code as a starting point to implement regularized techniques. . Method 1 . item_id_target_mean = all_data.groupby(&#39;item_id&#39;).target.mean() # In our non-regularized case we just *map* the computed means to the `item_id`&#39;s all_data[&#39;item_target_enc&#39;] = all_data[&#39;item_id&#39;].map(item_id_target_mean) # Fill NaNs all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) # Print correlation encoded_feature = all_data[&#39;item_target_enc&#39;].values print(np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1]) . 0.483038698862 . Method 2 . &#39;&#39;&#39; Differently to `.target.mean()` function `transform` will return a dataframe with an index like in `all_data`. Basically this single line of code is equivalent to the first two lines from of Method 1. &#39;&#39;&#39; all_data[&#39;item_target_enc&#39;] = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;mean&#39;) # Fill NaNs all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) # Print correlation encoded_feature = all_data[&#39;item_target_enc&#39;].values print(np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1]) . 0.483038698862 . See the printed value? It is the correlation coefficient between the target variable and your new encoded feature. You need to compute correlation coefficient between the encodings, that you will implement and submit those to coursera. . grader = Grader() . 1. KFold scheme . Explained starting at 41 sec of Regularization video. . Now it&#39;s your turn to write the code! . You may use &#39;Regularization&#39; video as a reference for all further tasks. . First, implement KFold scheme with five folds. Use KFold(5) from sklearn.model_selection. . Split your data in 5 folds with sklearn.model_selection.KFold with shuffle=False argument. | Iterate through folds: use all but the current fold to calculate mean target for each level item_id, and fill the current fold. . See the Method 1 from the example implementation. In particular learn what map and pd.Series.map functions do. They are pretty handy in many situations. | . | from sklearn.model_selection import KFold kf = KFold(n_splits=5,shuffle=False) for tr_ind, val_ind in kf.split(all_data): X_tr, X_val = all_data.iloc[tr_ind], all_data.iloc[val_ind] X_val[&#39;item_target_enc&#39;] = X_val[&#39;item_id&#39;].map(X_tr.groupby(&#39;item_id&#39;).target.mean()) all_data.iloc[val_ind] = X_val all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values # You will need to compute correlation like that corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;KFold_scheme&#39;, corr) . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy . 0.41645907128 Current answer for task KFold_scheme is: 0.41645907128 . 2. Leave-one-out scheme . Now, implement leave-one-out scheme. Note that if you just simply set the number of folds to the number of samples and run the code from the KFold scheme, you will probably wait for a very long time. . To implement a faster version, note, that to calculate mean target value using all the objects but one given object, you can: . Calculate sum of the target values using all the objects. | Then subtract the target of the given object and divide the resulting value by n_objects - 1. | Note that you do not need to perform 1. for every object. And 2. can be implemented without any for loop. . It is the most convenient to use .transform function as in Method 2. . target_sum = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;sum&#39;) n_objects = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;count&#39;) all_data[&#39;item_target_enc&#39;] = (target_sum - all_data[&#39;target&#39;]) / (n_objects - 1) all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Leave-one-out_scheme&#39;, corr) . 0.480384831129 Current answer for task Leave-one-out_scheme is: 0.480384831129 . 3. Smoothing . Explained starting at 4:03 of Regularization video. . Next, implement smoothing scheme with $ alpha = 100$. Use the formula from the first slide in the video and $0.3343$ as globalmean. Note that nrows is the number of objects that belong to a certain category (not the number of rows in the dataset). . item_id_target_mean = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;mean&#39;) n_objects = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].transform(&#39;count&#39;) all_data[&#39;item_target_enc&#39;] = (item_id_target_mean * n_objects + 0.3343 * 100) / (n_objects + 100) all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Smoothing_scheme&#39;, corr) . 0.48181987971 Current answer for task Smoothing_scheme is: 0.48181987971 . 4. Expanding mean scheme . Explained starting at 5:50 of Regularization video. . Finally, implement the expanding mean scheme. It is basically already implemented for you in the video, but you can challenge yourself and try to implement it yourself. You will need cumsum and cumcount functions from pandas. . cumsum = all_data.groupby(&#39;item_id&#39;)[&#39;target&#39;].cumsum() - all_data[&#39;target&#39;] cumcnt = all_data.groupby(&#39;item_id&#39;).cumcount() all_data[&#39;item_target_enc&#39;] = cumsum / cumcnt all_data[&#39;item_target_enc&#39;].fillna(0.3343, inplace=True) encoded_feature = all_data[&#39;item_target_enc&#39;].values corr = np.corrcoef(all_data[&#39;target&#39;].values, encoded_feature)[0][1] print(corr) grader.submit_tag(&#39;Expanding_mean_scheme&#39;, corr) . 0.502524521108 Current answer for task Expanding_mean_scheme is: 0.502524521108 . Authorization &amp; Submission . To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. Note: Token expires 30 minutes after generation. . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot;TOKEN HERE&quot; # TOKEN HERE grader.status() . You want to submit these numbers: Task KFold_scheme: 0.41645907128 Task Leave-one-out_scheme: 0.480384831129 Task Smoothing_scheme: 0.48181987971 Task Expanding_mean_scheme: 0.502524521108 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/02/Mean-Encodings.html",
            "relUrl": "/2022/04/02/Mean-Encodings.html",
            "date": " • Apr 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Ensembling Implementation",
            "content": "Version 1.0.1 . import numpy as np import pandas as pd import sklearn import scipy.sparse import lightgbm for p in [np, pd, scipy, sklearn, lightgbm]: print (p.__name__, p.__version__) . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 lightgbm 2.0.6 . Important! There is a huge chance that the assignment will be impossible to pass if the versions of lighgbm and scikit-learn are wrong. The versions being tested: . numpy 1.13.1 pandas 0.20.3 scipy 0.19.1 sklearn 0.19.0 ligthgbm 2.0.6 . To install an older version of lighgbm you may use the following command: . pip uninstall lightgbm pip install lightgbm==2.0.6 . Ensembling . In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking. . We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what&#39;s happening. . import pandas as pd import numpy as np import gc import matplotlib.pyplot as plt %matplotlib inline pd.set_option(&#39;display.max_rows&#39;, 600) pd.set_option(&#39;display.max_columns&#39;, 50) import lightgbm as lgb from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from tqdm import tqdm_notebook from itertools import product def downcast_dtypes(df): &#39;&#39;&#39; Changes column types in the dataframe: `float64` type to `float32` `int64` type to `int32` &#39;&#39;&#39; # Select columns to downcast float_cols = [c for c in df if df[c].dtype == &quot;float64&quot;] int_cols = [c for c in df if df[c].dtype == &quot;int64&quot;] # Downcast df[float_cols] = df[float_cols].astype(np.float32) df[int_cols] = df[int_cols].astype(np.int32) return df . Load data subset . Let&#39;s load the data from the hard drive first. . sales = pd.read_csv(&#39;../readonly/final_project_data/sales_train.csv.gz&#39;) shops = pd.read_csv(&#39;../readonly/final_project_data/shops.csv&#39;) items = pd.read_csv(&#39;../readonly/final_project_data/items.csv&#39;) item_cats = pd.read_csv(&#39;../readonly/final_project_data/item_categories.csv&#39;) . And use only 3 shops for simplicity. . sales = sales[sales[&#39;shop_id&#39;].isin([26, 27, 28])] . Get a feature matrix . We now need to prepare the features. This part is all implemented for you. . index_cols = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;date_block_num&#39;] # For every month we create a grid from all shops/items combinations from that month grid = [] for block_num in sales[&#39;date_block_num&#39;].unique(): cur_shops = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;shop_id&#39;].unique() cur_items = sales.loc[sales[&#39;date_block_num&#39;] == block_num, &#39;item_id&#39;].unique() grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype=&#39;int32&#39;)) # Turn the grid into a dataframe grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32) # Groupby data to get shop-item-month aggregates gb = sales.groupby(index_cols,as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target&#39;:&#39;sum&#39;}}) # Fix column names gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] # Join it to the grid all_data = pd.merge(grid, gb, how=&#39;left&#39;, on=index_cols).fillna(0) # Same as above but with shop-month aggregates gb = sales.groupby([&#39;shop_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_shop&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1]==&#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Same as above but with item-month aggregates gb = sales.groupby([&#39;item_id&#39;, &#39;date_block_num&#39;],as_index=False).agg({&#39;item_cnt_day&#39;:{&#39;target_item&#39;:&#39;sum&#39;}}) gb.columns = [col[0] if col[-1] == &#39;&#39; else col[-1] for col in gb.columns.values] all_data = pd.merge(all_data, gb, how=&#39;left&#39;, on=[&#39;item_id&#39;, &#39;date_block_num&#39;]).fillna(0) # Downcast dtypes from 64 to 32 bit to save memory all_data = downcast_dtypes(all_data) del grid, gb gc.collect(); . /opt/conda/lib/python3.6/site-packages/pandas/core/groupby.py:4036: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs) . After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago. . cols_to_rename = list(all_data.columns.difference(index_cols)) shift_range = [1, 2, 3, 4, 5, 12] for month_shift in tqdm_notebook(shift_range): train_shift = all_data[index_cols + cols_to_rename].copy() train_shift[&#39;date_block_num&#39;] = train_shift[&#39;date_block_num&#39;] + month_shift foo = lambda x: &#39;{}_lag_{}&#39;.format(x, month_shift) if x in cols_to_rename else x train_shift = train_shift.rename(columns=foo) all_data = pd.merge(all_data, train_shift, on=index_cols, how=&#39;left&#39;).fillna(0) del train_shift # Don&#39;t use old data from year 2013 all_data = all_data[all_data[&#39;date_block_num&#39;] &gt;= 12] # List of all lagged features fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] # We will drop these at fitting stage to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + [&#39;date_block_num&#39;] # Category for each item item_category_mapping = items[[&#39;item_id&#39;,&#39;item_category_id&#39;]].drop_duplicates() all_data = pd.merge(all_data, item_category_mapping, how=&#39;left&#39;, on=&#39;item_id&#39;) all_data = downcast_dtypes(all_data) gc.collect(); . . To this end, we&#39;ve created a feature matrix. It is stored in all_data variable. Take a look: . all_data.head() . shop_id item_id date_block_num target target_shop target_item target_lag_1 target_item_lag_1 target_shop_lag_1 target_lag_2 target_item_lag_2 target_shop_lag_2 target_lag_3 target_item_lag_3 target_shop_lag_3 target_lag_4 target_item_lag_4 target_shop_lag_4 target_lag_5 target_item_lag_5 target_shop_lag_5 target_lag_12 target_item_lag_12 target_shop_lag_12 item_category_id . 0 28 | 10994 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 1.0 | 6454.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37 | . 1 28 | 10992 | 12 | 3.0 | 6949.0 | 4.0 | 3.0 | 7.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 37 | . 2 28 | 10991 | 12 | 1.0 | 6949.0 | 5.0 | 1.0 | 3.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 2.0 | 4.0 | 7521.0 | 0.0 | 0.0 | 0.0 | 40 | . 3 28 | 10988 | 12 | 1.0 | 6949.0 | 2.0 | 2.0 | 5.0 | 8499.0 | 4.0 | 5.0 | 6454.0 | 5.0 | 6.0 | 5609.0 | 0.0 | 2.0 | 6753.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . 4 28 | 11002 | 12 | 1.0 | 6949.0 | 1.0 | 0.0 | 1.0 | 8499.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 40 | . Train/test split . For a sake of the programming assignment, let&#39;s artificially split the data into train and test. We will treat last month data as the test set. . dates = all_data[&#39;date_block_num&#39;] last_block = dates.max() print(&#39;Test `date_block_num` is %d&#39; % last_block) . Test `date_block_num` is 33 . dates_train = dates[dates &lt; last_block] dates_test = dates[dates == last_block] X_train = all_data.loc[dates &lt; last_block].drop(to_drop_cols, axis=1) X_test = all_data.loc[dates == last_block].drop(to_drop_cols, axis=1) y_train = all_data.loc[dates &lt; last_block, &#39;target&#39;].values y_test = all_data.loc[dates == last_block, &#39;target&#39;].values . First level models . You need to implement a basic stacking scheme. We have a time component here, so we will use scheme f) from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let&#39;s see how we get test meta-features first. . Test meta-features . Firts, we will run linear regression on numeric columns and get predictions for the last month. . lr = LinearRegression() lr.fit(X_train.values, y_train) pred_lr = lr.predict(X_test.values) print(&#39;Test R-squared for linreg is %f&#39; % r2_score(y_test, pred_lr)) . Test R-squared for linreg is 0.743180 . And the we run LightGBM. . lgb_params = { &#39;feature_fraction&#39;: 0.75, &#39;metric&#39;: &#39;rmse&#39;, &#39;nthread&#39;:1, &#39;min_data_in_leaf&#39;: 2**7, &#39;bagging_fraction&#39;: 0.75, &#39;learning_rate&#39;: 0.03, &#39;objective&#39;: &#39;mse&#39;, &#39;bagging_seed&#39;: 2**7, &#39;num_leaves&#39;: 2**7, &#39;bagging_freq&#39;:1, &#39;verbose&#39;:0 } model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100) pred_lgb = model.predict(X_test) print(&#39;Test R-squared for LightGBM is %f&#39; % r2_score(y_test, pred_lgb)) . Test R-squared for LightGBM is 0.738391 . Finally, concatenate test predictions to get test meta-features. . X_test_level2 = np.c_[pred_lr, pred_lgb] . Train meta-features . Now it is your turn to write the code. You need to implement scheme f) from the reading material. Here, we will use duration T equal to month and M=15. . That is, you need to get predictions (meta-features) from linear regression and LightGBM for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models. . dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])] # That is how we get target for the 2nd level dataset y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])] . X_train_level2 = np.zeros([y_train_level2.shape[0], 2]) # Now fill `X_train_level2` with metafeatures for cur_block_num in [27, 28, 29, 30, 31, 32]: print(cur_block_num) &#39;&#39;&#39; 1. Split `X_train` into parts Remember, that corresponding dates are stored in `dates_train` 2. Fit linear regression 3. Fit LightGBM and put predictions 4. Store predictions from 2. and 3. in the right place of `X_train_level2`. You can use `dates_train_level2` for it Make sure the order of the meta-features is the same as in `X_test_level2` &#39;&#39;&#39; # YOUR CODE GOES HERE X_train_meta = all_data.loc[dates &lt; cur_block_num].drop(to_drop_cols, axis=1) X_test_meta = all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1) y_train_meta = all_data.loc[dates &lt; cur_block_num, &#39;target&#39;].values y_test_meta = all_data.loc[dates == cur_block_num, &#39;target&#39;].values lr.fit(X_train_meta.values, y_train_meta) X_train_level2[dates_train_level2 == cur_block_num, 0] = lr.predict(X_test_meta.values) model = lgb.train(lgb_params, lgb.Dataset(X_train_meta, label=y_train_meta), 100) X_train_level2[dates_train_level2 == cur_block_num, 1] = model.predict(X_test_meta) # Sanity check assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988, 1.38811989])) . 27 28 29 30 31 32 . Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig scatter plot between the two metafeatures. Plot the scatter plot below. . plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1]) . &lt;matplotlib.collections.PathCollection at 0x7fa38c41ca58&gt; . Ensembling . Now, when the meta-features are created, we can ensemble our first level models. . Simple convex mix . Let&#39;s start with simple linear convex mix: . $$ mix= alpha cdot text{linreg_prediction}+(1- alpha) cdot text{lgb_prediction} $$We need to find an optimal $ alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $ alpha$ out of alphas_to_try array. Remember, that you need to use train meta-features (not test) when searching for $ alpha$. . alphas_to_try = np.linspace(0, 1, 1001) # YOUR CODE GOES HERE r2_scores = np.array([r2_score(y_train_level2, np.dot(X_train_level2, [alpha, 1 - alpha])) for alpha in alphas_to_try]) best_alpha = alphas_to_try[r2_scores.argmax()] # YOUR CODE GOES HERE r2_train_simple_mix = r2_scores.max() # YOUR CODE GOES HERE print(&#39;Best alpha: %f; Corresponding r2 score on train: %f&#39; % (best_alpha, r2_train_simple_mix)) . Best alpha: 0.765000; Corresponding r2 score on train: 0.627255 . Now use the $ alpha$ you&#39;ve found to compute predictions for the test set . test_preds = best_alpha * pred_lr + (1 - best_alpha) * pred_lgb # YOUR CODE GOES HERE r2_test_simple_mix = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Test R-squared for simple mix is %f&#39; % r2_test_simple_mix) . Test R-squared for simple mix is 0.781144 . Stacking . Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above. . lr.fit(X_train_level2, y_train_level2) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) . Compute R-squared on the train and test sets. . train_preds = lr.predict(X_train_level2) # YOUR CODE GOES HERE r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE test_preds = lr.predict(np.vstack((pred_lr, pred_lgb)).T) # YOUR CODE GOES HERE r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE print(&#39;Train R-squared for stacking is %f&#39; % r2_train_stacking) print(&#39;Test R-squared for stacking is %f&#39; % r2_test_stacking) . Train R-squared for stacking is 0.632176 Test R-squared for stacking is 0.771297 . Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. Examine and compare train and test scores for the two methods. . And of course this particular case does not mean simple mix is always better than stacking. . We all done! Submit everything we need to the grader now. . from grader import Grader grader = Grader() grader.submit_tag(&#39;best_alpha&#39;, best_alpha) grader.submit_tag(&#39;r2_train_simple_mix&#39;, r2_train_simple_mix) grader.submit_tag(&#39;r2_test_simple_mix&#39;, r2_test_simple_mix) grader.submit_tag(&#39;r2_train_stacking&#39;, r2_train_stacking) grader.submit_tag(&#39;r2_test_stacking&#39;, r2_test_stacking) . Current answer for task best_alpha is: 0.765 Current answer for task r2_train_simple_mix is: 0.627255043446 Current answer for task r2_test_simple_mix is: 0.781144169579 Current answer for task r2_train_stacking is: 0.632175561459 Current answer for task r2_test_stacking is: 0.771297132342 . STUDENT_EMAIL =&quot;EMAIL HERE&quot; # EMAIL HERE STUDENT_TOKEN =&quot; TOKEN HERE&quot;# TOKEN HERE grader.status() . You want to submit these numbers: Task best_alpha: 0.765 Task r2_train_simple_mix: 0.627255043446 Task r2_test_simple_mix: 0.781144169579 Task r2_train_stacking: 0.632175561459 Task r2_test_stacking: 0.771297132342 . grader.submit(STUDENT_EMAIL, STUDENT_TOKEN) . Submitted to Coursera platform. See results on assignment page! .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/04/01/Ensembling_Implementation.html",
            "relUrl": "/2022/04/01/Ensembling_Implementation.html",
            "date": " • Apr 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Z-unlock Challenge: Data Visualization",
            "content": "We will Analyze the correlation of temperatures changes on energy use, land cover,waste use and deforestoration by questioning these questions. . What are the areas with biggest/smallest change in temperature? | Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) | How does the seasonal temperature change look like? | How does this vary by continent? Particularly South America? | . # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; # You can also write temporary files to /kaggle/temp/, but they won&#39;t be saved outside of the current session . /kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv /kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) . df_temperature = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/temperature_change_data_11-29-2021.csv&quot;) df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . temp_max = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].max().sort_values(ascending=False).reset_index() temp_min = df_temperature.groupby(&quot;Area&quot;)[&quot;Value&quot;].min().sort_values().reset_index() d2 = temp_max[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Max temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . d2 = temp_min[:5] plt.figure(figsize=(10, 7)) plt.bar(d2[&#39;Area&#39;], d2[&#39;Value&#39;], width=0.3) for i, val in enumerate(d2[&#39;Value&#39;].values): plt.text(i, val, round(float(val)), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;bottom&#39;, fontdict={&#39;fontweight&#39;:500, &#39;size&#39;: 16}) plt.gca().set_xticklabels(d2[&#39;Area&#39;], fontdict={&#39;size&#39;: 14},rotation=60) plt.title(&quot;Min temperature Change for top 5 Area&quot;, fontsize=22) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Area&quot;, fontsize=16) plt.show() . Biggest/smallest change in temperature: . Svalbard and Jan Mayeb Island is the most change in temperature based on the chart above | . Are there any correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions etc.) . Look at all the possibilities from another dataset/tables | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . land_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/land_cover_data_11-30-2021.csv&quot;) land_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2001 | 2001 | 1000 ha | 88.1603 | FC | Calculated data | . 1 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2002 | 2002 | 1000 ha | 88.1818 | FC | Calculated data | . 2 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2003 | 2003 | 1000 ha | 88.2247 | FC | Calculated data | . 3 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2004 | 2004 | 1000 ha | 88.2462 | FC | Calculated data | . 4 LC | Land Cover | AFG | Afghanistan | 5007 | Area from MODIS | 6970 | Artificial surfaces (including urban and assoc... | 2005 | 2005 | 1000 ha | 88.3106 | FC | Calculated data | . energy_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/energy_use_data_11-29-2021.csv&quot;) energy_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1990 | 1990 | kilotonnes | 231.4918 | F | FAO estimate | . 1 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1991 | 1991 | kilotonnes | 188.5317 | F | FAO estimate | . 2 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1992 | 1992 | kilotonnes | 47.9904 | F | FAO estimate | . 3 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1993 | 1993 | kilotonnes | 38.6116 | F | FAO estimate | . 4 GN | Energy Use | AFG | Afghanistan | 7273 | Emissions (CO2) | 6801 | Gas-Diesel oil | 1994 | 1994 | kilotonnes | 31.4465 | F | FAO estimate | . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . waste_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/waste_disposal_data_11-29-2021.csv&quot;) waste_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Unit Value Flag Flag Description . 0 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1990 | 1990 | kilotonnes | 0.0 | Fc | Calculated data | . 1 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1991 | 1991 | kilotonnes | 0.0 | Fc | Calculated data | . 2 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1992 | 1992 | kilotonnes | 0.0 | Fc | Calculated data | . 3 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1993 | 1993 | kilotonnes | 0.0 | Fc | Calculated data | . 4 GW | Waste Disposal | AFG | Afghanistan | 7273 | Emissions (CO2) | 6990 | Incineration | 1994 | 1994 | kilotonnes | 0.0 | Fc | Calculated data | . fires_df = pd.read_csv(&quot;/kaggle/input/z-unlocked-challenge-1-data-visualization/fires_data_11-29-2021.csv&quot;) fires_df.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Item Code Item Year Code Year Source Code Source Unit Value Flag Flag Description Note . 0 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1990 | 1990 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 1 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1991 | 1991 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 2 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1992 | 1992 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 3 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1993 | 1993 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . 4 GI | Fires | AFG | Afghanistan | 7246 | Burned Area | 6796 | Humid tropical forest | 1994 | 1994 | 3050 | FAO TIER 1 | ha | 0.0 | Fc | Calculated data | NaN | . temp_change= df_temperature.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, hue=&#39;Months&#39;, legend=&#39;full&#39;, data=temp_change, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temp_change.Months.unique()))) max_value_per_year = temp_change.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.title(&quot;The trend for temperature change annually over Months&quot;) plt.axvspan(2015, 2020,alpha=0.15) plt.show() . land_cover= land_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=land_cover, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(land_cover.Year.unique()))) max_value_per_year = land_cover.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Land Cover&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2004, 2006,alpha=0.15) plt.show() . energy_use= energy_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=energy_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(energy_use.Year.unique()))) max_value_per_year = energy_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Energy Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1985, 1989,alpha=0.15) plt.show() . waste_use= waste_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=waste_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(waste_use.Year.unique()))) max_value_per_year = waste_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Waste Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1990, 1993,alpha=0.15) plt.show() . fires_use= fires_df.groupby([&quot;Year&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, data=fires_use, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(fires_use.Year.unique()))) max_value_per_year = fires_use.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Fires Use&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(1999, 2003,alpha=0.15) plt.show() . Correlations between the hottest changes and other phenomena (like land coverage, land fires, CO2 emissions and Fires.) . Insight Based on Aggregating the mean per year shows correlation among temperature, energy use, land cover, waste use, and fires. All country-Value indicator(Value feature based on each tables) combinations show an increase, but there are subtle differences: . In Land cover use, in 2004-2005, there was a signifant increase followed by a slighly increase in from 2011-2017. | In Energy use, in 1985-1989, there was a signifant increase followed by a slighly increase in from 2019-2020. | In Waste use, in 1999-1993, there was a signifant drop followed by a significant increase from 1994-2020. | In Fires use, in 1990-2003, there was a signifant increase followed by a slighly decrease from 2003-2020. . | Almost everywhere, the end-of-year show an correlation that the the temperature that increase yearly affect the use of waste, energy,deforestoration, and land cover yearly. . | . How does the seasonal temperature change look like? . df_temperature.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 0 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | -0.751 | Fc | Calculated data | . 1 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | 0.985 | Fc | Calculated data | . 2 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 1.931 | Fc | Calculated data | . 3 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | -2.056 | Fc | Calculated data | . 4 ET | Temperature change | AFG | Afghanistan | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.669 | Fc | Calculated data | . df_temperature.groupby(&quot;Months&quot;)[&quot;Value&quot;].agg([&quot;sum&quot;,&quot;mean&quot;,&quot;max&quot;]) . sum mean max . Months . Dec–Jan–Feb 6113.952 | 0.467428 | 8.206 | . Jun–Jul–Aug 6951.271 | 0.531890 | 4.764 | . Mar–Apr–May 6872.110 | 0.525511 | 5.533 | . Meteorological year 6413.093 | 0.491651 | 5.328 | . Sep–Oct–Nov 5761.315 | 0.441108 | 6.084 | . plt.figure(figsize=(18, 12)) for i, (combi, df) in enumerate(df_temperature.groupby([&#39;Months&#39;])): ax = plt.subplot(6, 3, i+1, ymargin=0.5) ax.plot(df.Value) ax.set_title(combi) #if i == 6: break plt.tight_layout(h_pad=3.0) plt.suptitle(&#39;Seasonal Temperature Change&#39;, y=1.03) plt.show() . Seasonal Temperature Change . We can see that on each month has different maximum temperrature. DEC-Jan-Feb has the hottest temperature with 8.206 followed by Sept-Oct-Nov. | . How does this vary by continent? Particularly South America? . south_america_countries =[&#39;Brazil&#39;,&#39;Argentina&#39;,&#39;Chile&#39;,&#39;Colombia&#39;, &#39;Ecuador&#39;,&#39;Venezuela (Bolivarian Republic of)&#39;, &#39;Bolivia (Plurinational State of)&#39;,&#39;Guyana&#39;, &#39;Uruguay&#39;,&#39;Suriname&#39;, &#39;Paraguay&#39;,&#39;Aruba&#39;,&#39;Trinidad and Tobago&#39;] temperature_sa =df_temperature[df_temperature[&quot;Area&quot;].isin(south_america_countries)] temperature_sa.head() . Domain Code Domain Area Code (ISO3) Area Element Code Element Months Code Months Year Code Year Unit Value Flag Flag Description . 2700 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1961 | 1961 | °C | 0.035 | Fc | Calculated data | . 2701 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1962 | 1962 | °C | -0.144 | Fc | Calculated data | . 2702 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1963 | 1963 | °C | 0.552 | Fc | Calculated data | . 2703 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1964 | 1964 | °C | 0.052 | Fc | Calculated data | . 2704 ET | Temperature change | ARG | Argentina | 7271 | Temperature change | 7016 | Dec–Jan–Feb | 1965 | 1965 | °C | -0.034 | Fc | Calculated data | . temperature_sa.groupby([&quot;Area&quot;])[&quot;Value&quot;].agg([&quot;max&quot;,&quot;min&quot;]).plot(kind=&quot;bar&quot;,figsize=(12,8)) plt.ylabel(&quot;Temperature&quot;) . Text(0, 0.5, &#39;Temperature&#39;) . How about Meterological season temperature changes in South America? . temperature_sa= temperature_sa.groupby([&quot;Year&quot;,&quot;Months&quot;])[&quot;Value&quot;].mean().reset_index() plt.figure(figsize=(15, 10)) ax = sns.scatterplot(x=&#39;Year&#39;, y=&#39;Value&#39;, legend=&#39;full&#39;, hue=&#39;Months&#39;, data=temperature_sa, palette=sns.color_palette(&quot;Set1&quot;, n_colors=len(temperature_sa.Months.unique()))) max_value_per_year = temperature_sa.groupby(&#39;Year&#39;)[&#39;Value&#39;].max() sns.lineplot(data=max_value_per_year, ax=ax.axes, color=&#39;black&#39;) plt.ylabel(&quot;Temperature Change&quot;, fontsize=16) plt.xlabel(&quot;Year&quot;, fontsize=16) plt.axvspan(2013, 2016,alpha=0.15) plt.show() . Ultimately, there is an uptrend for temperature change in South America annually in which the peak is around 2013-2016. | . For joining this competition, see Z-Unlocked_Challenge1. There is a chance to visit Barcelona for Kaggle Competition. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/2022/03/30/data-visualization-challenge.html",
            "relUrl": "/2022/03/30/data-visualization-challenge.html",
            "date": " • Mar 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Model Design using Pytorch",
            "content": "Source: 3Blue1Brown . Neural Network has been evolving recently. Many applications have been developed by using neural network. The development of GPU provided by big company like NVIDIA has fasten how the training process in Architecture of Neural network that needs many parameters in order to get better result for any kind of problems. You can see on the GIf above how neural network works throguh many layers that involve many parameters that can create good output that can identify the real value. The practical way is image identification where Neural network through combining many layers and parameters, activation function, and loss that can be improved to identify the image based on the GIF above. We will learn the implementation through Pytorch in this tutorial. . Study Case 1 . You work as an assistant of the mayor of Somerville and the HR department has asked you to build a model capable of predicting whether a person is happy with the current administration based on their satisfaction with the city&#39;s services . import pandas as pd import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.optim as optim import warnings warnings.filterwarnings(&quot;ignore&quot;) . df = pd.read_csv(&quot;SomervilleHappinessSurvey2015.csv&quot;) df.head() . D X1 X2 X3 X4 X5 X6 . 0 0 | 3 | 3 | 3 | 4 | 2 | 4 | . 1 0 | 3 | 2 | 3 | 5 | 4 | 3 | . 2 1 | 5 | 3 | 3 | 3 | 3 | 5 | . 3 0 | 5 | 4 | 3 | 3 | 3 | 5 | . 4 0 | 5 | 4 | 3 | 3 | 3 | 5 | . Columns Information: . D = decision attribute (D) with values 0 (unhappy) and 1 (happy) | X1 = the availability of information about the city services | X2 = the cost of housing | X3 = the overall quality of public schools | X4 = your trust in the local police | X5 = the maintenance of streets and sidewalks | X6 = the availability of social community events . | Attributes X1 to X6 have values 1 to 5. . | . X = torch.tensor(df.drop(&quot;D&quot;,axis=1).astype(np.float32).values) y = torch.tensor(df[&quot;D&quot;].astype(np.float32).values) . X[:10] . tensor([[3., 3., 3., 4., 2., 4.], [3., 2., 3., 5., 4., 3.], [5., 3., 3., 3., 3., 5.], [5., 4., 3., 3., 3., 5.], [5., 4., 3., 3., 3., 5.], [5., 5., 3., 5., 5., 5.], [3., 1., 2., 2., 1., 3.], [5., 4., 4., 4., 4., 5.], [4., 1., 4., 4., 4., 4.], [4., 4., 4., 2., 5., 5.]]) . # 6 from how many features we have # output whether happy or unhappy model = nn.Sequential(nn.Linear(6,1), nn.Sigmoid()) print(model) . Sequential( (0): Linear(in_features=6, out_features=1, bias=True) (1): Sigmoid() ) . loss_func = nn.MSELoss() optimizer = optim.Adam(model.parameters(),lr=1e-2) . losses =[] for i in range(20): y_pred = model(X) loss = loss_func(y_pred,y) # item() is used for getting value from tensor losses.append(loss.item()) optimizer.zero_grad() #do back propagation loss.backward() #update weights during backward propagation optimizer.step() if i%5 ==0: print(i,loss.item()) . 0 0.5094006061553955 5 0.46509113907814026 10 0.3730385899543762 15 0.26985085010528564 . Plot the loss for each epochs . plt.plot(range(0,20),losses) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Loss&quot;) . Text(0, 0.5, &#39;Loss&#39;) . This is just a simple sequential neural network by implementing pytorch. We will deep dive into a process of building model in the study case 2 where we implement the process of Data Science lifecycle from cleaning the data,splitting the data, making the prediction and evaluating the prediction. . Study Case 2 . Deep Learning in Bank . Deep Learning has been implementing in many sectors including Bank.The problem thas has been happening for this sector is to predict whether bank should grant loan for the customers who will be making credit card. This is essential for Bank because it can measure how they can validate how much money that they can provide and estimate the profit from customers who will use the credit card based on a period of time. We will detect the customers who will be potential to grant loan that can affect to the income of the bank through this dataset. . We will follow a few steps before modelling our data into ANN using Pytorch including : . Understand the data including dealing with quality of data | rescale the features (giving different scales for each features may result that a given features is more important thatn others as it has higher numerical values) | split the data | . df_credit = pd.read_excel(&quot;default of credit card clients.xls&quot;,skiprows=1) df_credit.head() . ID LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 1 | 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 2 | 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 3 | 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 4 | 50000 | 2 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 5 | 50000 | 1 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 25 columns . print(f&quot;Rows : {df_credit.shape[0]}, Columns:{df_credit.shape[1]}&quot;) . Rows : 30000, Columns:25 . data_clean =df_credit.drop([&quot;ID&quot;,&quot;SEX&quot;],axis=1) data_clean.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | -2 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | 2 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 50000 | 2 | 1 | 37 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 28314 | 28959 | 29547 | 2000 | 2019 | 1200 | 1100 | 1069 | 1000 | 0 | . 4 50000 | 2 | 1 | 57 | -1 | 0 | -1 | 0 | 0 | 0 | ... | 20940 | 19146 | 19131 | 2000 | 36681 | 10000 | 9000 | 689 | 679 | 0 | . 5 rows × 23 columns . (data_clean.isnull().sum()/data_clean.shape[0]).plot() . &lt;AxesSubplot:&gt; . data_clean.describe() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . count 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | ... | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | 3.000000e+04 | 30000.00000 | 30000.000000 | 30000.000000 | 30000.000000 | 30000.000000 | . mean 167484.322667 | 1.853133 | 1.551867 | 35.485500 | -0.016700 | -0.133767 | -0.166200 | -0.220667 | -0.266200 | -0.291100 | ... | 43262.948967 | 40311.400967 | 38871.760400 | 5663.580500 | 5.921163e+03 | 5225.68150 | 4826.076867 | 4799.387633 | 5215.502567 | 0.221200 | . std 129747.661567 | 0.790349 | 0.521970 | 9.217904 | 1.123802 | 1.197186 | 1.196868 | 1.169139 | 1.133187 | 1.149988 | ... | 64332.856134 | 60797.155770 | 59554.107537 | 16563.280354 | 2.304087e+04 | 17606.96147 | 15666.159744 | 15278.305679 | 17777.465775 | 0.415062 | . min 10000.000000 | 0.000000 | 0.000000 | 21.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | -2.000000 | ... | -170000.000000 | -81334.000000 | -339603.000000 | 0.000000 | 0.000000e+00 | 0.00000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 50000.000000 | 1.000000 | 1.000000 | 28.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | -1.000000 | ... | 2326.750000 | 1763.000000 | 1256.000000 | 1000.000000 | 8.330000e+02 | 390.00000 | 296.000000 | 252.500000 | 117.750000 | 0.000000 | . 50% 140000.000000 | 2.000000 | 2.000000 | 34.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 19052.000000 | 18104.500000 | 17071.000000 | 2100.000000 | 2.009000e+03 | 1800.00000 | 1500.000000 | 1500.000000 | 1500.000000 | 0.000000 | . 75% 240000.000000 | 2.000000 | 2.000000 | 41.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 54506.000000 | 50190.500000 | 49198.250000 | 5006.000000 | 5.000000e+03 | 4505.00000 | 4013.250000 | 4031.500000 | 4000.000000 | 0.000000 | . max 1000000.000000 | 6.000000 | 3.000000 | 79.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | 8.000000 | ... | 891586.000000 | 927171.000000 | 961664.000000 | 873552.000000 | 1.684259e+06 | 896040.00000 | 621000.000000 | 426529.000000 | 528666.000000 | 1.000000 | . 8 rows × 23 columns . outliers ={} for i in range(data_clean.shape[1]): min_t = data_clean[data_clean.columns[i]].mean()-(3*data_clean[data_clean.columns[i]].std()) max_t = data_clean[data_clean.columns[i]].mean()+(3*data_clean[data_clean.columns[i]].std()) count =0 for j in data_clean[data_clean.columns[i]]: if j &lt; min_t or j &gt; max_t: count +=1 percentage = count/data_clean.shape[0] outliers[data_clean.columns[i]] = round(percentage,3) . from pprint import pprint pprint(outliers) . {&#39;AGE&#39;: 0.005, &#39;BILL_AMT1&#39;: 0.023, &#39;BILL_AMT2&#39;: 0.022, &#39;BILL_AMT3&#39;: 0.022, &#39;BILL_AMT4&#39;: 0.023, &#39;BILL_AMT5&#39;: 0.022, &#39;BILL_AMT6&#39;: 0.022, &#39;EDUCATION&#39;: 0.011, &#39;LIMIT_BAL&#39;: 0.004, &#39;MARRIAGE&#39;: 0.0, &#39;PAY_0&#39;: 0.005, &#39;PAY_2&#39;: 0.005, &#39;PAY_3&#39;: 0.005, &#39;PAY_4&#39;: 0.006, &#39;PAY_5&#39;: 0.005, &#39;PAY_6&#39;: 0.004, &#39;PAY_AMT1&#39;: 0.013, &#39;PAY_AMT2&#39;: 0.01, &#39;PAY_AMT3&#39;: 0.012, &#39;PAY_AMT4&#39;: 0.013, &#39;PAY_AMT5&#39;: 0.014, &#39;PAY_AMT6&#39;: 0.015, &#39;default payment next month&#39;: 0.0} . data_clean[&quot;default payment next month&quot;].value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . target = data_clean[&quot;default payment next month&quot;] yes = target[target == 1].count() no = target[target == 0].count() . data_yes = data_clean[data_clean[&quot;default payment next month&quot;] == 1] data_no = data_clean[data_clean[&quot;default payment next month&quot;] == 0] over_sampling = data_yes.sample(no, replace=True, random_state = 0) data_resampled = pd.concat([data_no, over_sampling], axis=0) . data_resampled[&quot;default payment next month&quot;].value_counts().plot(kind=&quot;bar&quot;) . &lt;AxesSubplot:&gt; . data_resampled = data_resampled.reset_index(drop=True) X = data_resampled.drop(&quot;default payment next month&quot;,axis=1) y =data_resampled[&quot;default payment next month&quot;] . X = (X-X.min())/(X.max()-X.min()) X.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 . 0 0.080808 | 0.333333 | 0.666667 | 0.224138 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.093789 | 0.173637 | 0.095470 | 0.272928 | 0.001738 | 0.000891 | 0.001116 | 0.001610 | 0.002345 | 0.009458 | . 1 0.040404 | 0.333333 | 0.333333 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.113407 | 0.186809 | 0.109363 | 0.283685 | 0.002290 | 0.001199 | 0.001339 | 0.001771 | 0.002506 | 0.001892 | . 2 0.040404 | 0.333333 | 0.333333 | 0.620690 | 0.1 | 0.2 | 0.1 | 0.2 | 0.2 | 0.2 | ... | 0.106020 | 0.179863 | 0.099633 | 0.275681 | 0.002290 | 0.021779 | 0.011160 | 0.014493 | 0.001615 | 0.001284 | . 3 0.040404 | 0.166667 | 0.666667 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.117974 | 0.178407 | 0.100102 | 0.276367 | 0.002862 | 0.001078 | 0.000733 | 0.001610 | 0.002345 | 0.001513 | . 4 0.494949 | 0.166667 | 0.666667 | 0.137931 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.330672 | 0.671310 | 0.559578 | 0.625196 | 0.062961 | 0.023749 | 0.042409 | 0.032591 | 0.032237 | 0.026047 | . 5 rows × 22 columns . final_data =pd.concat([X,y],axis=1) final_data.to_csv(&quot;data_prepared.csv&quot;,index=False) . Build Model . import torch.nn.functional as F from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.metrics import accuracy_score . data = pd.read_csv(&quot;data_prepared.csv&quot;) data.head() . LIMIT_BAL EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 0.080808 | 0.333333 | 0.666667 | 0.224138 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.173637 | 0.095470 | 0.272928 | 0.001738 | 0.000891 | 0.001116 | 0.001610 | 0.002345 | 0.009458 | 0 | . 1 0.040404 | 0.333333 | 0.333333 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.186809 | 0.109363 | 0.283685 | 0.002290 | 0.001199 | 0.001339 | 0.001771 | 0.002506 | 0.001892 | 0 | . 2 0.040404 | 0.333333 | 0.333333 | 0.620690 | 0.1 | 0.2 | 0.1 | 0.2 | 0.2 | 0.2 | ... | 0.179863 | 0.099633 | 0.275681 | 0.002290 | 0.021779 | 0.011160 | 0.014493 | 0.001615 | 0.001284 | 0 | . 3 0.040404 | 0.166667 | 0.666667 | 0.275862 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.178407 | 0.100102 | 0.276367 | 0.002862 | 0.001078 | 0.000733 | 0.001610 | 0.002345 | 0.001513 | 0 | . 4 0.494949 | 0.166667 | 0.666667 | 0.137931 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | ... | 0.671310 | 0.559578 | 0.625196 | 0.062961 | 0.023749 | 0.042409 | 0.032591 | 0.032237 | 0.026047 | 0 | . 5 rows × 23 columns . X = data.drop(&quot;default payment next month&quot;,axis=1) y =data[&quot;default payment next month&quot;] . X_new , X_test,y_new,y_test =train_test_split(X,y,test_size=0.2,random_state=3) dev_per = X_test.shape[0]/X_new.shape[0] X_train,X_dev,y_train,y_dev = train_test_split(X_new,y_new,test_size=dev_per,random_state=3) . print(&quot;Training sets:&quot;,X_train.shape, y_train.shape) print(&quot;Validation sets:&quot;,X_dev.shape, y_dev.shape) print(&quot;Testing sets:&quot;,X_test.shape, y_test.shape) . Training sets: (28036, 22) (28036,) Validation sets: (9346, 22) (9346,) Testing sets: (9346, 22) (9346,) . X_dev_torch = torch.tensor(X_dev.values).float() y_dev_torch = torch.tensor(y_dev.values) X_test_torch = torch.tensor(X_test.values).float() y_test_torch = torch.tensor(y_test.values) . class Classifier(nn.Module): def __init__(self, input_size): super().__init__() self.hidden_1 = nn.Linear(input_size, 10) self.hidden_2 = nn.Linear(10, 10) self.hidden_3 = nn.Linear(10, 10) self.output = nn.Linear(10, 2) def forward(self, x): z = F.relu(self.hidden_1(x)) z = F.relu(self.hidden_2(z)) z = F.relu(self.hidden_3(z)) out = F.log_softmax(self.output(z), dim=1) return out . model = Classifier(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) epochs = 50 batch_size = 128 # for faster training procces/mini batch gradient descent . train_losses,dev_losses,train_acc,dev_acc =[],[],[],[] for epoch in range(epochs): X_,y_ =shuffle(X_train,y_train) running_loss=0 running_acc=0 iterations =0 for i in range(0,len(X_),batch_size): iterations +=1 b = i +batch_size X_batch = torch.tensor(X_.iloc[i:b,:].values).float() y_batch = torch.tensor(y_.iloc[i:b].values) pred = model(X_batch) loss = criterion(pred,y_batch) optimizer.zero_grad() loss.backward() optimizer.step() running_loss +=loss.item() ps = torch.exp(pred) top_p,top_class = ps.topk(1,dim=1) running_acc +=accuracy_score(y_batch,top_class) dev_loss =0 acc =0 with torch.no_grad(): pred_dev = model(X_dev_torch) dev_loss =criterion(pred_dev,y_dev_torch) ps_dev = torch.exp(pred_dev) top_p,top_class_dev = ps_dev.topk(1,dim=1) acc +=accuracy_score(y_dev_torch,top_class_dev) train_losses.append(running_loss/iterations) dev_losses.append(dev_loss) train_acc.append(running_acc/iterations) dev_acc.append(acc) print(&quot;Epoch: {}/{}.. &quot;.format(epoch+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/iterations), &quot;Validation Loss: {:.3f}.. &quot;.format(dev_loss), &quot;Training Accuracy: {:.3f}.. &quot;.format(running_acc/iterations), &quot;Validation Accuracy: {:.3f}&quot;.format(acc)) . Epoch: 1/50.. Training Loss: 0.675.. Validation Loss: 0.617.. Training Accuracy: 0.602.. Validation Accuracy: 0.663 Epoch: 2/50.. Training Loss: 0.607.. Validation Loss: 0.600.. Training Accuracy: 0.676.. Validation Accuracy: 0.686 Epoch: 3/50.. Training Loss: 0.598.. Validation Loss: 0.599.. Training Accuracy: 0.686.. Validation Accuracy: 0.688 Epoch: 4/50.. Training Loss: 0.595.. Validation Loss: 0.592.. Training Accuracy: 0.691.. Validation Accuracy: 0.695 Epoch: 5/50.. Training Loss: 0.592.. Validation Loss: 0.590.. Training Accuracy: 0.692.. Validation Accuracy: 0.693 Epoch: 6/50.. Training Loss: 0.589.. Validation Loss: 0.586.. Training Accuracy: 0.692.. Validation Accuracy: 0.693 Epoch: 7/50.. Training Loss: 0.585.. Validation Loss: 0.584.. Training Accuracy: 0.695.. Validation Accuracy: 0.696 Epoch: 8/50.. Training Loss: 0.583.. Validation Loss: 0.579.. Training Accuracy: 0.696.. Validation Accuracy: 0.700 Epoch: 9/50.. Training Loss: 0.576.. Validation Loss: 0.575.. Training Accuracy: 0.701.. Validation Accuracy: 0.703 Epoch: 10/50.. Training Loss: 0.572.. Validation Loss: 0.572.. Training Accuracy: 0.704.. Validation Accuracy: 0.707 Epoch: 11/50.. Training Loss: 0.572.. Validation Loss: 0.571.. Training Accuracy: 0.705.. Validation Accuracy: 0.709 Epoch: 12/50.. Training Loss: 0.570.. Validation Loss: 0.569.. Training Accuracy: 0.704.. Validation Accuracy: 0.709 Epoch: 13/50.. Training Loss: 0.569.. Validation Loss: 0.568.. Training Accuracy: 0.708.. Validation Accuracy: 0.709 Epoch: 14/50.. Training Loss: 0.567.. Validation Loss: 0.568.. Training Accuracy: 0.708.. Validation Accuracy: 0.709 Epoch: 15/50.. Training Loss: 0.566.. Validation Loss: 0.566.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 16/50.. Training Loss: 0.566.. Validation Loss: 0.567.. Training Accuracy: 0.708.. Validation Accuracy: 0.710 Epoch: 17/50.. Training Loss: 0.564.. Validation Loss: 0.566.. Training Accuracy: 0.711.. Validation Accuracy: 0.708 Epoch: 18/50.. Training Loss: 0.566.. Validation Loss: 0.564.. Training Accuracy: 0.707.. Validation Accuracy: 0.710 Epoch: 19/50.. Training Loss: 0.564.. Validation Loss: 0.564.. Training Accuracy: 0.712.. Validation Accuracy: 0.708 Epoch: 20/50.. Training Loss: 0.565.. Validation Loss: 0.564.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 21/50.. Training Loss: 0.562.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.707 Epoch: 22/50.. Training Loss: 0.563.. Validation Loss: 0.562.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 23/50.. Training Loss: 0.561.. Validation Loss: 0.564.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 24/50.. Training Loss: 0.562.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.709 Epoch: 25/50.. Training Loss: 0.561.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 26/50.. Training Loss: 0.562.. Validation Loss: 0.562.. Training Accuracy: 0.708.. Validation Accuracy: 0.707 Epoch: 27/50.. Training Loss: 0.562.. Validation Loss: 0.563.. Training Accuracy: 0.710.. Validation Accuracy: 0.709 Epoch: 28/50.. Training Loss: 0.560.. Validation Loss: 0.565.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 29/50.. Training Loss: 0.561.. Validation Loss: 0.562.. Training Accuracy: 0.711.. Validation Accuracy: 0.709 Epoch: 30/50.. Training Loss: 0.560.. Validation Loss: 0.564.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 31/50.. Training Loss: 0.561.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.709 Epoch: 32/50.. Training Loss: 0.560.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.706 Epoch: 33/50.. Training Loss: 0.559.. Validation Loss: 0.561.. Training Accuracy: 0.711.. Validation Accuracy: 0.709 Epoch: 34/50.. Training Loss: 0.559.. Validation Loss: 0.561.. Training Accuracy: 0.714.. Validation Accuracy: 0.710 Epoch: 35/50.. Training Loss: 0.560.. Validation Loss: 0.561.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 36/50.. Training Loss: 0.562.. Validation Loss: 0.559.. Training Accuracy: 0.710.. Validation Accuracy: 0.710 Epoch: 37/50.. Training Loss: 0.560.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.708 Epoch: 38/50.. Training Loss: 0.557.. Validation Loss: 0.563.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 39/50.. Training Loss: 0.561.. Validation Loss: 0.559.. Training Accuracy: 0.712.. Validation Accuracy: 0.711 Epoch: 40/50.. Training Loss: 0.560.. Validation Loss: 0.560.. Training Accuracy: 0.710.. Validation Accuracy: 0.708 Epoch: 41/50.. Training Loss: 0.559.. Validation Loss: 0.559.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 42/50.. Training Loss: 0.557.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.710 Epoch: 43/50.. Training Loss: 0.556.. Validation Loss: 0.559.. Training Accuracy: 0.714.. Validation Accuracy: 0.709 Epoch: 44/50.. Training Loss: 0.557.. Validation Loss: 0.563.. Training Accuracy: 0.714.. Validation Accuracy: 0.701 Epoch: 45/50.. Training Loss: 0.557.. Validation Loss: 0.558.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 46/50.. Training Loss: 0.560.. Validation Loss: 0.559.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 47/50.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 48/50.. Training Loss: 0.556.. Validation Loss: 0.558.. Training Accuracy: 0.714.. Validation Accuracy: 0.714 Epoch: 49/50.. Training Loss: 0.556.. Validation Loss: 0.564.. Training Accuracy: 0.714.. Validation Accuracy: 0.699 Epoch: 50/50.. Training Loss: 0.559.. Validation Loss: 0.560.. Training Accuracy: 0.712.. Validation Accuracy: 0.711 . fig = plt.subplots(figsize=(15,5)) plt.plot(train_losses,label=&quot;Training loss&quot;) plt.plot(dev_losses,label=&quot;Validation loss&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27ee31061c0&gt; . fig = plt.subplots(figsize=(15,5)) plt.plot(train_acc,label=&quot;Training accuracy&quot;) plt.plot(dev_acc,label=&quot;Validation accuracy&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efa576d30&gt; . We can see there is a change in the loss and accuracy accuracy during each epoch. We can do tune the learning rate for getting better result. You can do the experimentation by comparing the LR as well. This is a few steps in modelling using pytorch. You can see that we can do modelling by using Sequential or custom models using torch.nn. . # by adding hidden layer or epochs for training process class Classifier_Layer(nn.Module): def __init__(self, input_size): super().__init__() self.hidden_1 = nn.Linear(input_size, 10) self.hidden_2 = nn.Linear(10, 10) self.hidden_3 = nn.Linear(10, 10) self.hidden_4 = nn.Linear(10, 10) self.output = nn.Linear(10, 2) def forward(self, x): z = F.relu(self.hidden_1(x)) z = F.relu(self.hidden_2(z)) z = F.relu(self.hidden_3(z)) z = F.relu(self.hidden_4(z)) out = F.log_softmax(self.output(z), dim=1) return out . model = Classifier_Layer(X_train.shape[1]) criterion = nn.NLLLoss() optimizer = optim.Adam(model.parameters(), lr=0.01) epochs = 100 batch_size = 150 # for faster training procces/mini batch gradient descent . train_losses,dev_losses,train_acc,dev_acc =[],[],[],[] for epoch in range(epochs): X_,y_ =shuffle(X_train,y_train) running_loss=0 running_acc=0 iterations =0 for i in range(0,len(X_),batch_size): iterations +=1 b = i +batch_size X_batch = torch.tensor(X_.iloc[i:b,:].values).float() y_batch = torch.tensor(y_.iloc[i:b].values) pred = model(X_batch) loss = criterion(pred,y_batch) optimizer.zero_grad() loss.backward() optimizer.step() running_loss +=loss.item() ps = torch.exp(pred) top_p,top_class = ps.topk(1,dim=1) running_acc +=accuracy_score(y_batch,top_class) dev_loss =0 acc =0 with torch.no_grad(): pred_dev = model(X_dev_torch) dev_loss =criterion(pred_dev,y_dev_torch) ps_dev = torch.exp(pred_dev) top_p,top_class_dev = ps_dev.topk(1,dim=1) acc +=accuracy_score(y_dev_torch,top_class_dev) train_losses.append(running_loss/iterations) dev_losses.append(dev_loss) train_acc.append(running_acc/iterations) dev_acc.append(acc) print(&quot;Epoch: {}/{}.. &quot;.format(epoch+1, epochs), &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/iterations), &quot;Validation Loss: {:.3f}.. &quot;.format(dev_loss), &quot;Training Accuracy: {:.3f}.. &quot;.format(running_acc/iterations), &quot;Validation Accuracy: {:.3f}&quot;.format(acc)) . Epoch: 1/100.. Training Loss: 0.618.. Validation Loss: 0.595.. Training Accuracy: 0.662.. Validation Accuracy: 0.688 Epoch: 2/100.. Training Loss: 0.594.. Validation Loss: 0.587.. Training Accuracy: 0.686.. Validation Accuracy: 0.690 Epoch: 3/100.. Training Loss: 0.588.. Validation Loss: 0.580.. Training Accuracy: 0.687.. Validation Accuracy: 0.693 Epoch: 4/100.. Training Loss: 0.584.. Validation Loss: 0.583.. Training Accuracy: 0.692.. Validation Accuracy: 0.692 Epoch: 5/100.. Training Loss: 0.583.. Validation Loss: 0.577.. Training Accuracy: 0.695.. Validation Accuracy: 0.694 Epoch: 6/100.. Training Loss: 0.580.. Validation Loss: 0.573.. Training Accuracy: 0.696.. Validation Accuracy: 0.703 Epoch: 7/100.. Training Loss: 0.575.. Validation Loss: 0.569.. Training Accuracy: 0.703.. Validation Accuracy: 0.704 Epoch: 8/100.. Training Loss: 0.574.. Validation Loss: 0.574.. Training Accuracy: 0.704.. Validation Accuracy: 0.709 Epoch: 9/100.. Training Loss: 0.571.. Validation Loss: 0.582.. Training Accuracy: 0.705.. Validation Accuracy: 0.708 Epoch: 10/100.. Training Loss: 0.571.. Validation Loss: 0.564.. Training Accuracy: 0.706.. Validation Accuracy: 0.712 Epoch: 11/100.. Training Loss: 0.569.. Validation Loss: 0.565.. Training Accuracy: 0.707.. Validation Accuracy: 0.712 Epoch: 12/100.. Training Loss: 0.569.. Validation Loss: 0.568.. Training Accuracy: 0.707.. Validation Accuracy: 0.705 Epoch: 13/100.. Training Loss: 0.566.. Validation Loss: 0.569.. Training Accuracy: 0.710.. Validation Accuracy: 0.706 Epoch: 14/100.. Training Loss: 0.566.. Validation Loss: 0.563.. Training Accuracy: 0.709.. Validation Accuracy: 0.713 Epoch: 15/100.. Training Loss: 0.566.. Validation Loss: 0.561.. Training Accuracy: 0.709.. Validation Accuracy: 0.711 Epoch: 16/100.. Training Loss: 0.564.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.715 Epoch: 17/100.. Training Loss: 0.563.. Validation Loss: 0.562.. Training Accuracy: 0.712.. Validation Accuracy: 0.713 Epoch: 18/100.. Training Loss: 0.566.. Validation Loss: 0.572.. Training Accuracy: 0.708.. Validation Accuracy: 0.701 Epoch: 19/100.. Training Loss: 0.564.. Validation Loss: 0.565.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 20/100.. Training Loss: 0.564.. Validation Loss: 0.561.. Training Accuracy: 0.712.. Validation Accuracy: 0.712 Epoch: 21/100.. Training Loss: 0.562.. Validation Loss: 0.566.. Training Accuracy: 0.710.. Validation Accuracy: 0.706 Epoch: 22/100.. Training Loss: 0.563.. Validation Loss: 0.571.. Training Accuracy: 0.711.. Validation Accuracy: 0.705 Epoch: 23/100.. Training Loss: 0.563.. Validation Loss: 0.563.. Training Accuracy: 0.710.. Validation Accuracy: 0.711 Epoch: 24/100.. Training Loss: 0.559.. Validation Loss: 0.562.. Training Accuracy: 0.711.. Validation Accuracy: 0.710 Epoch: 25/100.. Training Loss: 0.561.. Validation Loss: 0.564.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 26/100.. Training Loss: 0.561.. Validation Loss: 0.565.. Training Accuracy: 0.710.. Validation Accuracy: 0.704 Epoch: 27/100.. Training Loss: 0.560.. Validation Loss: 0.563.. Training Accuracy: 0.712.. Validation Accuracy: 0.714 Epoch: 28/100.. Training Loss: 0.560.. Validation Loss: 0.566.. Training Accuracy: 0.715.. Validation Accuracy: 0.701 Epoch: 29/100.. Training Loss: 0.559.. Validation Loss: 0.568.. Training Accuracy: 0.712.. Validation Accuracy: 0.710 Epoch: 30/100.. Training Loss: 0.561.. Validation Loss: 0.566.. Training Accuracy: 0.711.. Validation Accuracy: 0.712 Epoch: 31/100.. Training Loss: 0.559.. Validation Loss: 0.560.. Training Accuracy: 0.712.. Validation Accuracy: 0.716 Epoch: 32/100.. Training Loss: 0.558.. Validation Loss: 0.561.. Training Accuracy: 0.713.. Validation Accuracy: 0.715 Epoch: 33/100.. Training Loss: 0.560.. Validation Loss: 0.559.. Training Accuracy: 0.711.. Validation Accuracy: 0.713 Epoch: 34/100.. Training Loss: 0.558.. Validation Loss: 0.565.. Training Accuracy: 0.714.. Validation Accuracy: 0.705 Epoch: 35/100.. Training Loss: 0.560.. Validation Loss: 0.560.. Training Accuracy: 0.711.. Validation Accuracy: 0.713 Epoch: 36/100.. Training Loss: 0.557.. Validation Loss: 0.556.. Training Accuracy: 0.714.. Validation Accuracy: 0.715 Epoch: 37/100.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.713 Epoch: 38/100.. Training Loss: 0.557.. Validation Loss: 0.565.. Training Accuracy: 0.712.. Validation Accuracy: 0.714 Epoch: 39/100.. Training Loss: 0.559.. Validation Loss: 0.559.. Training Accuracy: 0.711.. Validation Accuracy: 0.714 Epoch: 40/100.. Training Loss: 0.559.. Validation Loss: 0.566.. Training Accuracy: 0.714.. Validation Accuracy: 0.713 Epoch: 41/100.. Training Loss: 0.556.. Validation Loss: 0.559.. Training Accuracy: 0.714.. Validation Accuracy: 0.713 Epoch: 42/100.. Training Loss: 0.557.. Validation Loss: 0.575.. Training Accuracy: 0.714.. Validation Accuracy: 0.716 Epoch: 43/100.. Training Loss: 0.558.. Validation Loss: 0.559.. Training Accuracy: 0.713.. Validation Accuracy: 0.715 Epoch: 44/100.. Training Loss: 0.555.. Validation Loss: 0.556.. Training Accuracy: 0.716.. Validation Accuracy: 0.715 Epoch: 45/100.. Training Loss: 0.557.. Validation Loss: 0.560.. Training Accuracy: 0.714.. Validation Accuracy: 0.715 Epoch: 46/100.. Training Loss: 0.556.. Validation Loss: 0.560.. Training Accuracy: 0.715.. Validation Accuracy: 0.714 Epoch: 47/100.. Training Loss: 0.556.. Validation Loss: 0.561.. Training Accuracy: 0.715.. Validation Accuracy: 0.709 Epoch: 48/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.714 Epoch: 49/100.. Training Loss: 0.555.. Validation Loss: 0.561.. Training Accuracy: 0.712.. Validation Accuracy: 0.713 Epoch: 50/100.. Training Loss: 0.555.. Validation Loss: 0.564.. Training Accuracy: 0.715.. Validation Accuracy: 0.703 Epoch: 51/100.. Training Loss: 0.556.. Validation Loss: 0.566.. Training Accuracy: 0.713.. Validation Accuracy: 0.699 Epoch: 52/100.. Training Loss: 0.555.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 53/100.. Training Loss: 0.553.. Validation Loss: 0.554.. Training Accuracy: 0.713.. Validation Accuracy: 0.716 Epoch: 54/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 55/100.. Training Loss: 0.555.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 56/100.. Training Loss: 0.552.. Validation Loss: 0.558.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 57/100.. Training Loss: 0.555.. Validation Loss: 0.557.. Training Accuracy: 0.713.. Validation Accuracy: 0.712 Epoch: 58/100.. Training Loss: 0.554.. Validation Loss: 0.565.. Training Accuracy: 0.716.. Validation Accuracy: 0.699 Epoch: 59/100.. Training Loss: 0.554.. Validation Loss: 0.560.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 60/100.. Training Loss: 0.554.. Validation Loss: 0.557.. Training Accuracy: 0.713.. Validation Accuracy: 0.711 Epoch: 61/100.. Training Loss: 0.552.. Validation Loss: 0.554.. Training Accuracy: 0.715.. Validation Accuracy: 0.713 Epoch: 62/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 63/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.716 Epoch: 64/100.. Training Loss: 0.554.. Validation Loss: 0.555.. Training Accuracy: 0.715.. Validation Accuracy: 0.717 Epoch: 65/100.. Training Loss: 0.555.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 66/100.. Training Loss: 0.552.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.716 Epoch: 67/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.717.. Validation Accuracy: 0.713 Epoch: 68/100.. Training Loss: 0.552.. Validation Loss: 0.563.. Training Accuracy: 0.717.. Validation Accuracy: 0.705 Epoch: 69/100.. Training Loss: 0.552.. Validation Loss: 0.561.. Training Accuracy: 0.715.. Validation Accuracy: 0.705 Epoch: 70/100.. Training Loss: 0.553.. Validation Loss: 0.558.. Training Accuracy: 0.714.. Validation Accuracy: 0.716 Epoch: 71/100.. Training Loss: 0.554.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 72/100.. Training Loss: 0.553.. Validation Loss: 0.557.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 73/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 74/100.. Training Loss: 0.552.. Validation Loss: 0.562.. Training Accuracy: 0.716.. Validation Accuracy: 0.696 Epoch: 75/100.. Training Loss: 0.553.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.715 Epoch: 76/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.717.. Validation Accuracy: 0.715 Epoch: 77/100.. Training Loss: 0.550.. Validation Loss: 0.556.. Training Accuracy: 0.718.. Validation Accuracy: 0.714 Epoch: 78/100.. Training Loss: 0.551.. Validation Loss: 0.565.. Training Accuracy: 0.717.. Validation Accuracy: 0.700 Epoch: 79/100.. Training Loss: 0.552.. Validation Loss: 0.558.. Training Accuracy: 0.718.. Validation Accuracy: 0.714 Epoch: 80/100.. Training Loss: 0.552.. Validation Loss: 0.555.. Training Accuracy: 0.717.. Validation Accuracy: 0.712 Epoch: 81/100.. Training Loss: 0.553.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.712 Epoch: 82/100.. Training Loss: 0.551.. Validation Loss: 0.557.. Training Accuracy: 0.715.. Validation Accuracy: 0.710 Epoch: 83/100.. Training Loss: 0.550.. Validation Loss: 0.553.. Training Accuracy: 0.717.. Validation Accuracy: 0.716 Epoch: 84/100.. Training Loss: 0.550.. Validation Loss: 0.557.. Training Accuracy: 0.718.. Validation Accuracy: 0.711 Epoch: 85/100.. Training Loss: 0.551.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.715 Epoch: 86/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.716.. Validation Accuracy: 0.704 Epoch: 87/100.. Training Loss: 0.553.. Validation Loss: 0.556.. Training Accuracy: 0.715.. Validation Accuracy: 0.717 Epoch: 88/100.. Training Loss: 0.551.. Validation Loss: 0.554.. Training Accuracy: 0.716.. Validation Accuracy: 0.714 Epoch: 89/100.. Training Loss: 0.551.. Validation Loss: 0.559.. Training Accuracy: 0.715.. Validation Accuracy: 0.712 Epoch: 90/100.. Training Loss: 0.549.. Validation Loss: 0.554.. Training Accuracy: 0.717.. Validation Accuracy: 0.718 Epoch: 91/100.. Training Loss: 0.551.. Validation Loss: 0.556.. Training Accuracy: 0.716.. Validation Accuracy: 0.717 Epoch: 92/100.. Training Loss: 0.550.. Validation Loss: 0.554.. Training Accuracy: 0.717.. Validation Accuracy: 0.715 Epoch: 93/100.. Training Loss: 0.549.. Validation Loss: 0.559.. Training Accuracy: 0.720.. Validation Accuracy: 0.705 Epoch: 94/100.. Training Loss: 0.549.. Validation Loss: 0.558.. Training Accuracy: 0.717.. Validation Accuracy: 0.706 Epoch: 95/100.. Training Loss: 0.549.. Validation Loss: 0.559.. Training Accuracy: 0.717.. Validation Accuracy: 0.714 Epoch: 96/100.. Training Loss: 0.549.. Validation Loss: 0.553.. Training Accuracy: 0.719.. Validation Accuracy: 0.718 Epoch: 97/100.. Training Loss: 0.551.. Validation Loss: 0.555.. Training Accuracy: 0.716.. Validation Accuracy: 0.718 Epoch: 98/100.. Training Loss: 0.551.. Validation Loss: 0.558.. Training Accuracy: 0.716.. Validation Accuracy: 0.711 Epoch: 99/100.. Training Loss: 0.549.. Validation Loss: 0.557.. Training Accuracy: 0.719.. Validation Accuracy: 0.716 Epoch: 100/100.. Training Loss: 0.547.. Validation Loss: 0.563.. Training Accuracy: 0.719.. Validation Accuracy: 0.714 . fig = plt.subplots(figsize=(15,5)) plt.plot(train_losses,label=&quot;Training loss&quot;) plt.plot(dev_losses,label=&quot;Validation loss&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efa57d460&gt; . fig = plt.subplots(figsize=(15,5)) plt.plot(train_acc,label=&quot;Training accuracy&quot;) plt.plot(dev_acc,label=&quot;Validation accuracy&quot;) plt.legend(frameon=False, fontsize=15) . &lt;matplotlib.legend.Legend at 0x27efb09d940&gt; . You can experiment by changing the architecture of the model .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/pytorch/neural%20network/activation%20function/loss/2022/03/23/NN-for-new-user-of-Pytorch.html",
            "relUrl": "/pytorch/neural%20network/activation%20function/loss/2022/03/23/NN-for-new-user-of-Pytorch.html",
            "date": " • Mar 23, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "My key takeaways of reading Trustworthy Online Controlled Experiments Book",
            "content": "twitter: https://twitter.com/KirkDBorne/status/1240781773921017857 . This article contains about A/B testing, make sure the reader understand fundamental concepts about control and treatment, p-values, confidence interval, statistical significance, practical significance, randomization, sample, and population in order to get the idea about A/B testing. You can get the fundamental statistics by looking at this video. . Important: Pay attention! It’s important. youtube: https://www.youtube.com/watch?v=bGdTr7yJbNs . Experimentation/Testing has been everywhere. It is widely adopted by startups to the corporate firm to detect how good the simple or bigger changes of the project or additional feature(be it a website, mobile app, etc.) of the project to give impact to the real-life/business. In Data Science, Experimentation is widely used to predict how good our experimentation is based on a few metrics by using statistical approaches. Online Trustworthy Controlled Experiment is the book I wish I had when I started learning A/B testing/Experimentation. This book covered all the fundamental concepts to advanced concepts about A/B testing through a step-by-step walkthrough such as designing the experimentation, running the experimentation and getting data, interpreting the results and results to decision-making. The author explained clearly the pitfalls and solutions to the problems that could exist during the experimentation. . A step-by-step walkthrough of A/B Testing . Designing The Experiment . The first step of doing online experimentation is to ascertain our hypothesis, a practical significance boundary, and a few metrics before running the experimentation. We should check the randomization of the sample that we will use for the control and treatment. We also should pay attention to how large the sample is to be used for running the experimentation. If we are concerned about detecting a small change or being more confident about the conclusion, we have to consider using more samples and a lower p-value threshold to get a more accurate result. However, If we are no longer care about small changes, we could reduce the sample to detect the practical significance. . Getting the Data and Running The Experimentation . In this section, you are going to get some data pertaining to the experimentation that we will be analyzing such as analyzing how many samples should be used, day of week effect due to everyone having different behavior on weekdays over the weekend, and also seasonality where users behave differently on holiday. We also consider looking at primacy and novelty effects where users have a tendency to use more often new features, . Interpreting the Results . One thing we should consider when interpreting results is how our experimentation will run properly and avoid some bugs that could invalidate the experiment result(guardrail metrics). For instance, we can check the latency which is essential to check that can affect the control and treatment, or expect the control and treatment sample to be equal to the configuration we set for A/B testing. These factors must be fulfilled to get better results that can affect the metrics we are going to achieve. . From Results to Decisions . Getting a result from the experiment is not the end of the experimentation. Getting a result that can make an impact on the business will be a good way of implementing experimentation. In A/B Testing, good results can be considered good if they are repeatable and trustworthy. However, there are a few factors that should consider whether we need to make a tradeoff between metrics for instance user engagement and revenue. Should we launch if there is no correlation between this metric?. We also consider about launch cost whether the revenue will cover the launch cost or get more expected revenue even need much cost to launch the product. Furthermore, We also consider statistical and practical significance thresholds of whether to launch or not launch the product. . Statistical and Practical Significance Thresholds The figure shown above depicts the statistical and practical significance threshold where the two dashed lines are the practical significance boundary and the black box is the statistical significance threshold along with the confidence interval. We know from statistics theory that the statistical significance threshold is less than or equal to 5% to quantify that we should reject the null hypothesis and the practical significance is managed based on the condition of our objective that we wanted to achieve. Based on the practical and statistical significance, we can take a step either choosing to launch or not. However, we can even take a follow-up test to test our hypothesis in order to translate practical and statistical significance boundaries based on some consideration of our experiment to get statistical power(a condition where the p-value is less than(more extreme) or equal to 0.05 implying there is a difference between control and treatment mean assuming the null hypothesis is true). . Conclusion . This book is really essential for every data scientist who specialized in product analytics in order to cover our understanding of data better through A/B testing. You will get a lot of enlightenment after reading this book. Read it, buy it. .",
            "url": "https://naiborhujosua.github.io/mlnotes_josua/abtesting/statistics/2021/06/20/Trustworthy-Online-Controlled-Experiments.html",
            "relUrl": "/abtesting/statistics/2021/06/20/Trustworthy-Online-Controlled-Experiments.html",
            "date": " • Jun 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Josua is a business development analyst who turns into a self-taught Machine Learning Engineer. His interests include statistical learning, predictive modeling, and causal inference. He loves running and it teaches him against giving up doing anything, even when implementing the Machine Learning Lifecycle(MLOps). . Apart from pursuing his passion for Machine Learning, he is keen on investing in the Indonesian Stock Exchange and Cryptocurrency. He has been running a full marathon in Jakarta Marathon in 2015 and Osaka Marathon in 2019. His next dreams are to run a marathon in TCS New York City Marathon and Virgin Money London Marathon. . For more, please reach out to LinkedIn . View my certificates in Data Science . Books I am currently reading . Here is a snapshot of a (small) subset of all of my Coding, Data Science and Machine Learning books. This collection would get you close to 98%-99% of all the necessary core skills to be a good Data Scientists. 1/6 pic.twitter.com/c1M4wpEXWt . &mdash; Bojan Tunguz (@tunguz) March 23, 2022",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://naiborhujosua.github.io/mlnotes_josua/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}